{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "    \n",
    "TRAIN = '../data/train/training_data.csv'\n",
    "train_data = pd.read_csv(TRAIN, index_col=1)\n",
    "dev_data = pd.read_csv('../data/dev/development_data.csv', index_col=1)\n",
    "\n",
    "tweets = train_data[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "words_set = set(open(\"allwords.txt\").read().split())\n",
    "words_set = set(item.lower() for item in words_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams for the data splitting on spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams():\n",
    "    unigrams = Counter()\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        for word in s:\n",
    "            unigrams[word] += 1\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Emoji Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def make_emoji_dict():\n",
    "    emoji_dict = {}\n",
    "    with open(\"emoji_image_to_whatIs.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            emoji_dict[tokens[0]] = tokens[1]\n",
    "    return emoji_dict\n",
    "emoji_dict = make_emoji_dict()\n",
    "#emoji_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into quartiles?\n",
    "def find_quartile_values(counts):\n",
    "    counts.sort()\n",
    "    first = np.quantile(counts, .25)\n",
    "    second = np.quantile(counts, .5)\n",
    "    third = np.quantile(counts, .75)\n",
    "    return [first, second, third] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Sentiment Features\n",
    "\n",
    "Emoji sentiment dictionary\n",
    "\n",
    "Features:\n",
    "Negative Sentiment,\n",
    "Neutral Sentiment,\n",
    "Positive Sentiment,\n",
    "Overall Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def load_emoji_sentiment_dict():\n",
    "    emoji_sentiment_dict = {}\n",
    "    with open(\"emoji_image_sentimentScore_definition_category.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            #each emoji has a list of 5 terms for a value\n",
    "            #emoji_sentiment_dict[tokens[0]] = [('neg':tokens[1]), ('neutral':tokens[2]), ('positive':tokens[3]), ('sentiment score':tokens[4]), ('name':tokens[5]), ('category':tokens[6])]\n",
    "            emoji_sentiment_dict[tokens[0]] = [tokens[1], tokens[2], tokens[3], tokens[4], tokens[5], tokens[6]]\n",
    "    return emoji_sentiment_dict\n",
    "emoji_sentiment_dict = load_emoji_sentiment_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cite\n",
    "\n",
    "Emoji\tNeg[0...1]\tNeut[0...1]\tPos[0...1]\tSentiment score[-1...+1]\tUnicode name\tUnicode block\n",
    "\n",
    "@article{Kralj2015emojis,\n",
    "  author={{Kralj Novak}, Petra and Smailovi{\\'c}, Jasmina and Sluban, Borut and Mozeti\\v{c}, Igor},\n",
    "  title={Sentiment of emojis},\n",
    "  journal={PLoS ONE},\n",
    "  volume={10},\n",
    "  number={12},\n",
    "  pages={e0144296},\n",
    "  url={http://dx.doi.org/10.1371/journal.pone.0144296},\n",
    "  year={2015}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeValues = []\n",
    "def make_emoji_negative_sentiment_bins():\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #negative  \n",
    "                score += float(emoji_sentiment_dict[word][0])\n",
    "                negativeValues.append(score)\n",
    "    return find_quartile_values(negativeValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emoji counts for data\n",
    "def negative_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    score = sum(map(lambda word : float(emoji_sentiment_dict[word][0]) if word in emoji_sentiment_dict else 0, s))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "neutralValues = []\n",
    "\n",
    "def make_emoji_neutral_sentiment_bins():\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #neutral \n",
    "                score += float(emoji_sentiment_dict[word][1])\n",
    "                neutralValues.append(score)\n",
    "    return find_quartile_values(negativeValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "# get emoji counts for data\n",
    "def neutral_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    score = sum(map(lambda word : float(emoji_sentiment_dict[word][1]) if word in emoji_sentiment_dict else 0, s))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "positiveValues = []\n",
    "\n",
    "def make_emoji_positive_sentiment_bins():\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #positive \n",
    "                score += float(emoji_sentiment_dict[word][2])\n",
    "                positiveValues.append(score)\n",
    "    return find_quartile_values(positiveValues)\n",
    "#make_emoji_positive_sentiment_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "# get emoji counts for data\n",
    "def positive_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    score = sum(map(lambda word : float(emoji_sentiment_dict[word][2]) if word in emoji_sentiment_dict else 0, s))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "sentimentValues = []\n",
    "\n",
    "def make_emoji_overall_sentiment_bins():\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #sentiment\n",
    "                score += float(emoji_sentiment_dict[word][3])\n",
    "                sentimentValues.append(score)\n",
    "    return find_quartile_values(sentimentValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "# get emoji counts for data\n",
    "def overall_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    count = sum(map(lambda word : float(emoji_sentiment_dict[word][3]) if word in emoji_sentiment_dict else 0, s))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into quartiles?\n",
    "def find_quartile_values(counts):\n",
    "    counts.sort()\n",
    "    first = np.quantile(counts, .25)\n",
    "    second = np.quantile(counts, .5)\n",
    "    third = np.quantile(counts, .75)\n",
    "    return [first, second, third] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_at_bins():\n",
    "    at_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        at_counts.append(count)\n",
    "    return find_quartile_values(at_counts)\n",
    "make_at_bins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profanity and Racist Lexicon counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make count features binary by finding median values over entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profanity_banned from trinker-lexicon-4c5e22b\n",
    "profanity_banned = set([\"anal\", \"anus\", \"arse\", \"ass\", \"balls\", \"ballsack\", \"bastard\", \n",
    "\"biatch\", \"bitch\", \"bloody\", \"blow job\", \"blowjob\", \"bollock\", \n",
    "\"bollok\", \"boner\", \"boob\", \"bugger\", \"bum\", \"butt\", \"buttplug\", \n",
    "\"clitoris\", \"cock\", \"coon\", \"crap\", \"cunt\", \"damn\", \"dick\", \"dildo\", \n",
    "\"dyke\", \"f u c k\", \"fag\", \"feck\", \"felching\", \"fellate\", \"fellatio\", \n",
    "\"flange\", \"fuck\", \"fudge packer\", \"fudgepacker\", \"God damn\", \n",
    "\"Goddamn\", \"hell\", \"homo\", \"jerk\", \"jizz\", \"knob end\", \"knobend\", \n",
    "\"labia\", \"lmao\", \"lmfao\", \"muff\", \"nigga\", \"nigger\", \"omg\", \"penis\", \n",
    "\"piss\", \"poop\", \"prick\", \"pube\", \"pussy\", \"queer\", \"s hit\", \"scrotum\", \n",
    "\"sex\", \"sh1t\", \"shit\", \"slut\", \"smegma\", \"spunk\", \"tit\", \"tosser\", \n",
    "\"turd\", \"twat\", \"vagina\", \"wank\", \"whore\", \"wtf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_profanity_bins():\n",
    "    profanity = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if word in profanity_banned else 0, s))\n",
    "        profanity.append(count)\n",
    "    return find_quartile_values(profanity)\n",
    "#make_profanity_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def load_racist_lexicon():\n",
    "    racist_lexicon = []\n",
    "    with open(\"racist_lexicon.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            word = tokens[1].split(\"\\\"\") \n",
    "            racist_lexicon.append(word[1])\n",
    "    racist_lexicon = set(racist_lexicon)\n",
    "    return racist_lexicon\n",
    "racist_lexicon = load_racist_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE  ------ to discuss ------\n",
    "#very sparse, maybe this should be contains as if it's a word in that lexicon, \n",
    "#it's pretty darn offensive if not guaranteed to be hate speech\n",
    "\n",
    "def make_racist_lexicon_bins():\n",
    "    racist_words = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if word in racist_lexicon else 0, s))\n",
    "        racist_words.append(count)\n",
    "    return find_quartile_values(racist_words)\n",
    "make_racist_lexicon_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_at_bins():\n",
    "    at_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        at_counts.append(count)\n",
    "    return find_quartile_values(at_counts)\n",
    "make_at_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "emoji_counts = []\n",
    "\n",
    "# get emoji counts for data\n",
    "def make_emoji_bins():\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        count = sum(map(lambda word : 1 if word in emoji_dict else 0, s))\n",
    "        emoji_counts.append(count)\n",
    "    return find_quartile_values(emoji_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_num_token_bins():\n",
    "    lens = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = len(s)\n",
    "        lens.append(count)\n",
    "    \n",
    "    return find_quartile_values(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_swear_bins():\n",
    "    bad_words_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_bad = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "        bad_words_counts.append(tot_bad)\n",
    "    \n",
    "    return find_quartile_values(bad_words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mention_bins():\n",
    "    mentions = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        mentions.append(count)\n",
    "    return find_quartile_values(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashtag_bins():\n",
    "    hashtag_counts = []\n",
    "    at_sum = 0\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '#' in word else 0, s))\n",
    "        hashtag_counts.append(count)\n",
    "    \n",
    "    return find_quartile_values(hashtag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_misspelling_bins():\n",
    "    misspell_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_misspelled = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() not in words_set:\n",
    "                tot_misspelled+=1\n",
    "        misspell_counts.append(tot_misspelled)\n",
    "    \n",
    "    return find_quartile_values(misspell_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "# get emoji counts for data\n",
    "def count_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    count = sum(map(lambda word : 1 if word in emoji_dict else 0, s))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_vocab = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_eng_vocab = pd.read_csv('eng_lang_lexicon.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>average_offensiveness</th>\n",
       "      <th>created_on</th>\n",
       "      <th>hateful_meaning</th>\n",
       "      <th>is_about_class</th>\n",
       "      <th>is_about_disability</th>\n",
       "      <th>is_about_ethnicity</th>\n",
       "      <th>is_about_gender</th>\n",
       "      <th>is_about_nationality</th>\n",
       "      <th>is_about_religion</th>\n",
       "      <th>is_about_sexual_orientation</th>\n",
       "      <th>...</th>\n",
       "      <th>nonhateful_meaning</th>\n",
       "      <th>number_of_sightings</th>\n",
       "      <th>number_of_sightings_this_month</th>\n",
       "      <th>number_of_sightings_this_year</th>\n",
       "      <th>plural_of</th>\n",
       "      <th>term</th>\n",
       "      <th>transliteration_of</th>\n",
       "      <th>updated_on</th>\n",
       "      <th>variant_of</th>\n",
       "      <th>vocabulary_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-14 16:51:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>floppy</td>\n",
       "      <td>floppies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-14 16:51:32</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jkedcbraz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-14 01:09:31</td>\n",
       "      <td>An African.  Etymology is variously attributed...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>floppy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-14 01:09:31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gcJbMVkVW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.0</td>\n",
       "      <td>2018-11-01 19:42:27</td>\n",
       "      <td>To poorly repair or assemble something</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nigger-rig</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-10 18:57:27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FZLzRqPbD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37.0</td>\n",
       "      <td>2019-05-07 20:05:39</td>\n",
       "      <td>Contraction of liberal and retard</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>libtard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-05-10 16:46:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>zPrJTQa6Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:16:15</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hooknosed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:16:15</td>\n",
       "      <td>hooknose</td>\n",
       "      <td>arAfRhXak</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:15:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>hooknose</td>\n",
       "      <td>hooknoses</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:15:34</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pbWuDaapo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:14:59</td>\n",
       "      <td>Referring to a stereotypical facial feature of...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hooknose</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:14:59</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JWkFPrJmY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:11:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>half bred</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:11:51</td>\n",
       "      <td>half breed</td>\n",
       "      <td>WDZWQCEtL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:09:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>jihadi-american</td>\n",
       "      <td>jihadi-americans</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:09:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qVTLPxNRc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:08:44</td>\n",
       "      <td>Derogatory play on hyphenated ethnicity, relat...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jihadi-american</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:08:44</td>\n",
       "      <td>NaN</td>\n",
       "      <td>BjWPVTHeu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:06:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>gookie</td>\n",
       "      <td>gookies</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:06:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>UXQj23n9C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:05:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gookie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:05:41</td>\n",
       "      <td>gook</td>\n",
       "      <td>evC7urmnF</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:04:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>goloid</td>\n",
       "      <td>goloids</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:04:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>VwHrhRGUZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:03:54</td>\n",
       "      <td>Abbreviation of \"mongoloid\"</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>goloid</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:03:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dGjYptxhd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:02:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>gink</td>\n",
       "      <td>ginks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:02:29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>wrirPQbk9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:01:57</td>\n",
       "      <td>Combination of \"gook\" and \"chink\"</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gink</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:01:57</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ByToTroPs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:00:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ghetto monkey</td>\n",
       "      <td>ghetto monkeys</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 17:00:51</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NhwQUP9Hg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 16:59:54</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ghetto monkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 16:59:54</td>\n",
       "      <td>porch monkey</td>\n",
       "      <td>riyCcVeuu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 16:58:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>gas huffer</td>\n",
       "      <td>gas huffers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 16:58:40</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pTAsUsQRV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 16:58:05</td>\n",
       "      <td>Alluding to substance abuse issues in the Abor...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>gas huffer</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 16:58:05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>n2rWujzuc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:31:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>filthypinoes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:31:09</td>\n",
       "      <td>filthypinos</td>\n",
       "      <td>ahoDNdRbm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:30:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>filthypino</td>\n",
       "      <td>filthypinos</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:30:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9Tbjnfbjc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:29:46</td>\n",
       "      <td>A portmanteau of the word 'filthy' and 'Filipino'</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>filthypino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:29:46</td>\n",
       "      <td>NaN</td>\n",
       "      <td>nJXyLAJPa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:27:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>featherhead</td>\n",
       "      <td>featherheads</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:27:22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>odoeqs8Cb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:26:49</td>\n",
       "      <td>Referring to North American Aboriginals in par...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>featherhead</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:26:49</td>\n",
       "      <td>NaN</td>\n",
       "      <td>G8LcbzVdo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:22:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>deminigger</td>\n",
       "      <td>deminiggers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:22:58</td>\n",
       "      <td>NaN</td>\n",
       "      <td>P6zCCm9iW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:22:21</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>deminigger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:22:21</td>\n",
       "      <td>seminiggers</td>\n",
       "      <td>xpzmtaysc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:21:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>seminigger</td>\n",
       "      <td>seminiggers</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:21:17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TQzKRXetW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:20:19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>seminigger</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:20:19</td>\n",
       "      <td>sand nigger</td>\n",
       "      <td>mngQLBoHf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:19:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>dagowop</td>\n",
       "      <td>dagowops</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-04-12 15:19:09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hExD3rxQp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>96.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A black person.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Possessing colour</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>coloured</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>colored</td>\n",
       "      <td>9jPyZAbef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>71.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A German, especially a German soldier (probabl...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A person's name</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Jerry</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GyhWrGzB8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1495</th>\n",
       "      <td>77.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A black person, particularly a black woman.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A bird</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>crow</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3HDevapjj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1496</th>\n",
       "      <td>22.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>Used in Quebec to denote English-speaking Cana...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A person's name</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Angie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>V844h4YX4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1497</th>\n",
       "      <td>46.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A white woman to a black person, or a black wo...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A person's name</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ann</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4MGCV96uh</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1498</th>\n",
       "      <td>65.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>An American Indian (Native American) who is 'r...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A fruit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>apple</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Hm888YgpX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1499</th>\n",
       "      <td>74.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A Scottish person. Scots language nickname for...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>An athlete or form of athletic protection</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8eEfKqz2G</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1500</th>\n",
       "      <td>76.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A black person with Caucasian mannerisms, as i...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A type of candy</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Junior Mint</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dbfhF6mWR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1501</th>\n",
       "      <td>83.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>An Asian-American who has lost their heritage....</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A fruit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>banana</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kP3hGCGXB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>55.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>Denotes white males who identify with East Asi...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Organic material resulting from reproduction</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>egg</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t9fazA3ZH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1503</th>\n",
       "      <td>57.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A black person.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A fruit</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>eggplant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>uMYxVc7qt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1504</th>\n",
       "      <td>97.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>An Arab</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A Turkish hat</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fez</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ck9uyEMGY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1505</th>\n",
       "      <td>45.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A black woman when she wears her hair and make...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A doll</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>black Barbie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T2yn4MMCP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1506</th>\n",
       "      <td>57.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>In Australia, a person of Macedonian descent.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>blockhead</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3tGk74u3R</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1507</th>\n",
       "      <td>72.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A French person. Prior to 19th century, referr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>An amphibian</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>frog</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>h64JZxPYH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1508</th>\n",
       "      <td>79.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A black person.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Soft, fluffy or frayed</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fuzzy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Acu2wwkQv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1509</th>\n",
       "      <td>69.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>Colonialist term used to refer to the Hadendoa...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fuzzy wuzzy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8BqkTrar4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1510</th>\n",
       "      <td>78.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>In Hawaii, denotes a Filipino, from the allege...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>book book</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3pXV3WFRm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1511</th>\n",
       "      <td>88.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>[1] A person of mixed white and black ancestry...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Type of dessert</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>brownie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42h6Z2JeX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1512</th>\n",
       "      <td>55.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A person of Greek descent. 'Bubble and squeak'...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A globule of one substance suspended in another</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bubble</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mACdFt7Vt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1513</th>\n",
       "      <td>50.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>[1] A male Native American. [2] A male black p...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Male animal</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>buck</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>qfg2Mpzf4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1514</th>\n",
       "      <td>69.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A person of mixed black and Asian parentage.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>An insect</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>bumblebee</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>pJ4kZcdbK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>94.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>An Austrialian, from the perception that Austr...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>can eater</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xFz6Rgcbe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1516</th>\n",
       "      <td>42.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>In the late 1900s, Chinese people in Australia...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Relating to the cosmos</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>celestial</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yaA9bkM3c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1517</th>\n",
       "      <td>95.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A black person.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A primate</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>monkey</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kMDhTeXE2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1518</th>\n",
       "      <td>88.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>[1] To African-Americans of the 1960s-1970s, w...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A person's name</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Charlie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DGzCe8c78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1519</th>\n",
       "      <td>86.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>[1] A woman, from 'whore'. [2] A prostitute.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ho</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>t9yPVBHhT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>81.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>[1] A woman, from 'whore'. [2] A prostitute.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>A gardening tool</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>hoe</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>ho</td>\n",
       "      <td>P8PQn9TNZ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>85.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A male Native American.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>chief</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>WBwg3wFQC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>88.0</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>A Somali, especially a militia fighter.</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>Thin</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>skinny</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0001-11-30 05:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>R6bhHFcHy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1523 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      average_offensiveness            created_on  \\\n",
       "0                       NaN   2019-05-14 16:51:32   \n",
       "1                       NaN   2019-05-14 01:09:31   \n",
       "2                      98.0   2018-11-01 19:42:27   \n",
       "3                      37.0   2019-05-07 20:05:39   \n",
       "4                       NaN   2019-04-12 17:16:15   \n",
       "5                       NaN   2019-04-12 17:15:34   \n",
       "6                       NaN   2019-04-12 17:14:59   \n",
       "7                       NaN   2019-04-12 17:11:51   \n",
       "8                       NaN   2019-04-12 17:09:24   \n",
       "9                       NaN   2019-04-12 17:08:44   \n",
       "10                      NaN   2019-04-12 17:06:21   \n",
       "11                      NaN   2019-04-12 17:05:41   \n",
       "12                      NaN   2019-04-12 17:04:29   \n",
       "13                      NaN   2019-04-12 17:03:54   \n",
       "14                      NaN   2019-04-12 17:02:29   \n",
       "15                      NaN   2019-04-12 17:01:57   \n",
       "16                      NaN   2019-04-12 17:00:51   \n",
       "17                      NaN   2019-04-12 16:59:54   \n",
       "18                      NaN   2019-04-12 16:58:40   \n",
       "19                      NaN   2019-04-12 16:58:05   \n",
       "20                      NaN   2019-04-12 15:31:09   \n",
       "21                      NaN   2019-04-12 15:30:22   \n",
       "22                      NaN   2019-04-12 15:29:46   \n",
       "23                      NaN   2019-04-12 15:27:22   \n",
       "24                      NaN   2019-04-12 15:26:49   \n",
       "25                      NaN   2019-04-12 15:22:58   \n",
       "26                      NaN   2019-04-12 15:22:21   \n",
       "27                      NaN   2019-04-12 15:21:17   \n",
       "28                      NaN   2019-04-12 15:20:19   \n",
       "29                      NaN   2019-04-12 15:19:09   \n",
       "...                     ...                   ...   \n",
       "1493                   96.0  -0001-11-30 05:00:00   \n",
       "1494                   71.0  -0001-11-30 05:00:00   \n",
       "1495                   77.0  -0001-11-30 05:00:00   \n",
       "1496                   22.0  -0001-11-30 05:00:00   \n",
       "1497                   46.0  -0001-11-30 05:00:00   \n",
       "1498                   65.0  -0001-11-30 05:00:00   \n",
       "1499                   74.0  -0001-11-30 05:00:00   \n",
       "1500                   76.0  -0001-11-30 05:00:00   \n",
       "1501                   83.0  -0001-11-30 05:00:00   \n",
       "1502                   55.0  -0001-11-30 05:00:00   \n",
       "1503                   57.0  -0001-11-30 05:00:00   \n",
       "1504                   97.0  -0001-11-30 05:00:00   \n",
       "1505                   45.0  -0001-11-30 05:00:00   \n",
       "1506                   57.0  -0001-11-30 05:00:00   \n",
       "1507                   72.0  -0001-11-30 05:00:00   \n",
       "1508                   79.0  -0001-11-30 05:00:00   \n",
       "1509                   69.0  -0001-11-30 05:00:00   \n",
       "1510                   78.0  -0001-11-30 05:00:00   \n",
       "1511                   88.0  -0001-11-30 05:00:00   \n",
       "1512                   55.0  -0001-11-30 05:00:00   \n",
       "1513                   50.0  -0001-11-30 05:00:00   \n",
       "1514                   69.0  -0001-11-30 05:00:00   \n",
       "1515                   94.0  -0001-11-30 05:00:00   \n",
       "1516                   42.0  -0001-11-30 05:00:00   \n",
       "1517                   95.0  -0001-11-30 05:00:00   \n",
       "1518                   88.0  -0001-11-30 05:00:00   \n",
       "1519                   86.0  -0001-11-30 05:00:00   \n",
       "1520                   81.0  -0001-11-30 05:00:00   \n",
       "1521                   85.0  -0001-11-30 05:00:00   \n",
       "1522                   88.0  -0001-11-30 05:00:00   \n",
       "\n",
       "                                        hateful_meaning  is_about_class  \\\n",
       "0                                                   NaN           False   \n",
       "1     An African.  Etymology is variously attributed...           False   \n",
       "2                To poorly repair or assemble something           False   \n",
       "3                     Contraction of liberal and retard           False   \n",
       "4                                                   NaN           False   \n",
       "5                                                   NaN           False   \n",
       "6     Referring to a stereotypical facial feature of...           False   \n",
       "7                                                   NaN           False   \n",
       "8                                                   NaN           False   \n",
       "9     Derogatory play on hyphenated ethnicity, relat...           False   \n",
       "10                                                  NaN           False   \n",
       "11                                                  NaN           False   \n",
       "12                                                  NaN           False   \n",
       "13                          Abbreviation of \"mongoloid\"           False   \n",
       "14                                                  NaN           False   \n",
       "15                    Combination of \"gook\" and \"chink\"           False   \n",
       "16                                                  NaN           False   \n",
       "17                                                  NaN           False   \n",
       "18                                                  NaN           False   \n",
       "19    Alluding to substance abuse issues in the Abor...           False   \n",
       "20                                                  NaN           False   \n",
       "21                                                  NaN           False   \n",
       "22    A portmanteau of the word 'filthy' and 'Filipino'           False   \n",
       "23                                                  NaN           False   \n",
       "24    Referring to North American Aboriginals in par...           False   \n",
       "25                                                  NaN           False   \n",
       "26                                                  NaN           False   \n",
       "27                                                  NaN           False   \n",
       "28                                                  NaN           False   \n",
       "29                                                  NaN           False   \n",
       "...                                                 ...             ...   \n",
       "1493                                    A black person.           False   \n",
       "1494  A German, especially a German soldier (probabl...           False   \n",
       "1495        A black person, particularly a black woman.           False   \n",
       "1496  Used in Quebec to denote English-speaking Cana...           False   \n",
       "1497  A white woman to a black person, or a black wo...           False   \n",
       "1498  An American Indian (Native American) who is 'r...           False   \n",
       "1499  A Scottish person. Scots language nickname for...           False   \n",
       "1500  A black person with Caucasian mannerisms, as i...           False   \n",
       "1501  An Asian-American who has lost their heritage....           False   \n",
       "1502  Denotes white males who identify with East Asi...           False   \n",
       "1503                                    A black person.           False   \n",
       "1504                                            An Arab           False   \n",
       "1505  A black woman when she wears her hair and make...           False   \n",
       "1506      In Australia, a person of Macedonian descent.           False   \n",
       "1507  A French person. Prior to 19th century, referr...           False   \n",
       "1508                                    A black person.           False   \n",
       "1509  Colonialist term used to refer to the Hadendoa...           False   \n",
       "1510  In Hawaii, denotes a Filipino, from the allege...           False   \n",
       "1511  [1] A person of mixed white and black ancestry...           False   \n",
       "1512  A person of Greek descent. 'Bubble and squeak'...           False   \n",
       "1513  [1] A male Native American. [2] A male black p...           False   \n",
       "1514       A person of mixed black and Asian parentage.           False   \n",
       "1515  An Austrialian, from the perception that Austr...           False   \n",
       "1516  In the late 1900s, Chinese people in Australia...           False   \n",
       "1517                                    A black person.           False   \n",
       "1518  [1] To African-Americans of the 1960s-1970s, w...           False   \n",
       "1519       [1] A woman, from 'whore'. [2] A prostitute.           False   \n",
       "1520       [1] A woman, from 'whore'. [2] A prostitute.           False   \n",
       "1521                            A male Native American.           False   \n",
       "1522            A Somali, especially a militia fighter.           False   \n",
       "\n",
       "      is_about_disability  is_about_ethnicity  is_about_gender  \\\n",
       "0                   False                True            False   \n",
       "1                   False                True            False   \n",
       "2                   False                True            False   \n",
       "3                    True               False            False   \n",
       "4                   False                True            False   \n",
       "5                   False                True            False   \n",
       "6                   False                True            False   \n",
       "7                   False                True            False   \n",
       "8                   False                True            False   \n",
       "9                   False                True            False   \n",
       "10                  False                True            False   \n",
       "11                  False                True            False   \n",
       "12                  False                True            False   \n",
       "13                  False                True            False   \n",
       "14                  False                True            False   \n",
       "15                  False                True            False   \n",
       "16                  False                True            False   \n",
       "17                  False                True            False   \n",
       "18                  False                True            False   \n",
       "19                  False                True            False   \n",
       "20                  False               False            False   \n",
       "21                  False               False            False   \n",
       "22                  False               False            False   \n",
       "23                  False                True            False   \n",
       "24                  False                True            False   \n",
       "25                  False                True            False   \n",
       "26                  False                True            False   \n",
       "27                  False                True            False   \n",
       "28                  False                True            False   \n",
       "29                  False                True            False   \n",
       "...                   ...                 ...              ...   \n",
       "1493                False                True            False   \n",
       "1494                False               False            False   \n",
       "1495                False                True            False   \n",
       "1496                False                True            False   \n",
       "1497                False                True            False   \n",
       "1498                False                True            False   \n",
       "1499                False               False            False   \n",
       "1500                False                True            False   \n",
       "1501                False                True            False   \n",
       "1502                False                True            False   \n",
       "1503                False                True            False   \n",
       "1504                False                True            False   \n",
       "1505                False                True            False   \n",
       "1506                False               False            False   \n",
       "1507                False                True            False   \n",
       "1508                False                True            False   \n",
       "1509                False                True            False   \n",
       "1510                False                True            False   \n",
       "1511                False                True            False   \n",
       "1512                False                True            False   \n",
       "1513                False                True            False   \n",
       "1514                False                True            False   \n",
       "1515                False                True            False   \n",
       "1516                False                True            False   \n",
       "1517                False                True            False   \n",
       "1518                False                True            False   \n",
       "1519                False               False             True   \n",
       "1520                False               False             True   \n",
       "1521                False               False            False   \n",
       "1522                False               False            False   \n",
       "\n",
       "      is_about_nationality  is_about_religion  is_about_sexual_orientation  \\\n",
       "0                    False              False                        False   \n",
       "1                    False              False                        False   \n",
       "2                    False              False                        False   \n",
       "3                    False              False                        False   \n",
       "4                    False               True                        False   \n",
       "5                    False               True                        False   \n",
       "6                    False               True                        False   \n",
       "7                    False              False                        False   \n",
       "8                    False               True                        False   \n",
       "9                    False               True                        False   \n",
       "10                   False              False                        False   \n",
       "11                    True              False                        False   \n",
       "12                   False              False                        False   \n",
       "13                   False              False                        False   \n",
       "14                   False              False                        False   \n",
       "15                   False              False                        False   \n",
       "16                   False              False                        False   \n",
       "17                   False              False                        False   \n",
       "18                   False              False                        False   \n",
       "19                   False              False                        False   \n",
       "20                    True              False                        False   \n",
       "21                    True              False                        False   \n",
       "22                    True              False                        False   \n",
       "23                   False              False                        False   \n",
       "24                   False              False                        False   \n",
       "25                   False              False                        False   \n",
       "26                   False              False                        False   \n",
       "27                   False              False                        False   \n",
       "28                   False              False                        False   \n",
       "29                    True              False                        False   \n",
       "...                    ...                ...                          ...   \n",
       "1493                 False              False                        False   \n",
       "1494                  True              False                        False   \n",
       "1495                 False              False                        False   \n",
       "1496                 False              False                        False   \n",
       "1497                 False              False                        False   \n",
       "1498                 False              False                        False   \n",
       "1499                 False              False                        False   \n",
       "1500                 False              False                        False   \n",
       "1501                 False              False                        False   \n",
       "1502                 False              False                        False   \n",
       "1503                 False              False                        False   \n",
       "1504                 False              False                        False   \n",
       "1505                 False              False                        False   \n",
       "1506                  True              False                        False   \n",
       "1507                 False              False                        False   \n",
       "1508                 False              False                        False   \n",
       "1509                 False              False                        False   \n",
       "1510                  True              False                        False   \n",
       "1511                 False              False                        False   \n",
       "1512                  True              False                        False   \n",
       "1513                 False              False                        False   \n",
       "1514                 False              False                        False   \n",
       "1515                 False              False                        False   \n",
       "1516                  True              False                        False   \n",
       "1517                 False              False                        False   \n",
       "1518                  True              False                        False   \n",
       "1519                 False              False                        False   \n",
       "1520                 False              False                        False   \n",
       "1521                 False              False                        False   \n",
       "1522                  True              False                        False   \n",
       "\n",
       "          ...                                    nonhateful_meaning  \\\n",
       "0         ...                                                   NaN   \n",
       "1         ...                                                   NaN   \n",
       "2         ...                                                   NaN   \n",
       "3         ...                                                   NaN   \n",
       "4         ...                                                   NaN   \n",
       "5         ...                                                   NaN   \n",
       "6         ...                                                   NaN   \n",
       "7         ...                                                   NaN   \n",
       "8         ...                                                   NaN   \n",
       "9         ...                                                   NaN   \n",
       "10        ...                                                   NaN   \n",
       "11        ...                                                   NaN   \n",
       "12        ...                                                   NaN   \n",
       "13        ...                                                   NaN   \n",
       "14        ...                                                   NaN   \n",
       "15        ...                                                   NaN   \n",
       "16        ...                                                   NaN   \n",
       "17        ...                                                   NaN   \n",
       "18        ...                                                   NaN   \n",
       "19        ...                                                   NaN   \n",
       "20        ...                                                   NaN   \n",
       "21        ...                                                   NaN   \n",
       "22        ...                                                   NaN   \n",
       "23        ...                                                   NaN   \n",
       "24        ...                                                   NaN   \n",
       "25        ...                                                   NaN   \n",
       "26        ...                                                   NaN   \n",
       "27        ...                                                   NaN   \n",
       "28        ...                                                   NaN   \n",
       "29        ...                                                   NaN   \n",
       "...       ...                                                   ...   \n",
       "1493      ...                                     Possessing colour   \n",
       "1494      ...                                       A person's name   \n",
       "1495      ...                                                A bird   \n",
       "1496      ...                                       A person's name   \n",
       "1497      ...                                       A person's name   \n",
       "1498      ...                                               A fruit   \n",
       "1499      ...             An athlete or form of athletic protection   \n",
       "1500      ...                                       A type of candy   \n",
       "1501      ...                                               A fruit   \n",
       "1502      ...          Organic material resulting from reproduction   \n",
       "1503      ...                                               A fruit   \n",
       "1504      ...                                         A Turkish hat   \n",
       "1505      ...                                                A doll   \n",
       "1506      ...                                                   NaN   \n",
       "1507      ...                                          An amphibian   \n",
       "1508      ...                                Soft, fluffy or frayed   \n",
       "1509      ...                                                   NaN   \n",
       "1510      ...                                                   NaN   \n",
       "1511      ...                                       Type of dessert   \n",
       "1512      ...       A globule of one substance suspended in another   \n",
       "1513      ...                                           Male animal   \n",
       "1514      ...                                             An insect   \n",
       "1515      ...                                                   NaN   \n",
       "1516      ...                                Relating to the cosmos   \n",
       "1517      ...                                             A primate   \n",
       "1518      ...                                       A person's name   \n",
       "1519      ...                                                   NaN   \n",
       "1520      ...                                      A gardening tool   \n",
       "1521      ...                                                   NaN   \n",
       "1522      ...                                                  Thin   \n",
       "\n",
       "     number_of_sightings number_of_sightings_this_month  \\\n",
       "0                      0                              0   \n",
       "1                      0                              0   \n",
       "2                      3                              1   \n",
       "3                      0                              0   \n",
       "4                      0                              0   \n",
       "5                      0                              0   \n",
       "6                      0                              0   \n",
       "7                      0                              0   \n",
       "8                      0                              0   \n",
       "9                      0                              0   \n",
       "10                     0                              0   \n",
       "11                     0                              0   \n",
       "12                     0                              0   \n",
       "13                     0                              0   \n",
       "14                     0                              0   \n",
       "15                     0                              0   \n",
       "16                     0                              0   \n",
       "17                     0                              0   \n",
       "18                     0                              0   \n",
       "19                     0                              0   \n",
       "20                     0                              0   \n",
       "21                     0                              0   \n",
       "22                     0                              0   \n",
       "23                     0                              0   \n",
       "24                     0                              0   \n",
       "25                     0                              0   \n",
       "26                     0                              0   \n",
       "27                     0                              0   \n",
       "28                     0                              0   \n",
       "29                     0                              0   \n",
       "...                  ...                            ...   \n",
       "1493                   0                              0   \n",
       "1494                   0                              0   \n",
       "1495                   0                              0   \n",
       "1496                   0                              0   \n",
       "1497                   0                              0   \n",
       "1498                   0                              0   \n",
       "1499                   0                              0   \n",
       "1500                   0                              0   \n",
       "1501                   0                              0   \n",
       "1502                   0                              0   \n",
       "1503                   0                              0   \n",
       "1504                   0                              0   \n",
       "1505                   0                              0   \n",
       "1506                   0                              0   \n",
       "1507                   0                              0   \n",
       "1508                   0                              0   \n",
       "1509                   0                              0   \n",
       "1510                   0                              0   \n",
       "1511                   0                              0   \n",
       "1512                   0                              0   \n",
       "1513                   0                              0   \n",
       "1514                   0                              0   \n",
       "1515                   0                              0   \n",
       "1516                   0                              0   \n",
       "1517                   0                              0   \n",
       "1518                   0                              0   \n",
       "1519                   0                              0   \n",
       "1520                   0                              0   \n",
       "1521                   0                              0   \n",
       "1522                   0                              0   \n",
       "\n",
       "     number_of_sightings_this_year        plural_of              term  \\\n",
       "0                                0           floppy          floppies   \n",
       "1                                0              NaN            floppy   \n",
       "2                                3              NaN        nigger-rig   \n",
       "3                                0              NaN           libtard   \n",
       "4                                0              NaN         hooknosed   \n",
       "5                                0         hooknose         hooknoses   \n",
       "6                                0              NaN          hooknose   \n",
       "7                                0              NaN         half bred   \n",
       "8                                0  jihadi-american  jihadi-americans   \n",
       "9                                0              NaN   jihadi-american   \n",
       "10                               0           gookie           gookies   \n",
       "11                               0              NaN            gookie   \n",
       "12                               0           goloid           goloids   \n",
       "13                               0              NaN            goloid   \n",
       "14                               0             gink             ginks   \n",
       "15                               0              NaN              gink   \n",
       "16                               0    ghetto monkey    ghetto monkeys   \n",
       "17                               0              NaN     ghetto monkey   \n",
       "18                               0       gas huffer       gas huffers   \n",
       "19                               0              NaN        gas huffer   \n",
       "20                               0              NaN      filthypinoes   \n",
       "21                               0       filthypino       filthypinos   \n",
       "22                               0              NaN        filthypino   \n",
       "23                               0      featherhead      featherheads   \n",
       "24                               0              NaN       featherhead   \n",
       "25                               0       deminigger       deminiggers   \n",
       "26                               0              NaN        deminigger   \n",
       "27                               0       seminigger       seminiggers   \n",
       "28                               0              NaN        seminigger   \n",
       "29                               0          dagowop          dagowops   \n",
       "...                            ...              ...               ...   \n",
       "1493                             0              NaN          coloured   \n",
       "1494                             0              NaN             Jerry   \n",
       "1495                             0              NaN              crow   \n",
       "1496                             0              NaN             Angie   \n",
       "1497                             0              NaN               Ann   \n",
       "1498                             0              NaN             apple   \n",
       "1499                             0              NaN              jock   \n",
       "1500                             0              NaN       Junior Mint   \n",
       "1501                             0              NaN            banana   \n",
       "1502                             0              NaN               egg   \n",
       "1503                             0              NaN          eggplant   \n",
       "1504                             0              NaN               fez   \n",
       "1505                             0              NaN      black Barbie   \n",
       "1506                             0              NaN         blockhead   \n",
       "1507                             0              NaN              frog   \n",
       "1508                             0              NaN             fuzzy   \n",
       "1509                             0              NaN       fuzzy wuzzy   \n",
       "1510                             0              NaN         book book   \n",
       "1511                             0              NaN           brownie   \n",
       "1512                             0              NaN            bubble   \n",
       "1513                             0              NaN              buck   \n",
       "1514                             0              NaN         bumblebee   \n",
       "1515                             0              NaN         can eater   \n",
       "1516                             0              NaN         celestial   \n",
       "1517                             0              NaN            monkey   \n",
       "1518                             0              NaN           Charlie   \n",
       "1519                             0              NaN                ho   \n",
       "1520                             0              NaN               hoe   \n",
       "1521                             0              NaN             chief   \n",
       "1522                             0              NaN            skinny   \n",
       "\n",
       "      transliteration_of            updated_on    variant_of vocabulary_id  \n",
       "0                    NaN   2019-05-14 16:51:32           NaN     jkedcbraz  \n",
       "1                    NaN   2019-05-14 01:09:31           NaN     gcJbMVkVW  \n",
       "2                    NaN   2019-05-10 18:57:27           NaN     FZLzRqPbD  \n",
       "3                    NaN   2019-05-10 16:46:49           NaN     zPrJTQa6Y  \n",
       "4                    NaN   2019-04-12 17:16:15      hooknose     arAfRhXak  \n",
       "5                    NaN   2019-04-12 17:15:34           NaN     pbWuDaapo  \n",
       "6                    NaN   2019-04-12 17:14:59           NaN     JWkFPrJmY  \n",
       "7                    NaN   2019-04-12 17:11:51    half breed     WDZWQCEtL  \n",
       "8                    NaN   2019-04-12 17:09:24           NaN     qVTLPxNRc  \n",
       "9                    NaN   2019-04-12 17:08:44           NaN     BjWPVTHeu  \n",
       "10                   NaN   2019-04-12 17:06:21           NaN     UXQj23n9C  \n",
       "11                   NaN   2019-04-12 17:05:41          gook     evC7urmnF  \n",
       "12                   NaN   2019-04-12 17:04:29           NaN     VwHrhRGUZ  \n",
       "13                   NaN   2019-04-12 17:03:54           NaN     dGjYptxhd  \n",
       "14                   NaN   2019-04-12 17:02:29           NaN     wrirPQbk9  \n",
       "15                   NaN   2019-04-12 17:01:57           NaN     ByToTroPs  \n",
       "16                   NaN   2019-04-12 17:00:51           NaN     NhwQUP9Hg  \n",
       "17                   NaN   2019-04-12 16:59:54  porch monkey     riyCcVeuu  \n",
       "18                   NaN   2019-04-12 16:58:40           NaN     pTAsUsQRV  \n",
       "19                   NaN   2019-04-12 16:58:05           NaN     n2rWujzuc  \n",
       "20                   NaN   2019-04-12 15:31:09   filthypinos     ahoDNdRbm  \n",
       "21                   NaN   2019-04-12 15:30:22           NaN     9Tbjnfbjc  \n",
       "22                   NaN   2019-04-12 15:29:46           NaN     nJXyLAJPa  \n",
       "23                   NaN   2019-04-12 15:27:22           NaN     odoeqs8Cb  \n",
       "24                   NaN   2019-04-12 15:26:49           NaN     G8LcbzVdo  \n",
       "25                   NaN   2019-04-12 15:22:58           NaN     P6zCCm9iW  \n",
       "26                   NaN   2019-04-12 15:22:21   seminiggers     xpzmtaysc  \n",
       "27                   NaN   2019-04-12 15:21:17           NaN     TQzKRXetW  \n",
       "28                   NaN   2019-04-12 15:20:19   sand nigger     mngQLBoHf  \n",
       "29                   NaN   2019-04-12 15:19:09           NaN     hExD3rxQp  \n",
       "...                  ...                   ...           ...           ...  \n",
       "1493                 NaN  -0001-11-30 05:00:00       colored     9jPyZAbef  \n",
       "1494                 NaN  -0001-11-30 05:00:00           NaN     GyhWrGzB8  \n",
       "1495                 NaN  -0001-11-30 05:00:00           NaN     3HDevapjj  \n",
       "1496                 NaN  -0001-11-30 05:00:00           NaN     V844h4YX4  \n",
       "1497                 NaN  -0001-11-30 05:00:00           NaN     4MGCV96uh  \n",
       "1498                 NaN  -0001-11-30 05:00:00           NaN     Hm888YgpX  \n",
       "1499                 NaN  -0001-11-30 05:00:00           NaN     8eEfKqz2G  \n",
       "1500                 NaN  -0001-11-30 05:00:00           NaN     dbfhF6mWR  \n",
       "1501                 NaN  -0001-11-30 05:00:00           NaN     kP3hGCGXB  \n",
       "1502                 NaN  -0001-11-30 05:00:00           NaN     t9fazA3ZH  \n",
       "1503                 NaN  -0001-11-30 05:00:00           NaN     uMYxVc7qt  \n",
       "1504                 NaN  -0001-11-30 05:00:00           NaN     Ck9uyEMGY  \n",
       "1505                 NaN  -0001-11-30 05:00:00           NaN     T2yn4MMCP  \n",
       "1506                 NaN  -0001-11-30 05:00:00           NaN     3tGk74u3R  \n",
       "1507                 NaN  -0001-11-30 05:00:00           NaN     h64JZxPYH  \n",
       "1508                 NaN  -0001-11-30 05:00:00           NaN     Acu2wwkQv  \n",
       "1509                 NaN  -0001-11-30 05:00:00           NaN     8BqkTrar4  \n",
       "1510                 NaN  -0001-11-30 05:00:00           NaN     3pXV3WFRm  \n",
       "1511                 NaN  -0001-11-30 05:00:00           NaN     42h6Z2JeX  \n",
       "1512                 NaN  -0001-11-30 05:00:00           NaN     mACdFt7Vt  \n",
       "1513                 NaN  -0001-11-30 05:00:00           NaN     qfg2Mpzf4  \n",
       "1514                 NaN  -0001-11-30 05:00:00           NaN     pJ4kZcdbK  \n",
       "1515                 NaN  -0001-11-30 05:00:00           NaN     xFz6Rgcbe  \n",
       "1516                 NaN  -0001-11-30 05:00:00           NaN     yaA9bkM3c  \n",
       "1517                 NaN  -0001-11-30 05:00:00           NaN     kMDhTeXE2  \n",
       "1518                 NaN  -0001-11-30 05:00:00           NaN     DGzCe8c78  \n",
       "1519                 NaN  -0001-11-30 05:00:00           NaN     t9yPVBHhT  \n",
       "1520                 NaN  -0001-11-30 05:00:00            ho     P8PQn9TNZ  \n",
       "1521                 NaN  -0001-11-30 05:00:00           NaN     WBwg3wFQC  \n",
       "1522                 NaN  -0001-11-30 05:00:00           NaN     R6bhHFcHy  \n",
       "\n",
       "[1523 rows x 23 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_eng_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatebase_words_set = set(open(\"hatebase_terms.txt\").read().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get lists of all words from hatebase pertaining to a certain category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_religion = df_eng_vocab.loc[df_eng_vocab['is_about_religion']][['term']].values\n",
    "hb_sexual_orientation = df_eng_vocab.loc[df_eng_vocab['is_about_sexual_orientation']][['term']].values\n",
    "hb_ethnicity = df_eng_vocab.loc[df_eng_vocab['is_about_ethnicity']][['term']].values\n",
    "hb_disability = df_eng_vocab.loc[df_eng_vocab['is_about_disability']][['term']].values\n",
    "hb_social_class = df_eng_vocab.loc[df_eng_vocab['is_about_class']][['term']].values\n",
    "hb_nationality = df_eng_vocab.loc[df_eng_vocab['is_about_nationality']][['term']].values\n",
    "hb_gender = df_eng_vocab.loc[df_eng_vocab['is_about_gender']][['term']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bins contain the splits for which bin a tweet's feature counts will land in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token_bins = make_num_token_bins()\n",
    "swear_bins = make_swear_bins()\n",
    "at_bins = make_at_bins()\n",
    "hashtag_bins = make_hashtag_bins()\n",
    "emoji_bins = make_emoji_bins()\n",
    "#hatebase_words_bins = make_hatebase_bins()\n",
    "misspell_bins =  make_misspelling_bins()\n",
    "emoji_words_bins = make_emoji_bins()\n",
    "emoji_overall_sentiment_bins = make_emoji_overall_sentiment_bins()\n",
    "emoji_negative_sentiment_bins = make_emoji_negative_sentiment_bins()\n",
    "emoji_positive_sentiment_bins = make_emoji_positive_sentiment_bins()\n",
    "emoji_neutral_sentiment_bins = make_emoji_neutral_sentiment_bins()\n",
    "profanity_bins = make_profanity_bins()\n",
    "racist_bins = make_racist_lexicon_bins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_more_upper(tweet):\n",
    "    total_caps = sum(map(lambda ch : 1 if ch.isupper() else 0, tweet))\n",
    "    if total_caps > len(tweet) // 2:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_consecutive_punc(tweet):\n",
    "    for word in tweet.split():\n",
    "        if 'http://' in word: continue\n",
    "        for i in range(len(word)-1):\n",
    "            if word[i] in string.punctuation and word[i+1] in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_person = ['he', 'she', 'they', 'him', 'her', 'them', 'his', 'hers', 'their', 'theirs', 'themselves', 'himself', 'herself']\n",
    "second_person = ['you', 'your', 'yours']\n",
    "first_person =['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if first word in tweet is a third person pronoun. Also check if number of third person pronouns is greater\n",
    "# than first person pronouns\n",
    "def get_pronouns(tweet):\n",
    "    first_pronoun = 0\n",
    "    third_and_second_greater_than_first = 0\n",
    "    words = tweet.split()\n",
    "    if words[0] in third_person or words[0] in second_person: \n",
    "        first_pronoun = 1\n",
    "    \n",
    "    first_person_count = 0\n",
    "    third_second_person_count = 0\n",
    "    for word in words:\n",
    "        if word in third_person or word in second_person:\n",
    "            third_second_person_count += 1\n",
    "        elif word in first_person:\n",
    "            first_person_count += 1\n",
    "            \n",
    "    if first_person_count < third_second_person_count:\n",
    "        third_and_second_greater_than_first = 1\n",
    "       \n",
    "    return first_pronoun, third_and_second_greater_than_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite lexicon:\n",
    "    Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \n",
    "        Proceedings of the ACM SIGKDD International Conference on Knowledge \n",
    "        Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \n",
    "        Washington, USA,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = set(open(\"opinion-lexicon-English/positive-words.txt\").read().split())\n",
    "neg_words = set(open(\"opinion-lexicon-English/negative-words.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns 1, 1 if negative words outnumber positive words and there are no positive words\n",
    "def get_sentiment(tweet):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for word in tweet.split():\n",
    "        if word in pos_words:\n",
    "            pos_count += 1\n",
    "        if word in neg_words:\n",
    "            neg_count += 1\n",
    "    if neg_count > pos_count:\n",
    "        if pos_count > 0:\n",
    "            return 1, 0\n",
    "        else:\n",
    "            return 1, 1\n",
    "    else:\n",
    "        if pos_count > 0:\n",
    "            return 0, 0\n",
    "        else:\n",
    "            return 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns one if a word in the tweet has non alphanumeric characters (not including punctuation at the end of a word)\n",
    "def contains_non_alphanum(tweet):\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if not word.isalnum():\n",
    "            if word[-1] not in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most common unigrams\n",
    "unigram_counts = get_unigrams()\n",
    "top_unigrams = unigram_counts.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each count feature has four bins: 1 for < 25th percentile, 2 for < 50th percentile, \n",
    "#3 for < 75th percentile, and 4 for < 100th percentile\n",
    "def find_bin(count, bin_name):\n",
    "    for i in range(len(bin_name)):\n",
    "        if count < bin_name[i]:\n",
    "            return i + 1\n",
    "    return len(bin_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the tweets by getting their feature representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets, judgements):\n",
    "#     for word in [u[0] for u in top_unigrams]:\n",
    "#         tweets[word] = tweets['tweet'].str.contains(word).astype(int)\n",
    "    word_counts = []\n",
    "    swear_counts = []\n",
    "    at_counts = []\n",
    "    emoji_counts = []\n",
    "    \n",
    "    emoji_negative = []\n",
    "    emoji_positive = []\n",
    "    emoji_neutral = []\n",
    "    emoji_overall_sentiment = []\n",
    "    contains_emoji = []\n",
    "    \n",
    "    profanity_counts = []\n",
    "    contains_profanity = []\n",
    "    racist_counts = []\n",
    "    contains_racist = []\n",
    "    \n",
    "    contains_at = []\n",
    "    hashtag_counts = []\n",
    "    contains_hashtag = []\n",
    "    consecutive_punc = []\n",
    "    more_upper = []\n",
    "    first_pronoun = []\n",
    "    fewer_first_person = []\n",
    "    more_negative = []\n",
    "    no_positive = []\n",
    "    contains_url = []\n",
    "    not_alphanum = []\n",
    "    misspellings = []\n",
    "    disagreements = []\n",
    "    in_hatebase = []\n",
    "    about_gender = []\n",
    "    about_religion = []\n",
    "    about_sexual_orientation = []\n",
    "    about_ethnicity = []\n",
    "    about_disability = []\n",
    "    about_social_class = []\n",
    "    about_nationality = []\n",
    "\n",
    "\n",
    "    for tweet in tweets['tweet']:\n",
    "        #count tokens\n",
    "        tweet_words = tweet.split()\n",
    "        num_token_bin = find_bin(len(tweet_words), num_token_bins)\n",
    "        word_counts.append(num_token_bin)\n",
    "        \n",
    "        #emoji tokenizer\n",
    "        s = tknzr.tokenize(tweet)\n",
    "        #num_token_bin = find_bin(len(s), num_token_bins)\n",
    "        #word_counts.append(num_token_bin)\n",
    "        \n",
    "        #count swear words, misspellings, and if a word is in hatebase\n",
    "        misspell_count = 0\n",
    "        tot_bad = 0\n",
    "        tot_prof = 0\n",
    "        tot_racist = 0\n",
    "        hatebase = 0\n",
    "        hatebase_words = []\n",
    "        for word in tweet_words:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in profanity_banned:\n",
    "                tot_prof+=1\n",
    "                isProfane = 1\n",
    "            if word.lower() in racist_lexicon:\n",
    "                tot_racist+=1\n",
    "                isRacist = 1\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "            if word.lower() not in words_set:\n",
    "                misspell_count+=1\n",
    "            if word.lower() in hatebase_words_set:\n",
    "                hatebase = 1\n",
    "                hatebase_words.append(word)\n",
    "        swear_bin = find_bin(tot_bad, swear_bins)\n",
    "        swear_counts.append(swear_bin)\n",
    "        misspell_bin = find_bin(misspell_count, misspell_bins)\n",
    "        misspellings.append(misspell_bin)\n",
    "        in_hatebase.append(hatebase)\n",
    "        \n",
    "        racist_counts.append(tot_racist)\n",
    "        racist_bin = find_bin(tot_racist, racist_bins)\n",
    "        \n",
    "        \n",
    "        profanity_counts.append(tot_prof)\n",
    "        profanity_bin = find_bin(tot_prof, profanity_bins)\n",
    "        \n",
    "    \n",
    "        gender = religion = sexual_orientation = ethnicity = disability = social_class = nationality = 0\n",
    "        if hatebase == 1:\n",
    "            for word in hatebase_words:\n",
    "                if word in hb_gender: gender = 1\n",
    "                if word in hb_religion: religion = 1\n",
    "                if word in hb_sexual_orientation: sexual_orientation = 1\n",
    "                if word in hb_ethnicity: ethnicity = 1\n",
    "                if word in hb_disability: disability = 1\n",
    "                if word in hb_social_class: social_class = 1\n",
    "                if word in hb_nationality: nationality = 1\n",
    "        about_gender.append(gender)\n",
    "        about_religion.append(religion)\n",
    "        about_sexual_orientation.append(sexual_orientation)\n",
    "        about_ethnicity.append(ethnicity)\n",
    "        about_disability.append(disability)\n",
    "        about_social_class.append(social_class)\n",
    "        about_nationality.append(nationality)\n",
    "        contains_racist.append(isRacist)\n",
    "        contains_profanity.append(isProfane)\n",
    "        \n",
    "        #count mentions\n",
    "        at_count = tweet.count('@')\n",
    "        if at_count > 0:\n",
    "            contains_at.append(1)\n",
    "        else:\n",
    "            contains_at.append(0)\n",
    "        at_bin = find_bin(at_count, at_bins)\n",
    "        at_counts.append(at_bin)\n",
    "        \n",
    "        #count hashtags\n",
    "        hash_count = tweet.count('#')\n",
    "        if hash_count > 0:\n",
    "            contains_hashtag.append(1)\n",
    "        else:\n",
    "            contains_hashtag.append(0)\n",
    "        hash_bin = find_bin(hash_count, hashtag_bins)\n",
    "        hashtag_counts.append(hash_bin)\n",
    "        \n",
    "        #count emojis\n",
    "        emoji_count = count_emojis(tweet)\n",
    "        if emoji_count > 0:\n",
    "            contains_emoji.append(1)\n",
    "        else:\n",
    "            contains_emoji.append(0)\n",
    "        emoji_bin = find_bin(emoji_count, emoji_bins)\n",
    "        emoji_counts.append(emoji_bin)\n",
    "        \n",
    "        #emoji positive sentiment \n",
    "        emoji_positive_score = positive_sentiment_emojis(tweet) \n",
    "        emoji_positive_bin = find_bin(emoji_positive_score, emoji_bins)\n",
    "        emoji_positive.append(emoji_positive_bin)\n",
    "        \n",
    "        #emoji neutral sentiment \n",
    "        emoji_neutral_score = neutral_sentiment_emojis(tweet) \n",
    "        emoji_neutral_bin = find_bin(emoji_neutral_score, emoji_bins)\n",
    "        emoji_neutral.append(emoji_neutral_bin)\n",
    "        \n",
    "        #emoji negative sentiment \n",
    "        emoji_negative_score = negative_sentiment_emojis(tweet) \n",
    "        emoji_negative_bin = find_bin(emoji_negative_score, emoji_bins)\n",
    "        emoji_negative.append(emoji_negative_bin)\n",
    "        \n",
    "        #emoji overall sentiment \n",
    "        emoji_overall_score = overall_sentiment_emojis(tweet) \n",
    "        emoji_overall_bin = find_bin(emoji_overall_score, emoji_bins)\n",
    "        emoji_overall_sentiment.append(emoji_overall_bin)\n",
    "        \n",
    "        more_upper.append(has_more_upper(tweet))\n",
    "        consecutive_punc.append(has_consecutive_punc(tweet))\n",
    "        first, more = get_pronouns(tweet)\n",
    "        first_pronoun.append(first)\n",
    "        fewer_first_person.append(more)\n",
    "        \n",
    "        more_neg, pos = get_sentiment(tweet)\n",
    "        more_negative.append(more_neg)\n",
    "        no_positive.append(pos)\n",
    "        \n",
    "        if 'http://' in tweet:\n",
    "            contains_url.append(1)\n",
    "        else:\n",
    "            contains_url.append(0)\n",
    "            \n",
    "        not_alphanum.append(contains_non_alphanum(tweet))\n",
    "        \n",
    "        #See if there were disagreements about classification\n",
    "        i = tweets.loc[tweets['tweet']==tweet].index[0]\n",
    "        total_votes = judgements.at[i, 'count']\n",
    "        if (judgements.at[i, 'hate_speech'] == total_votes) or (judgements.at[i, 'offensive_language'] == total_votes)\\\n",
    "            or (judgements.at[i, 'neither'] == total_votes):\n",
    "            disagreements.append(0)\n",
    "        else:\n",
    "            disagreements.append(1)\n",
    "        \n",
    "    tweets['Word Counts'] = word_counts\n",
    "    tweets['Swear Counts'] = swear_counts\n",
    "    tweets['Profanity Counts'] = profanity_counts\n",
    "    tweets['Contains Profanity'] = contains_profanity\n",
    "    tweets['Racist Counts'] = racist_counts\n",
    "    tweets['Contains Racist'] = contains_racist\n",
    "    tweets['@ Counts'] = at_counts\n",
    "    tweets['Mention'] = contains_at\n",
    "    tweets['Contains Hashtag'] = contains_hashtag\n",
    "    tweets['Hashtag Counts'] = hashtag_counts\n",
    "    tweets['Emoji Counts'] = emoji_counts\n",
    "    tweets['Contains Emoji'] = contains_emoji\n",
    "    tweets['Negative Emoji'] = emoji_negative\n",
    "    tweets['Positive Emoji'] = emoji_positive\n",
    "    tweets['Neutral Emoji'] = emoji_neutral \n",
    "    tweets['Overall Emoji'] = emoji_overall_sentiment\n",
    "    tweets['Consecutive Punctuation'] = consecutive_punc\n",
    "    tweets['Majority Uppercase Letters'] = more_upper\n",
    "    tweets['First Word Second or Third Person Pronoun'] = first_pronoun\n",
    "    tweets['More Second or Third Person Pronouns than First'] = fewer_first_person\n",
    "    tweets['Majority Negative Words'] = more_negative\n",
    "    tweets['No Positive Words'] = no_positive\n",
    "    tweets['Contains URL'] = contains_url\n",
    "    tweets['Contains Non Alphanumeric Word'] = not_alphanum\n",
    "    tweets['Misspelling Count'] = misspellings\n",
    "    tweets['Judgement Disagreements'] = disagreements\n",
    "    tweets['About Gender (Hatebase)'] = about_gender\n",
    "    tweets['About Religion (Hatebase)'] = about_religion\n",
    "    tweets['About Ethnicity (Hatebase)'] = about_ethnicity\n",
    "    tweets['About Sexual Orientation (Hatebase)'] = about_sexual_orientation\n",
    "    tweets['About Disability (Hatebase)'] = about_disability\n",
    "    tweets['About Class (Hatebase)'] = about_social_class\n",
    "    tweets['About Nationality (Hatebase)'] = about_nationality\n",
    "    X = tweets[[col for col in tweets.columns if col!=\"tweet\"]].values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine train and dev sets for k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, dev_data], sort=False)\n",
    "tweets = data[['tweet']]\n",
    "judgements = data[['count', 'hate_speech', 'offensive_language', 'neither']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:182: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:183: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:185: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:186: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X = process_tweets(tweets, judgements)\n",
    "y = data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22214, 33), (22214,))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10-fold cross validation on combined training and dev sets on LR, SVM, and NB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(fit_intercept=True, max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "lr_y_pred = cross_val_predict(LR, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.15      0.24      1282\n",
      "          1       0.89      0.93      0.91     17186\n",
      "          2       0.67      0.69      0.68      3746\n",
      "\n",
      "avg / total       0.83      0.84      0.83     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LR:\", classification_report(y, lr_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1282\n",
      "          1       0.89      0.93      0.91     17186\n",
      "          2       0.66      0.77      0.71      3746\n",
      "\n",
      "avg / total       0.80      0.85      0.83     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(gamma='auto') \n",
    "svm_y_pred = cross_val_predict(svm, X, y, cv=cv)\n",
    "print(classification_report(y, svm_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.17      0.78      0.27      1282\n",
      "          1       0.95      0.72      0.82     17186\n",
      "          2       0.68      0.57      0.62      3746\n",
      "\n",
      "avg / total       0.86      0.70      0.76     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "nb_y_pred = cross_val_predict(gnb, X, y, cv=cv)\n",
    "print(classification_report(y, nb_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22214, 33), (22214,))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "State '4' does not have key '6'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-86dad9e0a41f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m#gets a posterior distribution P(label | feature_1 = f1, feature_2 = f2,..., feature_n = fn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m#gets the most likely label from that posterior distribution, which becomes our prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpomegranate/BayesianNetwork.pyx\u001b[0m in \u001b[0;36mpomegranate.BayesianNetwork.BayesianNetwork.predict_proba\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpomegranate/BayesianNetwork.pyx\u001b[0m in \u001b[0;36mpomegranate.BayesianNetwork._check_input\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: State '4' does not have key '6'"
     ]
    }
   ],
   "source": [
    "\n",
    "#need to add labels back into the data, since they technically will be a node in the network\n",
    "allData = np.hstack((X, y.reshape(-1,1)))\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pomegranate import *\n",
    "\n",
    "#single train test split\n",
    "#Note: didn't go for k-fold CV because then the network structure/parameters would have to be learned k times,\n",
    "#which seemed too time intensive to be really useful\n",
    "train,test = train_test_split(allData, test_size = 0.1)\n",
    "\n",
    "train.shape,test.shape\n",
    "\n",
    "#here's where the structure and params are learned - as it is, structure is learned with greedy algo. \n",
    "#To switch structure learning to Chow-Liu, run network = BayesianNetwork.from_samples(train, algorithm='chow-liu')\n",
    "#otherwise, if you want to learn with the greedy approach, just run BayesianNetwork.from_samples(train)\n",
    "network = BayesianNetwork.from_samples(train,algorithm=\"chow-liu\")\n",
    "\n",
    "#Each sample of the test set should be of shape (1,34) where the 34th column, corresponding to the label, \n",
    "#has the value None. That way, when we use network.predict_proba, we infer the label value given the feature values\n",
    "testNone = np.asarray([None for item in test])\n",
    "testSamples = np.asarray([item[:-1] for item in test])\n",
    "testToPredict = np.hstack((testSamples, testNone.reshape(-1,1)))\n",
    "\n",
    "y_true = [item[-1] for item in test]\n",
    "y_pred = []\n",
    "\n",
    "for item in testToPredict:\n",
    "    \n",
    "    #gets a posterior distribution P(label | feature_1 = f1, feature_2 = f2,..., feature_n = fn)\n",
    "    dist = network.predict_proba(item)[-1]\n",
    "    \n",
    "    #gets the most likely label from that posterior distribution, which becomes our prediction\n",
    "    prediction = dist.mle()\n",
    "    #print(prediction)\n",
    "    #print(prediction)\n",
    "    y_pred.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,fname,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            x = 5\n",
    "            #strr = format(cm[i, j], '.2f')\n",
    "            #ax.text(j,i,str(round(cm[i,j],2)))\n",
    "            ax.text(j, i, str(round(cm[i,j],2)),ha=\"center\", va=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fname)\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "#plot_confusion_matrix(np.asarray(y_true), np.asarray(y_pred), classes=['hate','off','nei'],\n",
    "                      #title='Normalized confusion matrix')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix bayesian network\n",
    "plot_confusion_matrix(np.asarray(y_true), np.asarray(y_pred), classes=np.asarray([0,1,2]),fname='bn.png',\n",
    "                      title='Confusion matrix, Bayesian Network',normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plot_confusion_matrix(np.asarray(y), np.asarray(nb_y_pred), classes=np.asarray([0,1,2]),fname='nb.png',\n",
    "                      title='Confusion matrix, Naive Bayes',normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(np.asarray(y), np.asarray(svm_y_pred), classes=np.asarray([0,1,2]),fname='svm.png',\n",
    "                      title='Confusion matrix, SVM',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(np.asarray(y), np.asarray(lr_y_pred), classes=np.asarray([0,1,2]),fname='lr.png',\n",
    "                      title='Confusion matrix, Logistic Regression',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the DAG structure (need graphviz, pygraphviz)\n",
    "network.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfortunate idiosyncrasy with saving: can't get the save to happen outside the exact current directory, so once\n",
    "#you make the image you should move it to misc\n",
    "network.plot('dag.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random aside: If you wanted to know the final log likelihood of the data given the learned network, log P(D | G)\n",
    "network.log_probability(train).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

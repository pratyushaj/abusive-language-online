{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "    \n",
    "TRAIN = '../data/train/training_data.csv'\n",
    "train_data = pd.read_csv(TRAIN, index_col=1)\n",
    "dev_data = pd.read_csv('../data/dev/development_data.csv', index_col=1)\n",
    "\n",
    "tweets = train_data[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "words_set = set(open(\"allwords.txt\").read().split())\n",
    "words_set = set(item.lower() for item in words_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams for the data splitting on spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams():\n",
    "    unigrams = Counter()\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        for word in s:\n",
    "            unigrams[word] += 1\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Emoji Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def make_emoji_dict():\n",
    "    emoji_dict = {}\n",
    "    with open(\"emoji_image_to_whatIs.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            emoji_dict[tokens[0]] = tokens[1]\n",
    "    return emoji_dict\n",
    "emoji_dict = make_emoji_dict()\n",
    "#emoji_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into quartiles?\n",
    "def find_quartile_values(counts):\n",
    "    counts.sort()\n",
    "    first = np.quantile(counts, .25)\n",
    "    second = np.quantile(counts, .5)\n",
    "    third = np.quantile(counts, .75)\n",
    "    return [first, second, third] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Sentiment Features\n",
    "\n",
    "Emoji sentiment dictionary\n",
    "\n",
    "Features:\n",
    "Negative Sentiment,\n",
    "Neutral Sentiment,\n",
    "Positive Sentiment,\n",
    "Overall Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def load_emoji_sentiment_dict():\n",
    "    emoji_sentiment_dict = {}\n",
    "    with open(\"emoji_image_sentimentScore_definition_category.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            #each emoji has a list of 5 terms for a value\n",
    "            #emoji_sentiment_dict[tokens[0]] = [('neg':tokens[1]), ('neutral':tokens[2]), ('positive':tokens[3]), ('sentiment score':tokens[4]), ('name':tokens[5]), ('category':tokens[6])]\n",
    "            emoji_sentiment_dict[tokens[0]] = [tokens[1], tokens[2], tokens[3], tokens[4], tokens[5], tokens[6]]\n",
    "    return emoji_sentiment_dict\n",
    "emoji_sentiment_dict = load_emoji_sentiment_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cite\n",
    "\n",
    "Emoji\tNeg[0...1]\tNeut[0...1]\tPos[0...1]\tSentiment score[-1...+1]\tUnicode name\tUnicode block\n",
    "\n",
    "@article{Kralj2015emojis,\n",
    "  author={{Kralj Novak}, Petra and Smailovi{\\'c}, Jasmina and Sluban, Borut and Mozeti\\v{c}, Igor},\n",
    "  title={Sentiment of emojis},\n",
    "  journal={PLoS ONE},\n",
    "  volume={10},\n",
    "  number={12},\n",
    "  pages={e0144296},\n",
    "  url={http://dx.doi.org/10.1371/journal.pone.0144296},\n",
    "  year={2015}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeValues = []\n",
    "def make_emoji_negative_sentiment_bins():\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #negative  \n",
    "                score += float(emoji_sentiment_dict[word][0])\n",
    "                negativeValues.append(score)\n",
    "    return find_quartile_values(negativeValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emoji counts for data\n",
    "def negative_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    check = False\n",
    "    emojis = []\n",
    "    score = 0\n",
    "    for word in s:\n",
    "        if word in emoji_sentiment_dict:\n",
    "            check = True\n",
    "            emojis.append(word)\n",
    "    if check:\n",
    "        for emoji in emojis:\n",
    "            num = emoji_sentiment_dict[emoji][0]\n",
    "            if type(num) is not str:\n",
    "                score += float(num)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutralValues = []\n",
    "\n",
    "def make_emoji_neutral_sentiment_bins():\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #neutral \n",
    "                score += float(emoji_sentiment_dict[word][1])\n",
    "                neutralValues.append(score)\n",
    "    return find_quartile_values(negativeValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emoji counts for data\n",
    "def neutral_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    check = False\n",
    "    emojis = []\n",
    "    score = 0\n",
    "    for word in s:\n",
    "        if word in emoji_sentiment_dict:\n",
    "            check = True\n",
    "            emojis.append(word)\n",
    "    if check:\n",
    "        for emoji in emojis:\n",
    "            num = emoji_sentiment_dict[emoji][1]\n",
    "            if type(num) is not str:\n",
    "                score += float(num)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveValues = []\n",
    "\n",
    "def make_emoji_positive_sentiment_bins():\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #positive \n",
    "                score += float(emoji_sentiment_dict[word][2])\n",
    "                positiveValues.append(score)\n",
    "    return find_quartile_values(positiveValues)\n",
    "#make_emoji_positive_sentiment_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emoji counts for data\n",
    "def positive_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    check = False\n",
    "    emojis = []\n",
    "    score = 0\n",
    "    for word in s:\n",
    "        if word in emoji_sentiment_dict:\n",
    "            check = True\n",
    "            emojis.append(word)\n",
    "    if check:\n",
    "        for emoji in emojis:\n",
    "            num = emoji_sentiment_dict[emoji][2]\n",
    "            if type(num) is not str:\n",
    "                score += float(num)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentValues = []\n",
    "\n",
    "def make_emoji_overall_sentiment_bins():\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #sentiment\n",
    "                score += float(emoji_sentiment_dict[word][3])\n",
    "                sentimentValues.append(score)\n",
    "    return find_quartile_values(sentimentValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emoji counts for data\n",
    "def overall_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    check = False\n",
    "    emojis = []\n",
    "    score = 0\n",
    "    for word in s:\n",
    "        if word in emoji_sentiment_dict:\n",
    "            check = True\n",
    "            emojis.append(word)\n",
    "    if check:\n",
    "        for emoji in emojis:\n",
    "            num = emoji_sentiment_dict[emoji][3]\n",
    "            if type(num) is not str:\n",
    "                score += float(num)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find quartiles based on counts from entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quartile_values(counts):\n",
    "    counts.sort()\n",
    "    first = np.quantile(counts, .25)\n",
    "    second = np.quantile(counts, .5)\n",
    "    third = np.quantile(counts, .75)\n",
    "    return [first, second, third] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profanity and Racist Lexicon counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make count features binary by finding median values over entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profanity_banned from trinker-lexicon-4c5e22b\n",
    "profanity_banned = set([\"anal\", \"anus\", \"arse\", \"ass\", \"balls\", \"ballsack\", \"bastard\", \n",
    "\"biatch\", \"bitch\", \"bloody\", \"blow job\", \"blowjob\", \"bollock\", \n",
    "\"bollok\", \"boner\", \"boob\", \"bugger\", \"bum\", \"butt\", \"buttplug\", \n",
    "\"clitoris\", \"cock\", \"coon\", \"crap\", \"cunt\", \"damn\", \"dick\", \"dildo\", \n",
    "\"dyke\", \"f u c k\", \"fag\", \"feck\", \"felching\", \"fellate\", \"fellatio\", \n",
    "\"flange\", \"fuck\", \"fudge packer\", \"fudgepacker\", \"God damn\", \n",
    "\"Goddamn\", \"hell\", \"homo\", \"jerk\", \"jizz\", \"knob end\", \"knobend\", \n",
    "\"labia\", \"lmao\", \"lmfao\", \"muff\", \"nigga\", \"nigger\", \"omg\", \"penis\", \n",
    "\"piss\", \"poop\", \"prick\", \"pube\", \"pussy\", \"queer\", \"s hit\", \"scrotum\", \n",
    "\"sex\", \"sh1t\", \"shit\", \"slut\", \"smegma\", \"spunk\", \"tit\", \"tosser\", \n",
    "\"turd\", \"twat\", \"vagina\", \"wank\", \"whore\", \"wtf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_profanity_bins():\n",
    "    profanity = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if word in profanity_banned else 0, s))\n",
    "        profanity.append(count)\n",
    "    return find_quartile_values(profanity)\n",
    "#make_profanity_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def load_racist_lexicon():\n",
    "    racist_lexicon = []\n",
    "    with open(\"racist_lexicon.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            word = tokens[1].split(\"\\\"\") \n",
    "            racist_lexicon.append(word[1])\n",
    "    racist_lexicon = set(racist_lexicon)\n",
    "    return racist_lexicon\n",
    "racist_lexicon = load_racist_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE  ------ to discuss ------\n",
    "#very sparse, maybe this should be contains as if it's a word in that lexicon, \n",
    "#it's pretty darn offensive if not guaranteed to be hate speech\n",
    "\n",
    "def make_racist_lexicon_bins():\n",
    "    racist_words = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if word in racist_lexicon else 0, s))\n",
    "        racist_words.append(count)\n",
    "    return find_quartile_values(racist_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_at_bins():\n",
    "    at_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        at_counts.append(count)\n",
    "    return find_quartile_values(at_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_counts = []\n",
    "\n",
    "# get emoji counts for data\n",
    "def make_emoji_bins():\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        count = sum(map(lambda word : 1 if word in emoji_dict else 0, s))\n",
    "        emoji_counts.append(count)\n",
    "    return find_quartile_values(emoji_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_num_token_bins():\n",
    "    lens = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = len(s)\n",
    "        lens.append(count)\n",
    "    \n",
    "    return find_quartile_values(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_swear_bins():\n",
    "    bad_words_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_bad = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "        bad_words_counts.append(tot_bad)\n",
    "    \n",
    "    return find_quartile_values(bad_words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mention_bins():\n",
    "    mentions = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        mentions.append(count)\n",
    "    return find_quartile_values(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashtag_bins():\n",
    "    hashtag_counts = []\n",
    "    at_sum = 0\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '#' in word else 0, s))\n",
    "        hashtag_counts.append(count)\n",
    "    \n",
    "    return find_quartile_values(hashtag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_misspelling_bins():\n",
    "    misspell_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_misspelled = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() not in words_set:\n",
    "                tot_misspelled+=1\n",
    "        misspell_counts.append(tot_misspelled)\n",
    "    \n",
    "    return find_quartile_values(misspell_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emoji counts for a specific tweet\n",
    "def count_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    count = sum(map(lambda word : 1 if word in emoji_dict else 0, s))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_vocab = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_vocab = pd.read_csv('eng_lang_lexicon.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatebase_words_set = set(open(\"hatebase_terms.txt\").read().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get lists of all words from hatebase pertaining to a certain category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_religion = df_eng_vocab.loc[df_eng_vocab['is_about_religion']][['term']].values\n",
    "hb_sexual_orientation = df_eng_vocab.loc[df_eng_vocab['is_about_sexual_orientation']][['term']].values\n",
    "hb_ethnicity = df_eng_vocab.loc[df_eng_vocab['is_about_ethnicity']][['term']].values\n",
    "hb_disability = df_eng_vocab.loc[df_eng_vocab['is_about_disability']][['term']].values\n",
    "hb_social_class = df_eng_vocab.loc[df_eng_vocab['is_about_class']][['term']].values\n",
    "hb_nationality = df_eng_vocab.loc[df_eng_vocab['is_about_nationality']][['term']].values\n",
    "hb_gender = df_eng_vocab.loc[df_eng_vocab['is_about_gender']][['term']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bins contain the splits for which bin a tweet's feature counts will land in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token_bins = make_num_token_bins()\n",
    "swear_bins = make_swear_bins()\n",
    "at_bins = make_at_bins()\n",
    "hashtag_bins = make_hashtag_bins()\n",
    "emoji_bins = make_emoji_bins()\n",
    "#hatebase_words_bins = make_hatebase_bins()\n",
    "misspell_bins =  make_misspelling_bins()\n",
    "emoji_words_bins = make_emoji_bins()\n",
    "emoji_overall_sentiment_bins = make_emoji_overall_sentiment_bins()\n",
    "emoji_negative_sentiment_bins = make_emoji_negative_sentiment_bins()\n",
    "emoji_positive_sentiment_bins = make_emoji_positive_sentiment_bins()\n",
    "emoji_neutral_sentiment_bins = make_emoji_neutral_sentiment_bins()\n",
    "profanity_bins = make_profanity_bins()\n",
    "racist_bins = make_racist_lexicon_bins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_more_upper(tweet):\n",
    "    total_caps = sum(map(lambda ch : 1 if ch.isupper() else 0, tweet))\n",
    "    if total_caps > len(tweet) // 2:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_consecutive_punc(tweet):\n",
    "    for word in tweet.split():\n",
    "        if 'http://' in word: continue\n",
    "        for i in range(len(word)-1):\n",
    "            if word[i] in string.punctuation and word[i+1] in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_person = ['he', 'she', 'they', 'him', 'her', 'them', 'his', 'hers', 'their', 'theirs', 'themselves', 'himself', 'herself']\n",
    "second_person = ['you', 'your', 'yours']\n",
    "first_person =['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if first word in tweet is a third person pronoun. Also check if number of third person pronouns is greater\n",
    "# than first person pronouns\n",
    "def get_pronouns(tweet):\n",
    "    first_pronoun = 0\n",
    "    third_and_second_greater_than_first = 0\n",
    "    words = tweet.split()\n",
    "    if words[0] in third_person or words[0] in second_person: \n",
    "        first_pronoun = 1\n",
    "    \n",
    "    first_person_count = 0\n",
    "    third_second_person_count = 0\n",
    "    for word in words:\n",
    "        if word in third_person or word in second_person:\n",
    "            third_second_person_count += 1\n",
    "        elif word in first_person:\n",
    "            first_person_count += 1\n",
    "            \n",
    "    if first_person_count < third_second_person_count:\n",
    "        third_and_second_greater_than_first = 1\n",
    "       \n",
    "    return first_pronoun, third_and_second_greater_than_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite lexicon:\n",
    "    Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \n",
    "        Proceedings of the ACM SIGKDD International Conference on Knowledge \n",
    "        Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \n",
    "        Washington, USA,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = set(open(\"opinion-lexicon-English/positive-words.txt\").read().split())\n",
    "neg_words = set(open(\"opinion-lexicon-English/negative-words.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns 1, 1 if negative words outnumber positive words and there are no positive words\n",
    "def get_sentiment(tweet):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for word in tweet.split():\n",
    "        if word in pos_words:\n",
    "            pos_count += 1\n",
    "        if word in neg_words:\n",
    "            neg_count += 1\n",
    "    if neg_count > pos_count:\n",
    "        if pos_count > 0:\n",
    "            return 1, 0\n",
    "        else:\n",
    "            return 1, 1\n",
    "    else:\n",
    "        if pos_count > 0:\n",
    "            return 0, 0\n",
    "        else:\n",
    "            return 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns one if a word in the tweet has non alphanumeric characters (not including punctuation at the end of a word)\n",
    "def contains_non_alphanum(tweet):\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if not word.isalnum():\n",
    "            if word[-1] not in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most common unigrams\n",
    "unigram_counts = get_unigrams()\n",
    "top_unigrams = unigram_counts.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each count feature has four bins: 1 for < 25th percentile, 2 for < 50th percentile, \n",
    "#3 for < 75th percentile, and 4 for < 100th percentile\n",
    "def find_bin(count, bin_name):\n",
    "    for i in range(len(bin_name)):\n",
    "        if count < bin_name[i]:\n",
    "            return i + 1\n",
    "    return len(bin_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the tweets by getting their feature representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets, judgements):\n",
    "#     for word in [u[0] for u in top_unigrams]:\n",
    "#         tweets[word] = tweets['tweet'].str.contains(word).astype(int)\n",
    "    word_counts = []\n",
    "    swear_counts = []\n",
    "    at_counts = []\n",
    "    emoji_counts = []\n",
    "    \n",
    "    emoji_negative = []\n",
    "    emoji_positive = []\n",
    "    emoji_neutral = []\n",
    "    emoji_overall_sentiment = []\n",
    "    contains_emoji = []\n",
    "    \n",
    "    profanity_counts = []\n",
    "    contains_profanity = []\n",
    "    racist_counts = []\n",
    "    contains_racist = []\n",
    "    \n",
    "    contains_at = []\n",
    "    hashtag_counts = []\n",
    "    contains_hashtag = []\n",
    "    consecutive_punc = []\n",
    "    more_upper = []\n",
    "    first_pronoun = []\n",
    "    fewer_first_person = []\n",
    "    more_negative = []\n",
    "    no_positive = []\n",
    "    contains_url = []\n",
    "    not_alphanum = []\n",
    "    misspellings = []\n",
    "    disagreements = []\n",
    "    in_hatebase = []\n",
    "    about_gender = []\n",
    "    about_religion = []\n",
    "    about_sexual_orientation = []\n",
    "    about_ethnicity = []\n",
    "    about_disability = []\n",
    "    about_social_class = []\n",
    "    about_nationality = []\n",
    "\n",
    "\n",
    "    for tweet in tweets['tweet']:\n",
    "        #count tokens\n",
    "        tweet_words = tweet.split()\n",
    "        num_token_bin = find_bin(len(tweet_words), num_token_bins)\n",
    "        word_counts.append(num_token_bin)\n",
    "        \n",
    "        #emoji tokenizer\n",
    "        s = tknzr.tokenize(tweet)\n",
    "        #num_token_bin = find_bin(len(s), num_token_bins)\n",
    "        #word_counts.append(num_token_bin)\n",
    "        \n",
    "        #count swear words, misspellings, and if a word is in hatebase\n",
    "        misspell_count = 0\n",
    "        tot_bad = 0\n",
    "        tot_prof = isProfane = 0\n",
    "        tot_racist = isRacist = 0\n",
    "        hatebase = 0\n",
    "        hatebase_words = []\n",
    "        for word in tweet_words:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in profanity_banned:\n",
    "                tot_prof+=1\n",
    "                isProfane = 1\n",
    "            if word.lower() in racist_lexicon:\n",
    "                tot_racist+=1\n",
    "                isRacist = 1\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "            if word.lower() not in words_set:\n",
    "                misspell_count+=1\n",
    "            if word.lower() in hatebase_words_set:\n",
    "                hatebase = 1\n",
    "                hatebase_words.append(word)\n",
    "        swear_bin = find_bin(tot_bad, swear_bins)\n",
    "        swear_counts.append(swear_bin)\n",
    "        misspell_bin = find_bin(misspell_count, misspell_bins)\n",
    "        misspellings.append(misspell_bin)\n",
    "        in_hatebase.append(hatebase)\n",
    "        \n",
    "        racist_counts.append(tot_racist)\n",
    "        racist_bin = find_bin(tot_racist, racist_bins)\n",
    "        \n",
    "        \n",
    "        profanity_counts.append(tot_prof)\n",
    "        profanity_bin = find_bin(tot_prof, profanity_bins)\n",
    "        \n",
    "    \n",
    "        gender = religion = sexual_orientation = ethnicity = disability = social_class = nationality = 0\n",
    "        if hatebase == 1:\n",
    "            for word in hatebase_words:\n",
    "                if word in hb_gender: gender = 1\n",
    "                if word in hb_religion: religion = 1\n",
    "                if word in hb_sexual_orientation: sexual_orientation = 1\n",
    "                if word in hb_ethnicity: ethnicity = 1\n",
    "                if word in hb_disability: disability = 1\n",
    "                if word in hb_social_class: social_class = 1\n",
    "                if word in hb_nationality: nationality = 1\n",
    "        about_gender.append(gender)\n",
    "        about_religion.append(religion)\n",
    "        about_sexual_orientation.append(sexual_orientation)\n",
    "        about_ethnicity.append(ethnicity)\n",
    "        about_disability.append(disability)\n",
    "        about_social_class.append(social_class)\n",
    "        about_nationality.append(nationality)\n",
    "        contains_racist.append(isRacist)\n",
    "        contains_profanity.append(isProfane)\n",
    "        \n",
    "        #count mentions\n",
    "        at_count = tweet.count('@')\n",
    "        if at_count > 0:\n",
    "            contains_at.append(1)\n",
    "        else:\n",
    "            contains_at.append(0)\n",
    "        at_bin = find_bin(at_count, at_bins)\n",
    "        at_counts.append(at_bin)\n",
    "        \n",
    "        #count hashtags\n",
    "        hash_count = tweet.count('#')\n",
    "        if hash_count > 0:\n",
    "            contains_hashtag.append(1)\n",
    "        else:\n",
    "            contains_hashtag.append(0)\n",
    "        hash_bin = find_bin(hash_count, hashtag_bins)\n",
    "        hashtag_counts.append(hash_bin)\n",
    "        \n",
    "        #count emojis\n",
    "        emoji_count = count_emojis(tweet)\n",
    "        if emoji_count > 0:\n",
    "            contains_emoji.append(1)\n",
    "        else:\n",
    "            contains_emoji.append(0)\n",
    "        emoji_bin = find_bin(emoji_count, emoji_bins)\n",
    "        emoji_counts.append(emoji_bin)\n",
    "        \n",
    "        #emoji positive sentiment \n",
    "        emoji_positive_score = positive_sentiment_emojis(tweet) \n",
    "        emoji_positive_bin = find_bin(emoji_positive_score, emoji_bins)\n",
    "        emoji_positive.append(emoji_positive_bin)\n",
    "        \n",
    "        #emoji neutral sentiment \n",
    "        emoji_neutral_score = neutral_sentiment_emojis(tweet) \n",
    "        emoji_neutral_bin = find_bin(emoji_neutral_score, emoji_bins)\n",
    "        emoji_neutral.append(emoji_neutral_bin)\n",
    "        \n",
    "        #emoji negative sentiment \n",
    "        emoji_negative_score = negative_sentiment_emojis(tweet) \n",
    "        emoji_negative_bin = find_bin(emoji_negative_score, emoji_bins)\n",
    "        emoji_negative.append(emoji_negative_bin)\n",
    "        \n",
    "        #emoji overall sentiment \n",
    "        emoji_overall_score = overall_sentiment_emojis(tweet) \n",
    "        emoji_overall_bin = find_bin(emoji_overall_score, emoji_bins)\n",
    "        emoji_overall_sentiment.append(emoji_overall_bin)\n",
    "        \n",
    "        more_upper.append(has_more_upper(tweet))\n",
    "        consecutive_punc.append(has_consecutive_punc(tweet))\n",
    "        first, more = get_pronouns(tweet)\n",
    "        first_pronoun.append(first)\n",
    "        fewer_first_person.append(more)\n",
    "        \n",
    "        more_neg, pos = get_sentiment(tweet)\n",
    "        more_negative.append(more_neg)\n",
    "        no_positive.append(pos)\n",
    "        \n",
    "        if 'http://' in tweet:\n",
    "            contains_url.append(1)\n",
    "        else:\n",
    "            contains_url.append(0)\n",
    "            \n",
    "        not_alphanum.append(contains_non_alphanum(tweet))\n",
    "        \n",
    "        #See if there were disagreements about classification\n",
    "        i = tweets.loc[tweets['tweet']==tweet].index[0]\n",
    "        total_votes = judgements.at[i, 'count']\n",
    "        if (judgements.at[i, 'hate_speech'] == total_votes) or (judgements.at[i, 'offensive_language'] == total_votes)\\\n",
    "            or (judgements.at[i, 'neither'] == total_votes):\n",
    "            disagreements.append(0)\n",
    "        else:\n",
    "            disagreements.append(1)\n",
    "        \n",
    "    tweets['Word Counts'] = word_counts\n",
    "    tweets['Swear Counts'] = swear_counts\n",
    "    tweets['Profanity Counts'] = profanity_counts\n",
    "    tweets['Contains Profanity'] = contains_profanity\n",
    "    tweets['Racist Counts'] = racist_counts\n",
    "    tweets['Contains Racist'] = contains_racist\n",
    "    tweets['@ Counts'] = at_counts\n",
    "    tweets['Mention'] = contains_at\n",
    "    tweets['Contains Hashtag'] = contains_hashtag\n",
    "    tweets['Hashtag Counts'] = hashtag_counts\n",
    "    tweets['Emoji Counts'] = emoji_counts\n",
    "    tweets['Contains Emoji'] = contains_emoji\n",
    "    tweets['Negative Emoji'] = emoji_negative\n",
    "    tweets['Positive Emoji'] = emoji_positive\n",
    "    tweets['Neutral Emoji'] = emoji_neutral \n",
    "    tweets['Overall Emoji'] = emoji_overall_sentiment\n",
    "    tweets['Consecutive Punctuation'] = consecutive_punc\n",
    "    tweets['Majority Uppercase Letters'] = more_upper\n",
    "    tweets['First Word Second or Third Person Pronoun'] = first_pronoun\n",
    "    tweets['More Second or Third Person Pronouns than First'] = fewer_first_person\n",
    "    tweets['Majority Negative Words'] = more_negative\n",
    "    tweets['No Positive Words'] = no_positive\n",
    "    tweets['Contains URL'] = contains_url\n",
    "    tweets['Contains Non Alphanumeric Word'] = not_alphanum\n",
    "    tweets['Misspelling Count'] = misspellings\n",
    "    tweets['Judgement Disagreements'] = disagreements\n",
    "    tweets['About Gender (Hatebase)'] = about_gender\n",
    "    tweets['About Religion (Hatebase)'] = about_religion\n",
    "    tweets['About Ethnicity (Hatebase)'] = about_ethnicity\n",
    "    tweets['About Sexual Orientation (Hatebase)'] = about_sexual_orientation\n",
    "    tweets['About Disability (Hatebase)'] = about_disability\n",
    "    tweets['About Class (Hatebase)'] = about_social_class\n",
    "    tweets['About Nationality (Hatebase)'] = about_nationality\n",
    "    X = tweets[[col for col in tweets.columns if col!=\"tweet\"]]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine train and dev sets for k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, dev_data], sort=False)\n",
    "train_tweets = data[['tweet']]\n",
    "\n",
    "judgements = data[['count', 'hate_speech', 'offensive_language', 'neither']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:183: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:185: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:186: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:188: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X = process_tweets(train_tweets, judgements)\n",
    "y = data['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10-fold cross validation on combined training and dev sets on LR, SVM, and NB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(fit_intercept=True, max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "lr_y_pred = cross_val_score(LR, X.values, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n",
       "                   multi_class='ovr', n_jobs=None, penalty='l2',\n",
       "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.fit(X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:              precision    recall  f1-score   support\n",
      "\n",
      "          0       0.49      0.15      0.24      1282\n",
      "          1       0.89      0.93      0.91     17186\n",
      "          2       0.67      0.69      0.68      3746\n",
      "\n",
      "    accuracy                           0.84     22214\n",
      "   macro avg       0.69      0.59      0.61     22214\n",
      "weighted avg       0.83      0.84      0.83     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LR:\", classification_report(y, lr_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(gamma='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1282\n",
      "          1       0.89      0.93      0.91     17186\n",
      "          2       0.66      0.77      0.71      3746\n",
      "\n",
      "    accuracy                           0.85     22214\n",
      "   macro avg       0.52      0.57      0.54     22214\n",
      "weighted avg       0.80      0.85      0.83     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_y_pred = cross_val_predict(svm, X.values, y, cv=cv)\n",
    "print(classification_report(y, svm_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "    tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.17      0.78      0.27      1282\n",
      "          1       0.95      0.72      0.82     17186\n",
      "          2       0.68      0.57      0.62      3746\n",
      "\n",
      "    accuracy                           0.70     22214\n",
      "   macro avg       0.60      0.69      0.57     22214\n",
      "weighted avg       0.86      0.70      0.76     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_y_pred = cross_val_predict(gnb, X.values, y, cv=cv)\n",
    "print(classification_report(y, nb_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test/testing_data.csv', index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = test[['tweet']]\n",
    "test_judgements = test[['count', 'hate_speech', 'offensive_language', 'neither']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'http://t.co/QecHHKO42q' in emoji_sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:183: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:185: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:186: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "test_X = process_tweets(test_tweets, test_judgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LR' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-f498e4ac36b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLR_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'LR' is not defined"
     ]
    }
   ],
   "source": [
    "LR_preds = LR.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.13      0.20       142\n",
      "           1       0.90      0.93      0.91      1928\n",
      "           2       0.67      0.71      0.69       399\n",
      "\n",
      "    accuracy                           0.85      2469\n",
      "   macro avg       0.67      0.59      0.60      2469\n",
      "weighted avg       0.83      0.85      0.84      2469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, LR_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_preds = svm.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       142\n",
      "           1       0.90      0.93      0.91      1928\n",
      "           2       0.65      0.79      0.72       399\n",
      "\n",
      "    accuracy                           0.85      2469\n",
      "   macro avg       0.52      0.57      0.54      2469\n",
      "weighted avg       0.81      0.85      0.83      2469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/221/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_preds = gnb.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.70      0.27       142\n",
      "           1       0.96      0.72      0.82      1928\n",
      "           2       0.63      0.67      0.65       399\n",
      "\n",
      "    accuracy                           0.71      2469\n",
      "   macro avg       0.58      0.70      0.58      2469\n",
      "weighted avg       0.86      0.71      0.76      2469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, gnb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Error Analysis: Missed Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = data[['tweet', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missed_classifications(true, y_preds):\n",
    "    misses = np.where(np.asarray(true) != y_preds)[0]\n",
    "    missed_preds = []\n",
    "    for i in range(len(true)):\n",
    "        if np.asarray(true)[i] != y_preds[i]:\n",
    "            missed_preds.append(y_preds[i])\n",
    "\n",
    "    return misses, missed_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = data['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_missed_indices, LR_missed_preds = get_missed_classifications(true_labels, lr_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_missed = all_tweets.iloc[LR_missed_indices]\n",
    "LR_missed.loc[:,'prediction'] = LR_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_missed_indices, SVM_missed_preds = get_missed_classifications(true_labels, svm_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_missed = all_tweets.iloc[SVM_missed_indices]\n",
    "SVM_missed.loc[:,'prediction'] = SVM_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_missed_indices, NB_missed_preds = get_missed_classifications(true_labels, nb_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_missed = all_tweets.iloc[NB_missed_indices]\n",
    "NB_missed.loc[:,'prediction'] = NB_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_missed.to_csv(\"LR_missed.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_missed.to_csv(\"SVM_missed.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_missed.to_csv(\"NB_missed.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_test = test[['tweet', 'class']]\n",
    "true_y = test['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_missed_indices, LR_missed_preds = get_missed_classifications(true_y, LR_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/221/lib/python3.7/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "LR_missed = all_test.iloc[LR_missed_indices]\n",
    "LR_missed.loc[:,'prediction'] = LR_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_missed_indices, svm_missed_preds = get_missed_classifications(true_y, svm_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_missed = all_test.iloc[svm_missed_indices]\n",
    "svm_missed.loc[:,'prediction'] = svm_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_missed_indices, gnb_missed_preds = get_missed_classifications(true_y, gnb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_missed = all_test.iloc[gnb_missed_indices]\n",
    "gnb_missed.loc[:,'prediction'] = gnb_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_missed.to_csv(\"test_LR_missed.csv\", sep='\\t')\n",
    "svm_missed.to_csv(\"test_svm_missed.csv\", sep='\\t')\n",
    "gnb_missed.to_csv(\"test_gnb_missed.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22214, 33), (22214,))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, true_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['class'] = true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X['class'] = true_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to add labels back into the data, since they technically will be a node in the network\n",
    "#allData = np.hstack((X, true_labels.values.reshape(-1,1)))\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pomegranate import *\n",
    "\n",
    "#single train test split\n",
    "#Note: didn't go for k-fold CV because then the network structure/parameters would have to be learned k times,\n",
    "#which seemed too time intensive to be really useful\n",
    "\n",
    "train = X.values\n",
    "test = test_X.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's where the structure and params are learned - as it is, structure is learned with greedy algo. \n",
    "#To switch structure learning to Chow-Liu, run network = BayesianNetwork.from_samples(train, algorithm='chow-liu')\n",
    "#otherwise, if you want to learn with the greedy approach, just run BayesianNetwork.from_samples(train)\n",
    "network = BayesianNetwork.from_samples(train,algorithm=\"chow-liu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2469, 34)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each sample of the test set should be of shape (1,34) where the 34th column, corresponding to the label, \n",
    "#has the value None. That way, when we use network.predict_proba, we infer the label value given the feature values\n",
    "testNone = np.asarray([None for item in test])\n",
    "testSamples = np.asarray([item[:-1] for item in test])\n",
    "testToPredict = np.hstack((testSamples, testNone.reshape(-1,1)))\n",
    "testToPredict.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [item[-1] for item in test]\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in testToPredict:\n",
    "    \n",
    "    #gets a posterior distribution P(label | feature_1 = f1, feature_2 = f2,..., feature_n = fn)\n",
    "    dist = network.predict_proba(item)[-1]\n",
    "    \n",
    "    #gets the most likely label from that posterior distribution, which becomes our prediction\n",
    "    prediction = dist.mle()\n",
    "    #print(prediction)\n",
    "    #print(prediction)\n",
    "    y_pred.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.47      0.20      0.28       142\n",
      "           1       0.89      0.92      0.90      1928\n",
      "           2       0.65      0.67      0.66       399\n",
      "\n",
      "    accuracy                           0.84      2469\n",
      "   macro avg       0.67      0.60      0.62      2469\n",
      "weighted avg       0.82      0.84      0.83      2469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>class</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>640</th>\n",
       "      <td>\"call me Keyshawn Johnson cause im catchin err...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8207</th>\n",
       "      <td>Beer ball isn't for pussies</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1928</th>\n",
       "      <td>&amp;#8220;@salsteelo: I got 1 felony!!!&amp;#8221; I ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8294</th>\n",
       "      <td>Bitches be like niggas ain't shit ... Naw bitc...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21005</th>\n",
       "      <td>Rule number one never pursue a bitch from da o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11221</th>\n",
       "      <td>I take full pride in the ass my momma has so b...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14170</th>\n",
       "      <td>Photo: My mains @riri22barbie @georgegia yesss...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2470</th>\n",
       "      <td>@AmPowerBlog but can we all at least agree tha...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9758</th>\n",
       "      <td>Good morning Monday I have my truck money full...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5415</th>\n",
       "      <td>@ZaneIsClasker the fuck r u talking about I to...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14021</th>\n",
       "      <td>Only people not home right now are nurses, hoe...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15169</th>\n",
       "      <td>RT @CuhCuhCuh: bitches who smoke cigs are disg...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8867</th>\n",
       "      <td>Damn rednecks outside shooting guns keep wakin...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4442</th>\n",
       "      <td>@OG_HARM @moetzart @duckydynamo look at this n...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7078</th>\n",
       "      <td>@rachh_burkhart talkin bout the retard your si...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8965</th>\n",
       "      <td>Did he say wayne chung bitch ?</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19760</th>\n",
       "      <td>RT @jaylynnkoliba: @TonyJRodriguez imma get my...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3056</th>\n",
       "      <td>@DavetheBrave1 perfect smoking weather haha br...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15271</th>\n",
       "      <td>RT @DamnFoodPorn: Oreo Cookie Pancakes #FoodPo...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8342</th>\n",
       "      <td>Blood this nigga Dion retarded</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>#porn,#android,#iphone,#ipad,#sex,#xxx, | #Cre...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6005</th>\n",
       "      <td>@denytheprophecy shit so Joel went off and swo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24130</th>\n",
       "      <td>bitch niggas owe me money</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15747</th>\n",
       "      <td>RT @GrownAssMidget: I&amp;#8217;m a bitch? You&amp;#82...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>\"Maybe she wants to be more than your friend o...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4697</th>\n",
       "      <td>@RodMarley44 fr 12 was Finna bam a nicca he sa...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4338</th>\n",
       "      <td>@NFLBadwitch bitch please. Don't get shot, Dou...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12741</th>\n",
       "      <td>Like god damn you dirty bitch</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10093</th>\n",
       "      <td>His bitch ass a hoe ass nigga prolly don't get...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>@karlamorenoo niggah you dont know me! Lol you...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22114</th>\n",
       "      <td>Them pretty bitches love me them bitches love ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8787</th>\n",
       "      <td>Cruisin round the street in my 64', jockin the...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5080</th>\n",
       "      <td>@TheLovely_CC whatever lol my tan line is reta...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4678</th>\n",
       "      <td>@RihannaHasAids I had free tickets and couldn'...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19487</th>\n",
       "      <td>RT @granniewinkle: bow down to me bitches, i a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11257</th>\n",
       "      <td>I think you a fat bitch if I never see Ya body...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20997</th>\n",
       "      <td>Round #3 on you hoes #chitown #sb2014 http://t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21944</th>\n",
       "      <td>The #teabagger #freemarket really doing wonder...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24014</th>\n",
       "      <td>Zelda bitches lol @joeylattime https://t.co/Cp...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20418</th>\n",
       "      <td>RT @savethesehos: If I ride up to your bitch i...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24052</th>\n",
       "      <td>all these beautiful bitches, sucha beautiful t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6140</th>\n",
       "      <td>@firemunky Google's a bitch: \"Your search - An...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3415</th>\n",
       "      <td>@HAL3Ygang she was a dirty bitch always beggin...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20393</th>\n",
       "      <td>RT @ryannotsheckler: \"That shit can break you ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11844</th>\n",
       "      <td>If luke hemmings really thinks they're trash, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23120</th>\n",
       "      <td>What's that makeup called on your cheeks? I ca...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18363</th>\n",
       "      <td>RT @VICD713ENT: Good pussy ain't changin nothi...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19665</th>\n",
       "      <td>RT @ikesizzle: Trey Songz album is about being...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1790</th>\n",
       "      <td>&amp;#8220;@drphilDONTEhue: You know idgaf about a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10876</th>\n",
       "      <td>I just touched down on that pussy</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>\" bitch who do you love \"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7864</th>\n",
       "      <td>All these touch screen phones and hoe u still ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10788</th>\n",
       "      <td>I hate when niggas come up to me and tell me t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11164</th>\n",
       "      <td>I should gt a scurl on.deez hoes</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10734</th>\n",
       "      <td>I hate hoes who be looking all nice nshit with...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23302</th>\n",
       "      <td>Who watches wrestling besides little kids and ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>855</th>\n",
       "      <td>#honeybadger bitch</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19718</th>\n",
       "      <td>RT @itscoleharmon_: Top 3 Lies girls tell on t...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12802</th>\n",
       "      <td>Lmao RT @iPOSTBADTWEETS: Lowkey Peyton Manning...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4834</th>\n",
       "      <td>@Shitter_Gape @OptimisticDoom \\nHey...hoe di d...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2469 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          tweet  class  \\\n",
       "Unnamed: 0.1                                                             \n",
       "640           \"call me Keyshawn Johnson cause im catchin err...      1   \n",
       "8207                                Beer ball isn't for pussies      1   \n",
       "1928          &#8220;@salsteelo: I got 1 felony!!!&#8221; I ...      1   \n",
       "8294          Bitches be like niggas ain't shit ... Naw bitc...      1   \n",
       "21005         Rule number one never pursue a bitch from da o...      1   \n",
       "11221         I take full pride in the ass my momma has so b...      1   \n",
       "14170         Photo: My mains @riri22barbie @georgegia yesss...      1   \n",
       "2470          @AmPowerBlog but can we all at least agree tha...      1   \n",
       "9758          Good morning Monday I have my truck money full...      1   \n",
       "5415          @ZaneIsClasker the fuck r u talking about I to...      0   \n",
       "14021         Only people not home right now are nurses, hoe...      1   \n",
       "15169         RT @CuhCuhCuh: bitches who smoke cigs are disg...      1   \n",
       "8867          Damn rednecks outside shooting guns keep wakin...      1   \n",
       "4442          @OG_HARM @moetzart @duckydynamo look at this n...      0   \n",
       "7078          @rachh_burkhart talkin bout the retard your si...      1   \n",
       "8965                             Did he say wayne chung bitch ?      1   \n",
       "19760         RT @jaylynnkoliba: @TonyJRodriguez imma get my...      1   \n",
       "3056          @DavetheBrave1 perfect smoking weather haha br...      1   \n",
       "15271         RT @DamnFoodPorn: Oreo Cookie Pancakes #FoodPo...      2   \n",
       "8342                             Blood this nigga Dion retarded      1   \n",
       "889           #porn,#android,#iphone,#ipad,#sex,#xxx, | #Cre...      1   \n",
       "6005          @denytheprophecy shit so Joel went off and swo...      1   \n",
       "24130                                 bitch niggas owe me money      1   \n",
       "15747         RT @GrownAssMidget: I&#8217;m a bitch? You&#82...      1   \n",
       "533           \"Maybe she wants to be more than your friend o...      1   \n",
       "4697          @RodMarley44 fr 12 was Finna bam a nicca he sa...      1   \n",
       "4338          @NFLBadwitch bitch please. Don't get shot, Dou...      1   \n",
       "12741                             Like god damn you dirty bitch      1   \n",
       "10093         His bitch ass a hoe ass nigga prolly don't get...      1   \n",
       "6524          @karlamorenoo niggah you dont know me! Lol you...      1   \n",
       "...                                                         ...    ...   \n",
       "22114         Them pretty bitches love me them bitches love ...      1   \n",
       "8787          Cruisin round the street in my 64', jockin the...      1   \n",
       "5080          @TheLovely_CC whatever lol my tan line is reta...      1   \n",
       "4678          @RihannaHasAids I had free tickets and couldn'...      1   \n",
       "19487         RT @granniewinkle: bow down to me bitches, i a...      1   \n",
       "11257         I think you a fat bitch if I never see Ya body...      1   \n",
       "20997         Round #3 on you hoes #chitown #sb2014 http://t...      1   \n",
       "21944         The #teabagger #freemarket really doing wonder...      2   \n",
       "24014         Zelda bitches lol @joeylattime https://t.co/Cp...      1   \n",
       "20418         RT @savethesehos: If I ride up to your bitch i...      1   \n",
       "24052         all these beautiful bitches, sucha beautiful t...      1   \n",
       "6140          @firemunky Google's a bitch: \"Your search - An...      1   \n",
       "3415          @HAL3Ygang she was a dirty bitch always beggin...      1   \n",
       "20393         RT @ryannotsheckler: \"That shit can break you ...      1   \n",
       "11844         If luke hemmings really thinks they're trash, ...      1   \n",
       "23120         What's that makeup called on your cheeks? I ca...      1   \n",
       "18363         RT @VICD713ENT: Good pussy ain't changin nothi...      1   \n",
       "19665         RT @ikesizzle: Trey Songz album is about being...      1   \n",
       "1790          &#8220;@drphilDONTEhue: You know idgaf about a...      1   \n",
       "10876                         I just touched down on that pussy      1   \n",
       "17                                    \" bitch who do you love \"      1   \n",
       "7864          All these touch screen phones and hoe u still ...      1   \n",
       "10788         I hate when niggas come up to me and tell me t...      1   \n",
       "11164                          I should gt a scurl on.deez hoes      1   \n",
       "10734         I hate hoes who be looking all nice nshit with...      1   \n",
       "23302         Who watches wrestling besides little kids and ...      1   \n",
       "855                                          #honeybadger bitch      1   \n",
       "19718         RT @itscoleharmon_: Top 3 Lies girls tell on t...      1   \n",
       "12802         Lmao RT @iPOSTBADTWEETS: Lowkey Peyton Manning...      2   \n",
       "4834          @Shitter_Gape @OptimisticDoom \\nHey...hoe di d...      1   \n",
       "\n",
       "              prediction  \n",
       "Unnamed: 0.1              \n",
       "640                    2  \n",
       "8207                   1  \n",
       "1928                   1  \n",
       "8294                   1  \n",
       "21005                  1  \n",
       "11221                  1  \n",
       "14170                  1  \n",
       "2470                   1  \n",
       "9758                   1  \n",
       "5415                   1  \n",
       "14021                  1  \n",
       "15169                  1  \n",
       "8867                   1  \n",
       "4442                   1  \n",
       "7078                   1  \n",
       "8965                   1  \n",
       "19760                  0  \n",
       "3056                   1  \n",
       "15271                  2  \n",
       "8342                   1  \n",
       "889                    1  \n",
       "6005                   1  \n",
       "24130                  1  \n",
       "15747                  1  \n",
       "533                    1  \n",
       "4697                   2  \n",
       "4338                   1  \n",
       "12741                  1  \n",
       "10093                  1  \n",
       "6524                   1  \n",
       "...                  ...  \n",
       "22114                  1  \n",
       "8787                   1  \n",
       "5080                   1  \n",
       "4678                   1  \n",
       "19487                  1  \n",
       "11257                  1  \n",
       "20997                  1  \n",
       "21944                  2  \n",
       "24014                  1  \n",
       "20418                  1  \n",
       "24052                  1  \n",
       "6140                   2  \n",
       "3415                   1  \n",
       "20393                  1  \n",
       "11844                  1  \n",
       "23120                  1  \n",
       "18363                  1  \n",
       "19665                  1  \n",
       "1790                   1  \n",
       "10876                  1  \n",
       "17                     1  \n",
       "7864                   1  \n",
       "10788                  1  \n",
       "11164                  1  \n",
       "10734                  1  \n",
       "23302                  2  \n",
       "855                    1  \n",
       "19718                  1  \n",
       "12802                  2  \n",
       "4834                   2  \n",
       "\n",
       "[2469 rows x 3 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BN_results = all_test\n",
    "BN_results.loc[:,'prediction'] = y_pred\n",
    "BN_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred = np.loadtxt(\"CL_pred_values.txt\")\n",
    "\n",
    "BN_missed_indices, BN_missed_preds = get_missed_classifications(y_true, y_pred)\n",
    "BN_missed = test_tweets.iloc[BN_missed_indices]\n",
    "BN_missed.loc[:,'prediction'] = BN_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "BN_missed.to_csv(\"test_BN_missed.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-4e1243bd22c6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,fname,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            x = 5\n",
    "            #strr = format(cm[i, j], '.2f')\n",
    "            #ax.text(j,i,str(round(cm[i,j],2)))\n",
    "            ax.text(j, i, str(round(cm[i,j],2)),ha=\"center\", va=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fname)\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "#plot_confusion_matrix(np.asarray(y_true), np.asarray(y_pred), classes=['hate','off','nei'],\n",
    "                      #title='Normalized confusion matrix')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix bayesian network\n",
    "plot_confusion_matrix(np.asarray(y_true), np.asarray(y_pred), classes=np.asarray([0,1,2]),fname='bn.png',\n",
    "                      title='Confusion matrix, Bayesian Network',normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plot_confusion_matrix(np.asarray(y), np.asarray(nb_y_pred), classes=np.asarray([0,1,2]),fname='nb.png',\n",
    "                      title='Confusion matrix, Naive Bayes',normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(np.asarray(y), np.asarray(svm_y_pred), classes=np.asarray([0,1,2]),fname='svm.png',\n",
    "                      title='Confusion matrix, SVM',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(np.asarray(y), np.asarray(lr_y_pred), classes=np.asarray([0,1,2]),fname='lr.png',\n",
    "                      title='Confusion matrix, Logistic Regression',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the DAG structure (need graphviz, pygraphviz)\n",
    "network.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfortunate idiosyncrasy with saving: can't get the save to happen outside the exact current directory, so once\n",
    "#you make the image you should move it to misc\n",
    "network.plot('dag.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random aside: If you wanted to know the final log likelihood of the data given the learned network, log P(D | G)\n",
    "network.log_probability(train).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "221",
   "language": "python",
   "name": "221"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

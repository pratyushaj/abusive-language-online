{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "    \n",
    "TRAIN = '../data/train/training_data.csv'\n",
    "train_data = pd.read_csv(TRAIN, index_col=1)\n",
    "dev_data = pd.read_csv('../data/dev/development_data.csv', index_col=1)\n",
    "\n",
    "tweets = train_data[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "words_set = set(open(\"allwords.txt\").read().split())\n",
    "words_set = set(item.lower() for item in words_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'©': 'copyright_sign',\n",
       " '®': 'registered_sign',\n",
       " '‼': 'double_exclamation_mark',\n",
       " '⁉': 'exclamation_question_mark',\n",
       " '™': 'trade_mark_sign',\n",
       " 'ℹ': 'information_source',\n",
       " '↔': 'left_right_arrow',\n",
       " '↕': 'up_down_arrow',\n",
       " '↖': 'north_west_arrow',\n",
       " '↗': 'north_east_arrow',\n",
       " '↘': 'south_east_arrow',\n",
       " '↙': 'south_west_arrow',\n",
       " '↩': 'leftwards_arrow_with_hook',\n",
       " '↪': 'rightwards_arrow_with_hook',\n",
       " '⌚': 'watch',\n",
       " '⌛': 'hourglass',\n",
       " '⌨': 'keyboard',\n",
       " '⏏': 'eject_symbol',\n",
       " '⏩': 'black_right_pointing_double_triangle',\n",
       " '⏪': 'black_left_pointing_double_triangle',\n",
       " '⏫': 'black_up_pointing_double_triangle',\n",
       " '⏬': 'black_down_pointing_double_triangle',\n",
       " '⏭': 'black_right_pointing_double_triangle_with_vertical_bar',\n",
       " '⏮': 'black_left_pointing_double_triangle_with_vertical_bar',\n",
       " '⏯': 'black_right_pointing_triangle_with_double_vertical_bar',\n",
       " '⏰': 'alarm_clock',\n",
       " '⏱': 'stopwatch',\n",
       " '⏲': 'timer_clock',\n",
       " '⏳': 'hourglass_with_flowing_sand',\n",
       " '⏸': 'double_vertical_bar',\n",
       " '⏹': 'black_square_for_stop',\n",
       " '⏺': 'black_circle_for_record',\n",
       " 'Ⓜ': 'circled_latin_capital_letter_m',\n",
       " '▪': 'black_small_square',\n",
       " '▫': 'white_small_square',\n",
       " '▶': 'black_right_pointing_triangle',\n",
       " '◀': 'black_left_pointing_triangle',\n",
       " '◻': 'white_medium_square',\n",
       " '◼': 'black_medium_square',\n",
       " '◽': 'white_medium_small_square',\n",
       " '◾': 'black_medium_small_square',\n",
       " '☀': 'black_sun_with_rays',\n",
       " '☁': 'cloud',\n",
       " '☂': 'umbrella',\n",
       " '☃': 'snowman',\n",
       " '☄': 'comet',\n",
       " '☎': 'black_telephone',\n",
       " '☑': 'ballot_box_with_check',\n",
       " '☔': 'umbrella_with_rain_drops',\n",
       " '☕': 'hot_beverage',\n",
       " '☘': 'shamrock',\n",
       " '☝': 'white_up_pointing_index',\n",
       " '☠': 'skull_and_crossbones',\n",
       " '☢': 'radioactive_sign',\n",
       " '☣': 'biohazard_sign',\n",
       " '☦': 'orthodox_cross',\n",
       " '☪': 'star_and_crescent',\n",
       " '☮': 'peace_symbol',\n",
       " '☯': 'yin_yang',\n",
       " '☸': 'wheel_of_dharma',\n",
       " '☹': 'white_frowning_face',\n",
       " '☺': 'white_smiling_face',\n",
       " '♀': 'female_sign',\n",
       " '♂': 'male_sign',\n",
       " '♈': 'aries',\n",
       " '♉': 'taurus',\n",
       " '♊': 'gemini',\n",
       " '♋': 'cancer',\n",
       " '♌': 'leo',\n",
       " '♍': 'virgo',\n",
       " '♎': 'libra',\n",
       " '♏': 'scorpius',\n",
       " '♐': 'sagittarius',\n",
       " '♑': 'capricorn',\n",
       " '♒': 'aquarius',\n",
       " '♓': 'pisces',\n",
       " '♠': 'black_spade_suit',\n",
       " '♣': 'black_club_suit',\n",
       " '♥': 'black_heart_suit',\n",
       " '♦': 'black_diamond_suit',\n",
       " '♨': 'hot_springs',\n",
       " '♻': 'black_universal_recycling_symbol',\n",
       " '♿': 'wheelchair_symbol',\n",
       " '⚒': 'hammer_and_pick',\n",
       " '⚓': 'anchor',\n",
       " '⚔': 'crossed_swords',\n",
       " '⚕': 'staff_of_aesculapius',\n",
       " '⚖': 'scales',\n",
       " '⚗': 'alembic',\n",
       " '⚙': 'gear',\n",
       " '⚛': 'atom_symbol',\n",
       " '⚜': 'fleur_de_lis',\n",
       " '⚠': 'warning_sign',\n",
       " '⚡': 'high_voltage_sign',\n",
       " '⚪': 'medium_white_circle',\n",
       " '⚫': 'medium_black_circle',\n",
       " '⚰': 'coffin',\n",
       " '⚱': 'funeral_urn',\n",
       " '⚽': 'soccer_ball',\n",
       " '⚾': 'baseball',\n",
       " '⛄': 'snowman_without_snow',\n",
       " '⛅': 'sun_behind_cloud',\n",
       " '⛈': 'thunder_cloud_and_rain',\n",
       " '⛎': 'ophiuchus',\n",
       " '⛏': 'pick',\n",
       " '⛑': 'helmet_with_white_cross',\n",
       " '⛓': 'chains',\n",
       " '⛔': 'no_entry',\n",
       " '⛩': 'shinto_shrine',\n",
       " '⛪': 'church',\n",
       " '⛰': 'mountain',\n",
       " '⛱': 'umbrella_on_ground',\n",
       " '⛲': 'fountain',\n",
       " '⛳': 'flag_in_hole',\n",
       " '⛴': 'ferry',\n",
       " '⛵': 'sailboat',\n",
       " '⛷': 'skier',\n",
       " '⛸': 'ice_skate',\n",
       " '⛹': 'person_with_ball',\n",
       " '⛺': 'tent',\n",
       " '⛽': 'fuel_pump',\n",
       " '✂': 'black_scissors',\n",
       " '✅': 'white_heavy_check_mark',\n",
       " '✈': 'airplane',\n",
       " '✉': 'envelope',\n",
       " '✊': 'raised_fist',\n",
       " '✋': 'raised_hand',\n",
       " '✌': 'victory_hand',\n",
       " '✍': 'writing_hand',\n",
       " '✏': 'pencil',\n",
       " '✒': 'black_nib',\n",
       " '✔': 'heavy_check_mark',\n",
       " '✖': 'heavy_multiplication_x',\n",
       " '✝': 'latin_cross',\n",
       " '✡': 'star_of_david',\n",
       " '✨': 'sparkles',\n",
       " '✳': 'eight_spoked_asterisk',\n",
       " '✴': 'eight_pointed_black_star',\n",
       " '❄': 'snowflake',\n",
       " '❇': 'sparkle',\n",
       " '❌': 'cross_mark',\n",
       " '❎': 'negative_squared_cross_mark',\n",
       " '❓': 'black_question_mark_ornament',\n",
       " '❔': 'white_question_mark_ornament',\n",
       " '❕': 'white_exclamation_mark_ornament',\n",
       " '❗': 'heavy_exclamation_mark_symbol',\n",
       " '❣': 'heavy_heart_exclamation_mark_ornament',\n",
       " '❤': 'heavy_black_heart',\n",
       " '➕': 'heavy_plus_sign',\n",
       " '➖': 'heavy_minus_sign',\n",
       " '➗': 'heavy_division_sign',\n",
       " '➡': 'black_rightwards_arrow',\n",
       " '➰': 'curly_loop',\n",
       " '➿': 'double_curly_loop',\n",
       " '⤴': 'arrow_pointing_rightwards_then_curving_upwards',\n",
       " '⤵': 'arrow_pointing_rightwards_then_curving_downwards',\n",
       " '⬅': 'leftwards_black_arrow',\n",
       " '⬆': 'upwards_black_arrow',\n",
       " '⬇': 'downwards_black_arrow',\n",
       " '⬛': 'black_large_square',\n",
       " '⬜': 'white_large_square',\n",
       " '⭐': 'white_medium_star',\n",
       " '⭕': 'heavy_large_circle',\n",
       " '〰': 'wavy_dash',\n",
       " '〽': 'part_alternation_mark',\n",
       " '㊗': 'circled_ideograph_congratulation',\n",
       " '㊙': 'circled_ideograph_secret',\n",
       " '🀄': 'mahjong_tile_red_dragon',\n",
       " '🃏': 'playing_card_black_joker',\n",
       " '🅰': 'negative_squared_latin_capital_letter_a',\n",
       " '🅱': 'negative_squared_latin_capital_letter_b',\n",
       " '🅾': 'negative_squared_latin_capital_letter_o',\n",
       " '🅿': 'negative_squared_latin_capital_letter_p',\n",
       " '🆎': 'negative_squared_ab',\n",
       " '🆑': 'squared_cl',\n",
       " '🆒': 'squared_cool',\n",
       " '🆓': 'squared_free',\n",
       " '🆔': 'squared_id',\n",
       " '🆕': 'squared_new',\n",
       " '🆖': 'squared_ng',\n",
       " '🆗': 'squared_ok',\n",
       " '🆘': 'squared_sos',\n",
       " '🆙': 'squared_up_with_exclamation_mark',\n",
       " '🆚': 'squared_vs',\n",
       " '🈁': 'squared_katakana_koko',\n",
       " '🈂': 'squared_katakana_sa',\n",
       " '🈚': 'squared_cjk_unified_ideograph_7121',\n",
       " '🈯': 'squared_cjk_unified_ideograph_6307',\n",
       " '🈲': 'squared_cjk_unified_ideograph_7981',\n",
       " '🈳': 'squared_cjk_unified_ideograph_7a7a',\n",
       " '🈴': 'squared_cjk_unified_ideograph_5408',\n",
       " '🈵': 'squared_cjk_unified_ideograph_6e80',\n",
       " '🈶': 'squared_cjk_unified_ideograph_6709',\n",
       " '🈷': 'squared_cjk_unified_ideograph_6708',\n",
       " '🈸': 'squared_cjk_unified_ideograph_7533',\n",
       " '🈹': 'squared_cjk_unified_ideograph_5272',\n",
       " '🈺': 'squared_cjk_unified_ideograph_55b6',\n",
       " '🉐': 'circled_ideograph_advantage',\n",
       " '🉑': 'circled_ideograph_accept',\n",
       " '🌀': 'cyclone',\n",
       " '🌁': 'foggy',\n",
       " '🌂': 'closed_umbrella',\n",
       " '🌃': 'night_with_stars',\n",
       " '🌄': 'sunrise_over_mountains',\n",
       " '🌅': 'sunrise',\n",
       " '🌆': 'cityscape_at_dusk',\n",
       " '🌇': 'sunset_over_buildings',\n",
       " '🌈': 'rainbow',\n",
       " '🌉': 'bridge_at_night',\n",
       " '🌊': 'water_wave',\n",
       " '🌋': 'volcano',\n",
       " '🌌': 'milky_way',\n",
       " '🌍': 'earth_globe_europe_africa',\n",
       " '🌎': 'earth_globe_americas',\n",
       " '🌏': 'earth_globe_asia_australia',\n",
       " '🌐': 'globe_with_meridians',\n",
       " '🌑': 'new_moon_symbol',\n",
       " '🌒': 'waxing_crescent_moon_symbol',\n",
       " '🌓': 'first_quarter_moon_symbol',\n",
       " '🌔': 'waxing_gibbous_moon_symbol',\n",
       " '🌕': 'full_moon_symbol',\n",
       " '🌖': 'waning_gibbous_moon_symbol',\n",
       " '🌗': 'last_quarter_moon_symbol',\n",
       " '🌘': 'waning_crescent_moon_symbol',\n",
       " '🌙': 'crescent_moon',\n",
       " '🌚': 'new_moon_with_face',\n",
       " '🌛': 'first_quarter_moon_with_face',\n",
       " '🌜': 'last_quarter_moon_with_face',\n",
       " '🌝': 'full_moon_with_face',\n",
       " '🌞': 'sun_with_face',\n",
       " '🌟': 'glowing_star',\n",
       " '🌠': 'shooting_star',\n",
       " '🌡': 'thermometer',\n",
       " '🌤': 'white_sun_with_small_cloud',\n",
       " '🌥': 'white_sun_behind_cloud',\n",
       " '🌦': 'white_sun_behind_cloud_with_rain',\n",
       " '🌧': 'cloud_with_rain',\n",
       " '🌨': 'cloud_with_snow',\n",
       " '🌩': 'cloud_with_lightning',\n",
       " '🌪': 'cloud_with_tornado',\n",
       " '🌫': 'fog',\n",
       " '🌬': 'wind_blowing_face',\n",
       " '🌭': 'hot_dog',\n",
       " '🌮': 'taco',\n",
       " '🌯': 'burrito',\n",
       " '🌰': 'chestnut',\n",
       " '🌱': 'seedling',\n",
       " '🌲': 'evergreen_tree',\n",
       " '🌳': 'deciduous_tree',\n",
       " '🌴': 'palm_tree',\n",
       " '🌵': 'cactus',\n",
       " '🌶': 'hot_pepper',\n",
       " '🌷': 'tulip',\n",
       " '🌸': 'cherry_blossom',\n",
       " '🌹': 'rose',\n",
       " '🌺': 'hibiscus',\n",
       " '🌻': 'sunflower',\n",
       " '🌼': 'blossom',\n",
       " '🌽': 'ear_of_maize',\n",
       " '🌾': 'ear_of_rice',\n",
       " '🌿': 'herb',\n",
       " '🍀': 'four_leaf_clover',\n",
       " '🍁': 'maple_leaf',\n",
       " '🍂': 'fallen_leaf',\n",
       " '🍃': 'leaf_fluttering_in_wind',\n",
       " '🍄': 'mushroom',\n",
       " '🍅': 'tomato',\n",
       " '🍆': 'aubergine',\n",
       " '🍇': 'grapes',\n",
       " '🍈': 'melon',\n",
       " '🍉': 'watermelon',\n",
       " '🍊': 'tangerine',\n",
       " '🍋': 'lemon',\n",
       " '🍌': 'banana',\n",
       " '🍍': 'pineapple',\n",
       " '🍎': 'red_apple',\n",
       " '🍏': 'green_apple',\n",
       " '🍐': 'pear',\n",
       " '🍑': 'peach',\n",
       " '🍒': 'cherries',\n",
       " '🍓': 'strawberry',\n",
       " '🍔': 'hamburger',\n",
       " '🍕': 'slice_of_pizza',\n",
       " '🍖': 'meat_on_bone',\n",
       " '🍗': 'poultry_leg',\n",
       " '🍘': 'rice_cracker',\n",
       " '🍙': 'rice_ball',\n",
       " '🍚': 'cooked_rice',\n",
       " '🍛': 'curry_and_rice',\n",
       " '🍜': 'steaming_bowl',\n",
       " '🍝': 'spaghetti',\n",
       " '🍞': 'bread',\n",
       " '🍟': 'french_fries',\n",
       " '🍠': 'roasted_sweet_potato',\n",
       " '🍡': 'dango',\n",
       " '🍢': 'oden',\n",
       " '🍣': 'sushi',\n",
       " '🍤': 'fried_shrimp',\n",
       " '🍥': 'fish_cake_with_swirl_design',\n",
       " '🍦': 'soft_ice_cream',\n",
       " '🍧': 'shaved_ice',\n",
       " '🍨': 'ice_cream',\n",
       " '🍩': 'doughnut',\n",
       " '🍪': 'cookie',\n",
       " '🍫': 'chocolate_bar',\n",
       " '🍬': 'candy',\n",
       " '🍭': 'lollipop',\n",
       " '🍮': 'custard',\n",
       " '🍯': 'honey_pot',\n",
       " '🍰': 'shortcake',\n",
       " '🍱': 'bento_box',\n",
       " '🍲': 'pot_of_food',\n",
       " '🍳': 'cooking',\n",
       " '🍴': 'fork_and_knife',\n",
       " '🍵': 'teacup_without_handle',\n",
       " '🍶': 'sake_bottle_and_cup',\n",
       " '🍷': 'wine_glass',\n",
       " '🍸': 'cocktail_glass',\n",
       " '🍹': 'tropical_drink',\n",
       " '🍺': 'beer_mug',\n",
       " '🍻': 'clinking_beer_mugs',\n",
       " '🍼': 'baby_bottle',\n",
       " '🍽': 'fork_and_knife_with_plate',\n",
       " '🍾': 'bottle_with_popping_cork',\n",
       " '🍿': 'popcorn',\n",
       " '🎀': 'ribbon',\n",
       " '🎁': 'wrapped_present',\n",
       " '🎂': 'birthday_cake',\n",
       " '🎃': 'jack_o_lantern',\n",
       " '🎄': 'christmas_tree',\n",
       " '🎅': 'father_christmas',\n",
       " '🎆': 'fireworks',\n",
       " '🎇': 'firework_sparkler',\n",
       " '🎈': 'balloon',\n",
       " '🎉': 'party_popper',\n",
       " '🎊': 'confetti_ball',\n",
       " '🎋': 'tanabata_tree',\n",
       " '🎌': 'crossed_flags',\n",
       " '🎍': 'pine_decoration',\n",
       " '🎎': 'japanese_dolls',\n",
       " '🎏': 'carp_streamer',\n",
       " '🎐': 'wind_chime',\n",
       " '🎑': 'moon_viewing_ceremony',\n",
       " '🎒': 'school_satchel',\n",
       " '🎓': 'graduation_cap',\n",
       " '🎖': 'military_medal',\n",
       " '🎗': 'reminder_ribbon',\n",
       " '🎙': 'studio_microphone',\n",
       " '🎚': 'level_slider',\n",
       " '🎛': 'control_knobs',\n",
       " '🎞': 'film_frames',\n",
       " '🎟': 'admission_tickets',\n",
       " '🎠': 'carousel_horse',\n",
       " '🎡': 'ferris_wheel',\n",
       " '🎢': 'roller_coaster',\n",
       " '🎣': 'fishing_pole_and_fish',\n",
       " '🎤': 'microphone',\n",
       " '🎥': 'movie_camera',\n",
       " '🎦': 'cinema',\n",
       " '🎧': 'headphone',\n",
       " '🎨': 'artist_palette',\n",
       " '🎩': 'top_hat',\n",
       " '🎪': 'circus_tent',\n",
       " '🎫': 'ticket',\n",
       " '🎬': 'clapper_board',\n",
       " '🎭': 'performing_arts',\n",
       " '🎮': 'video_game',\n",
       " '🎯': 'direct_hit',\n",
       " '🎰': 'slot_machine',\n",
       " '🎱': 'billiards',\n",
       " '🎲': 'game_die',\n",
       " '🎳': 'bowling',\n",
       " '🎴': 'flower_playing_cards',\n",
       " '🎵': 'musical_note',\n",
       " '🎶': 'multiple_musical_notes',\n",
       " '🎷': 'saxophone',\n",
       " '🎸': 'guitar',\n",
       " '🎹': 'musical_keyboard',\n",
       " '🎺': 'trumpet',\n",
       " '🎻': 'violin',\n",
       " '🎼': 'musical_score',\n",
       " '🎽': 'running_shirt_with_sash',\n",
       " '🎾': 'tennis_racquet_and_ball',\n",
       " '🎿': 'ski_and_ski_boot',\n",
       " '🏀': 'basketball_and_hoop',\n",
       " '🏁': 'chequered_flag',\n",
       " '🏂': 'snowboarder',\n",
       " '🏃': 'runner',\n",
       " '🏄': 'surfer',\n",
       " '🏅': 'sports_medal',\n",
       " '🏆': 'trophy',\n",
       " '🏇': 'horse_racing',\n",
       " '🏈': 'american_football',\n",
       " '🏉': 'rugby_football',\n",
       " '🏊': 'swimmer',\n",
       " '🏋': 'weight_lifter',\n",
       " '🏌': 'golfer',\n",
       " '🏍': 'racing_motorcycle',\n",
       " '🏎': 'racing_car',\n",
       " '🏏': 'cricket_bat_and_ball',\n",
       " '🏐': 'volleyball',\n",
       " '🏑': 'field_hockey_stick_and_ball',\n",
       " '🏒': 'ice_hockey_stick_and_puck',\n",
       " '🏓': 'table_tennis_paddle_and_ball',\n",
       " '🏔': 'snow_capped_mountain',\n",
       " '🏕': 'camping',\n",
       " '🏖': 'beach_with_umbrella',\n",
       " '🏗': 'building_construction',\n",
       " '🏘': 'house_buildings',\n",
       " '🏙': 'cityscape',\n",
       " '🏚': 'derelict_house_building',\n",
       " '🏛': 'classical_building',\n",
       " '🏜': 'desert',\n",
       " '🏝': 'desert_island',\n",
       " '🏞': 'national_park',\n",
       " '🏟': 'stadium',\n",
       " '🏠': 'house_building',\n",
       " '🏡': 'house_with_garden',\n",
       " '🏢': 'office_building',\n",
       " '🏣': 'japanese_post_office',\n",
       " '🏤': 'european_post_office',\n",
       " '🏥': 'hospital',\n",
       " '🏦': 'bank',\n",
       " '🏧': 'automated_teller_machine',\n",
       " '🏨': 'hotel',\n",
       " '🏩': 'love_hotel',\n",
       " '🏪': 'convenience_store',\n",
       " '🏫': 'school',\n",
       " '🏬': 'department_store',\n",
       " '🏭': 'factory',\n",
       " '🏮': 'izakaya_lantern',\n",
       " '🏯': 'japanese_castle',\n",
       " '🏰': 'european_castle',\n",
       " '🏳': 'waving_white_flag',\n",
       " '🏴': 'waving_black_flag',\n",
       " '🏵': 'rosette',\n",
       " '🏷': 'label',\n",
       " '🏸': 'badminton_racquet_and_shuttlecock',\n",
       " '🏹': 'bow_and_arrow',\n",
       " '🏺': 'amphora',\n",
       " '🏻': 'emoji_modifier_fitzpatrick_type_1_2',\n",
       " '🏼': 'emoji_modifier_fitzpatrick_type_3',\n",
       " '🏽': 'emoji_modifier_fitzpatrick_type_4',\n",
       " '🏾': 'emoji_modifier_fitzpatrick_type_5',\n",
       " '🏿': 'emoji_modifier_fitzpatrick_type_6',\n",
       " '🐀': 'rat',\n",
       " '🐁': 'mouse',\n",
       " '🐂': 'ox',\n",
       " '🐃': 'water_buffalo',\n",
       " '🐄': 'cow',\n",
       " '🐅': 'tiger',\n",
       " '🐆': 'leopard',\n",
       " '🐇': 'rabbit',\n",
       " '🐈': 'cat',\n",
       " '🐉': 'dragon',\n",
       " '🐊': 'crocodile',\n",
       " '🐋': 'whale',\n",
       " '🐌': 'snail',\n",
       " '🐍': 'snake',\n",
       " '🐎': 'horse',\n",
       " '🐏': 'ram',\n",
       " '🐐': 'goat',\n",
       " '🐑': 'sheep',\n",
       " '🐒': 'monkey',\n",
       " '🐓': 'rooster',\n",
       " '🐔': 'chicken',\n",
       " '🐕': 'dog',\n",
       " '🐖': 'pig',\n",
       " '🐗': 'boar',\n",
       " '🐘': 'elephant',\n",
       " '🐙': 'octopus',\n",
       " '🐚': 'spiral_shell',\n",
       " '🐛': 'bug',\n",
       " '🐜': 'ant',\n",
       " '🐝': 'honeybee',\n",
       " '🐞': 'lady_beetle',\n",
       " '🐟': 'fish',\n",
       " '🐠': 'tropical_fish',\n",
       " '🐡': 'blowfish',\n",
       " '🐢': 'turtle',\n",
       " '🐣': 'hatching_chick',\n",
       " '🐤': 'baby_chick',\n",
       " '🐥': 'front_facing_baby_chick',\n",
       " '🐦': 'bird',\n",
       " '🐧': 'penguin',\n",
       " '🐨': 'koala',\n",
       " '🐩': 'poodle',\n",
       " '🐪': 'dromedary_camel',\n",
       " '🐫': 'bactrian_camel',\n",
       " '🐬': 'dolphin',\n",
       " '🐭': 'mouse_face',\n",
       " '🐮': 'cow_face',\n",
       " '🐯': 'tiger_face',\n",
       " '🐰': 'rabbit_face',\n",
       " '🐱': 'cat_face',\n",
       " '🐲': 'dragon_face',\n",
       " '🐳': 'spouting_whale',\n",
       " '🐴': 'horse_face',\n",
       " '🐵': 'monkey_face',\n",
       " '🐶': 'dog_face',\n",
       " '🐷': 'pig_face',\n",
       " '🐸': 'frog_face',\n",
       " '🐹': 'hamster_face',\n",
       " '🐺': 'wolf_face',\n",
       " '🐻': 'bear_face',\n",
       " '🐼': 'panda_face',\n",
       " '🐽': 'pig_nose',\n",
       " '🐾': 'paw_prints',\n",
       " '🐿': 'chipmunk',\n",
       " '👀': 'eyes',\n",
       " '👁': 'eye',\n",
       " '👂': 'ear',\n",
       " '👃': 'nose',\n",
       " '👄': 'mouth',\n",
       " '👅': 'tongue',\n",
       " '👆': 'white_up_pointing_backhand_index',\n",
       " '👇': 'white_down_pointing_backhand_index',\n",
       " '👈': 'white_left_pointing_backhand_index',\n",
       " '👉': 'white_right_pointing_backhand_index',\n",
       " '👊': 'fisted_hand_sign',\n",
       " '👋': 'waving_hand_sign',\n",
       " '👌': 'ok_hand_sign',\n",
       " '👍': 'thumbs_up_sign',\n",
       " '👎': 'thumbs_down_sign',\n",
       " '👏': 'clapping_hands_sign',\n",
       " '👐': 'open_hands_sign',\n",
       " '👑': 'crown',\n",
       " '👒': 'womans_hat',\n",
       " '👓': 'eyeglasses',\n",
       " '👔': 'necktie',\n",
       " '👕': 't_shirt',\n",
       " '👖': 'jeans',\n",
       " '👗': 'dress',\n",
       " '👘': 'kimono',\n",
       " '👙': 'bikini',\n",
       " '👚': 'womans_clothes',\n",
       " '👛': 'purse',\n",
       " '👜': 'handbag',\n",
       " '👝': 'pouch',\n",
       " '👞': 'mans_shoe',\n",
       " '👟': 'athletic_shoe',\n",
       " '👠': 'high_heeled_shoe',\n",
       " '👡': 'womans_sandal',\n",
       " '👢': 'womans_boots',\n",
       " '👣': 'footprints',\n",
       " '👤': 'bust_in_silhouette',\n",
       " '👥': 'busts_in_silhouette',\n",
       " '👦': 'boy',\n",
       " '👧': 'girl',\n",
       " '👨': 'man',\n",
       " '👩': 'woman',\n",
       " '👪': 'family',\n",
       " '👫': 'man_and_woman_holding_hands',\n",
       " '👬': 'two_men_holding_hands',\n",
       " '👭': 'two_women_holding_hands',\n",
       " '👮': 'police_officer',\n",
       " '👯': 'woman_with_bunny_ears',\n",
       " '👰': 'bride_with_veil',\n",
       " '👱': 'person_with_blond_hair',\n",
       " '👲': 'man_with_gua_pi_mao',\n",
       " '👳': 'man_with_turban',\n",
       " '👴': 'older_man',\n",
       " '👵': 'older_woman',\n",
       " '👶': 'baby',\n",
       " '👷': 'construction_worker',\n",
       " '👸': 'princess',\n",
       " '👹': 'japanese_ogre',\n",
       " '👺': 'japanese_goblin',\n",
       " '👻': 'ghost',\n",
       " '👼': 'baby_angel',\n",
       " '👽': 'extraterrestrial_alien',\n",
       " '👾': 'alien_monster',\n",
       " '👿': 'imp',\n",
       " '💀': 'skull',\n",
       " '💁': 'information_desk_person',\n",
       " '💂': 'guardsman',\n",
       " '💃': 'dancer',\n",
       " '💄': 'lipstick',\n",
       " '💅': 'nail_polish',\n",
       " '💆': 'face_massage',\n",
       " '💇': 'haircut',\n",
       " '💈': 'barber_pole',\n",
       " '💉': 'syringe',\n",
       " '💊': 'pill',\n",
       " '💋': 'kiss_mark',\n",
       " '💌': 'love_letter',\n",
       " '💍': 'ring',\n",
       " '💎': 'gem_stone',\n",
       " '💏': 'kiss',\n",
       " '💐': 'bouquet',\n",
       " '💑': 'couple_with_heart',\n",
       " '💒': 'wedding',\n",
       " '💓': 'beating_heart',\n",
       " '💔': 'broken_heart',\n",
       " '💕': 'two_hearts',\n",
       " '💖': 'sparkling_heart',\n",
       " '💗': 'growing_heart',\n",
       " '💘': 'heart_with_arrow',\n",
       " '💙': 'blue_heart',\n",
       " '💚': 'green_heart',\n",
       " '💛': 'yellow_heart',\n",
       " '💜': 'purple_heart',\n",
       " '💝': 'heart_with_ribbon',\n",
       " '💞': 'revolving_hearts',\n",
       " '💟': 'heart_decoration',\n",
       " '💠': 'diamond_shape_with_a_dot_inside',\n",
       " '💡': 'electric_light_bulb',\n",
       " '💢': 'anger_symbol',\n",
       " '💣': 'bomb',\n",
       " '💤': 'sleeping_symbol',\n",
       " '💥': 'collision_symbol',\n",
       " '💦': 'splashing_sweat_symbol',\n",
       " '💧': 'droplet',\n",
       " '💨': 'dash_symbol',\n",
       " '💩': 'pile_of_poo',\n",
       " '💪': 'flexed_biceps',\n",
       " '💫': 'dizzy_symbol',\n",
       " '💬': 'speech_balloon',\n",
       " '💭': 'thought_balloon',\n",
       " '💮': 'white_flower',\n",
       " '💯': 'hundred_points_symbol',\n",
       " '💰': 'money_bag',\n",
       " '💱': 'currency_exchange',\n",
       " '💲': 'heavy_dollar_sign',\n",
       " '💳': 'credit_card',\n",
       " '💴': 'banknote_with_yen_sign',\n",
       " '💵': 'banknote_with_dollar_sign',\n",
       " '💶': 'banknote_with_euro_sign',\n",
       " '💷': 'banknote_with_pound_sign',\n",
       " '💸': 'money_with_wings',\n",
       " '💹': 'chart_with_upwards_trend_and_yen_sign',\n",
       " '💺': 'seat',\n",
       " '💻': 'personal_computer',\n",
       " '💼': 'briefcase',\n",
       " '💽': 'minidisc',\n",
       " '💾': 'floppy_disk',\n",
       " '💿': 'optical_disc',\n",
       " '📀': 'dvd',\n",
       " '📁': 'file_folder',\n",
       " '📂': 'open_file_folder',\n",
       " '📃': 'page_with_curl',\n",
       " '📄': 'page_facing_up',\n",
       " '📅': 'calendar',\n",
       " '📆': 'tear_off_calendar',\n",
       " '📇': 'card_index',\n",
       " '📈': 'chart_with_upwards_trend',\n",
       " '📉': 'chart_with_downwards_trend',\n",
       " '📊': 'bar_chart',\n",
       " '📋': 'clipboard',\n",
       " '📌': 'pushpin',\n",
       " '📍': 'round_pushpin',\n",
       " '📎': 'paperclip',\n",
       " '📏': 'straight_ruler',\n",
       " '📐': 'triangular_ruler',\n",
       " '📑': 'bookmark_tabs',\n",
       " '📒': 'ledger',\n",
       " '📓': 'notebook',\n",
       " '📔': 'notebook_with_decorative_cover',\n",
       " '📕': 'closed_book',\n",
       " '📖': 'open_book',\n",
       " '📗': 'green_book',\n",
       " '📘': 'blue_book',\n",
       " '📙': 'orange_book',\n",
       " '📚': 'books',\n",
       " '📛': 'name_badge',\n",
       " '📜': 'scroll',\n",
       " '📝': 'memo',\n",
       " '📞': 'telephone_receiver',\n",
       " '📟': 'pager',\n",
       " '📠': 'fax_machine',\n",
       " '📡': 'satellite_antenna',\n",
       " '📢': 'public_address_loudspeaker',\n",
       " '📣': 'cheering_megaphone',\n",
       " '📤': 'outbox_tray',\n",
       " '📥': 'inbox_tray',\n",
       " '📦': 'package',\n",
       " '📧': 'e_mail_symbol',\n",
       " '📨': 'incoming_envelope',\n",
       " '📩': 'envelope_with_downwards_arrow_above',\n",
       " '📪': 'closed_mailbox_with_lowered_flag',\n",
       " '📫': 'closed_mailbox_with_raised_flag',\n",
       " '📬': 'open_mailbox_with_raised_flag',\n",
       " '📭': 'open_mailbox_with_lowered_flag',\n",
       " '📮': 'postbox',\n",
       " '📯': 'postal_horn',\n",
       " '📰': 'newspaper',\n",
       " '📱': 'mobile_phone',\n",
       " '📲': 'mobile_phone_with_rightwards_arrow_at_left',\n",
       " '📳': 'vibration_mode',\n",
       " '📴': 'mobile_phone_off',\n",
       " '📵': 'no_mobile_phones',\n",
       " '📶': 'antenna_with_bars',\n",
       " '📷': 'camera',\n",
       " '📸': 'camera_with_flash',\n",
       " '📹': 'video_camera',\n",
       " '📺': 'television',\n",
       " '📻': 'radio',\n",
       " '📼': 'videocassette',\n",
       " '📽': 'film_projector',\n",
       " '📿': 'prayer_beads',\n",
       " '🔀': 'twisted_rightwards_arrows',\n",
       " '🔁': 'clockwise_rightwards_and_leftwards_open_circle_arrows',\n",
       " '🔂': 'clockwise_rightwards_and_leftwards_open_circle_arrows_with_circled_one_overlay',\n",
       " '🔃': 'clockwise_downwards_and_upwards_open_circle_arrows',\n",
       " '🔄': 'anticlockwise_downwards_and_upwards_open_circle_arrows',\n",
       " '🔅': 'low_brightness_symbol',\n",
       " '🔆': 'high_brightness_symbol',\n",
       " '🔇': 'speaker_with_cancellation_stroke',\n",
       " '🔈': 'speaker',\n",
       " '🔉': 'speaker_with_one_sound_wave',\n",
       " '🔊': 'speaker_with_three_sound_waves',\n",
       " '🔋': 'battery',\n",
       " '🔌': 'electric_plug',\n",
       " '🔍': 'left_pointing_magnifying_glass',\n",
       " '🔎': 'right_pointing_magnifying_glass',\n",
       " '🔏': 'lock_with_ink_pen',\n",
       " '🔐': 'closed_lock_with_key',\n",
       " '🔑': 'key',\n",
       " '🔒': 'lock',\n",
       " '🔓': 'open_lock',\n",
       " '🔔': 'bell',\n",
       " '🔕': 'bell_with_cancellation_stroke',\n",
       " '🔖': 'bookmark',\n",
       " '🔗': 'link_symbol',\n",
       " '🔘': 'radio_button',\n",
       " '🔙': 'back_with_leftwards_arrow_above',\n",
       " '🔚': 'end_with_leftwards_arrow_above',\n",
       " '🔛': 'on_with_exclamation_mark_with_left_right_arrow_above',\n",
       " '🔜': 'soon_with_rightwards_arrow_above',\n",
       " '🔝': 'top_with_upwards_arrow_above',\n",
       " '🔞': 'no_one_under_eighteen_symbol',\n",
       " '🔟': 'keycap_ten',\n",
       " '🔠': 'input_symbol_for_latin_capital_letters',\n",
       " '🔡': 'input_symbol_for_latin_small_letters',\n",
       " '🔢': 'input_symbol_for_numbers',\n",
       " '🔣': 'input_symbol_for_symbols',\n",
       " '🔤': 'input_symbol_for_latin_letters',\n",
       " '🔥': 'fire',\n",
       " '🔦': 'electric_torch',\n",
       " '🔧': 'wrench',\n",
       " '🔨': 'hammer',\n",
       " '🔩': 'nut_and_bolt',\n",
       " '🔪': 'hocho',\n",
       " '🔫': 'pistol',\n",
       " '🔬': 'microscope',\n",
       " '🔭': 'telescope',\n",
       " '🔮': 'crystal_ball',\n",
       " '🔯': 'six_pointed_star_with_middle_dot',\n",
       " '🔰': 'japanese_symbol_for_beginner',\n",
       " '🔱': 'trident_emblem',\n",
       " '🔲': 'black_square_button',\n",
       " '🔳': 'white_square_button',\n",
       " '🔴': 'large_red_circle',\n",
       " '🔵': 'large_blue_circle',\n",
       " '🔶': 'large_orange_diamond',\n",
       " '🔷': 'large_blue_diamond',\n",
       " '🔸': 'small_orange_diamond',\n",
       " '🔹': 'small_blue_diamond',\n",
       " '🔺': 'up_pointing_red_triangle',\n",
       " '🔻': 'down_pointing_red_triangle',\n",
       " '🔼': 'up_pointing_small_red_triangle',\n",
       " '🔽': 'down_pointing_small_red_triangle',\n",
       " '🕉': 'om_symbol',\n",
       " '🕊': 'dove_of_peace',\n",
       " '🕋': 'kaaba',\n",
       " '🕌': 'mosque',\n",
       " '🕍': 'synagogue',\n",
       " '🕎': 'menorah_with_nine_branches',\n",
       " '🕐': 'clock_face_one_oclock',\n",
       " '🕑': 'clock_face_two_oclock',\n",
       " '🕒': 'clock_face_three_oclock',\n",
       " '🕓': 'clock_face_four_oclock',\n",
       " '🕔': 'clock_face_five_oclock',\n",
       " '🕕': 'clock_face_six_oclock',\n",
       " '🕖': 'clock_face_seven_oclock',\n",
       " '🕗': 'clock_face_eight_oclock',\n",
       " '🕘': 'clock_face_nine_oclock',\n",
       " '🕙': 'clock_face_ten_oclock',\n",
       " '🕚': 'clock_face_eleven_oclock',\n",
       " '🕛': 'clock_face_twelve_oclock',\n",
       " '🕜': 'clock_face_one_thirty',\n",
       " '🕝': 'clock_face_two_thirty',\n",
       " '🕞': 'clock_face_three_thirty',\n",
       " '🕟': 'clock_face_four_thirty',\n",
       " '🕠': 'clock_face_five_thirty',\n",
       " '🕡': 'clock_face_six_thirty',\n",
       " '🕢': 'clock_face_seven_thirty',\n",
       " '🕣': 'clock_face_eight_thirty',\n",
       " '🕤': 'clock_face_nine_thirty',\n",
       " '🕥': 'clock_face_ten_thirty',\n",
       " '🕦': 'clock_face_eleven_thirty',\n",
       " '🕧': 'clock_face_twelve_thirty',\n",
       " '🕯': 'candle',\n",
       " '🕰': 'mantelpiece_clock',\n",
       " '🕳': 'hole',\n",
       " '🕴': 'man_in_business_suit_levitating',\n",
       " '🕵': 'sleuth_or_spy',\n",
       " '🕶': 'dark_sunglasses',\n",
       " '🕷': 'spider',\n",
       " '🕸': 'spider_web',\n",
       " '🕹': 'joystick',\n",
       " '🕺': 'man_dancing',\n",
       " '🖇': 'linked_paperclips',\n",
       " '🖊': 'lower_left_ballpoint_pen',\n",
       " '🖋': 'lower_left_fountain_pen',\n",
       " '🖌': 'lower_left_paintbrush',\n",
       " '🖍': 'lower_left_crayon',\n",
       " '🖐': 'raised_hand_with_fingers_splayed',\n",
       " '🖕': 'reversed_hand_with_middle_finger_extended',\n",
       " '🖖': 'raised_hand_with_part_between_middle_and_ring_fingers',\n",
       " '🖤': 'black_heart',\n",
       " '🖥': 'desktop_computer',\n",
       " '🖨': 'printer',\n",
       " '🖱': 'three_button_mouse',\n",
       " '🖲': 'trackball',\n",
       " '🖼': 'frame_with_picture',\n",
       " '🗂': 'card_index_dividers',\n",
       " '🗃': 'card_file_box',\n",
       " '🗄': 'file_cabinet',\n",
       " '🗑': 'wastebasket',\n",
       " '🗒': 'spiral_note_pad',\n",
       " '🗓': 'spiral_calendar_pad',\n",
       " '🗜': 'compression',\n",
       " '🗝': 'old_key',\n",
       " '🗞': 'rolled_up_newspaper',\n",
       " '🗡': 'dagger_knife',\n",
       " '🗣': 'speaking_head_in_silhouette',\n",
       " '🗨': 'left_speech_bubble',\n",
       " '🗯': 'right_anger_bubble',\n",
       " '🗳': 'ballot_box_with_ballot',\n",
       " '🗺': 'world_map',\n",
       " '🗻': 'mount_fuji',\n",
       " '🗼': 'tokyo_tower',\n",
       " '🗽': 'statue_of_liberty',\n",
       " '🗾': 'silhouette_of_japan',\n",
       " '🗿': 'moyai',\n",
       " '😀': 'grinning_face',\n",
       " '😁': 'grinning_face_with_smiling_eyes',\n",
       " '😂': 'face_with_tears_of_joy',\n",
       " '😃': 'smiling_face_with_open_mouth',\n",
       " '😄': 'smiling_face_with_open_mouth_and_smiling_eyes',\n",
       " '😅': 'smiling_face_with_open_mouth_and_cold_sweat',\n",
       " '😆': 'smiling_face_with_open_mouth_and_tightly_closed_eyes',\n",
       " '😇': 'smiling_face_with_halo',\n",
       " '😈': 'smiling_face_with_horns',\n",
       " '😉': 'winking_face',\n",
       " '😊': 'smiling_face_with_smiling_eyes',\n",
       " '😋': 'face_savouring_delicious_food',\n",
       " '😌': 'relieved_face',\n",
       " '😍': 'smiling_face_with_heart_shaped_eyes',\n",
       " '😎': 'smiling_face_with_sunglasses',\n",
       " '😏': 'smirking_face',\n",
       " '😐': 'neutral_face',\n",
       " '😑': 'expressionless_face',\n",
       " '😒': 'unamused_face',\n",
       " '😓': 'face_with_cold_sweat',\n",
       " '😔': 'pensive_face',\n",
       " '😕': 'confused_face',\n",
       " '😖': 'confounded_face',\n",
       " '😗': 'kissing_face',\n",
       " '😘': 'face_throwing_a_kiss',\n",
       " '😙': 'kissing_face_with_smiling_eyes',\n",
       " '😚': 'kissing_face_with_closed_eyes',\n",
       " '😛': 'face_with_stuck_out_tongue',\n",
       " '😜': 'face_with_stuck_out_tongue_and_winking_eye',\n",
       " '😝': 'face_with_stuck_out_tongue_and_tightly_closed_eyes',\n",
       " '😞': 'disappointed_face',\n",
       " '😟': 'worried_face',\n",
       " '😠': 'angry_face',\n",
       " '😡': 'pouting_face',\n",
       " '😢': 'crying_face',\n",
       " '😣': 'persevering_face',\n",
       " '😤': 'face_with_look_of_triumph',\n",
       " '😥': 'disappointed_but_relieved_face',\n",
       " '😦': 'frowning_face_with_open_mouth',\n",
       " '😧': 'anguished_face',\n",
       " '😨': 'fearful_face',\n",
       " '😩': 'weary_face',\n",
       " '😪': 'sleepy_face',\n",
       " '😫': 'tired_face',\n",
       " '😬': 'grimacing_face',\n",
       " '😭': 'loudly_crying_face',\n",
       " '😮': 'face_with_open_mouth',\n",
       " '😯': 'hushed_face',\n",
       " '😰': 'face_with_open_mouth_and_cold_sweat',\n",
       " '😱': 'face_screaming_in_fear',\n",
       " '😲': 'astonished_face',\n",
       " '😳': 'flushed_face',\n",
       " '😴': 'sleeping_face',\n",
       " '😵': 'dizzy_face',\n",
       " '😶': 'face_without_mouth',\n",
       " '😷': 'face_with_medical_mask',\n",
       " '😸': 'grinning_cat_face_with_smiling_eyes',\n",
       " '😹': 'cat_face_with_tears_of_joy',\n",
       " '😺': 'smiling_cat_face_with_open_mouth',\n",
       " '😻': 'smiling_cat_face_with_heart_shaped_eyes',\n",
       " '😼': 'cat_face_with_wry_smile',\n",
       " '😽': 'kissing_cat_face_with_closed_eyes',\n",
       " '😾': 'pouting_cat_face',\n",
       " '😿': 'crying_cat_face',\n",
       " '🙀': 'weary_cat_face',\n",
       " '🙁': 'slightly_frowning_face',\n",
       " '🙂': 'slightly_smiling_face',\n",
       " '🙃': 'upside_down_face',\n",
       " '🙄': 'face_with_rolling_eyes',\n",
       " '🙅': 'face_with_no_good_gesture',\n",
       " '🙆': 'face_with_ok_gesture',\n",
       " '🙇': 'person_bowing_deeply',\n",
       " '🙈': 'see_no_evil_monkey',\n",
       " '🙉': 'hear_no_evil_monkey',\n",
       " '🙊': 'speak_no_evil_monkey',\n",
       " '🙋': 'happy_person_raising_one_hand',\n",
       " '🙌': 'person_raising_both_hands_in_celebration',\n",
       " '🙍': 'person_frowning',\n",
       " '🙎': 'person_with_pouting_face',\n",
       " '🙏': 'person_with_folded_hands',\n",
       " '🚀': 'rocket',\n",
       " '🚁': 'helicopter',\n",
       " '🚂': 'steam_locomotive',\n",
       " '🚃': 'railway_car',\n",
       " '🚄': 'high_speed_train',\n",
       " '🚅': 'high_speed_train_with_bullet_nose',\n",
       " '🚆': 'train',\n",
       " '🚇': 'metro',\n",
       " '🚈': 'light_rail',\n",
       " '🚉': 'station',\n",
       " '🚊': 'tram',\n",
       " '🚋': 'tram_car',\n",
       " '🚌': 'bus',\n",
       " '🚍': 'oncoming_bus',\n",
       " '🚎': 'trolleybus',\n",
       " '🚏': 'bus_stop',\n",
       " '🚐': 'minibus',\n",
       " '🚑': 'ambulance',\n",
       " '🚒': 'fire_engine',\n",
       " '🚓': 'police_car',\n",
       " '🚔': 'oncoming_police_car',\n",
       " '🚕': 'taxi',\n",
       " '🚖': 'oncoming_taxi',\n",
       " '🚗': 'automobile',\n",
       " '🚘': 'oncoming_automobile',\n",
       " '🚙': 'recreational_vehicle',\n",
       " '🚚': 'delivery_truck',\n",
       " '🚛': 'articulated_lorry',\n",
       " '🚜': 'tractor',\n",
       " '🚝': 'monorail',\n",
       " '🚞': 'mountain_railway',\n",
       " '🚟': 'suspension_railway',\n",
       " '🚠': 'mountain_cableway',\n",
       " '🚡': 'aerial_tramway',\n",
       " '🚢': 'ship',\n",
       " '🚣': 'rowboat',\n",
       " '🚤': 'speedboat',\n",
       " '🚥': 'horizontal_traffic_light',\n",
       " '🚦': 'vertical_traffic_light',\n",
       " '🚧': 'construction_sign',\n",
       " '🚨': 'police_cars_revolving_light',\n",
       " '🚩': 'triangular_flag_on_post',\n",
       " '🚪': 'door',\n",
       " '🚫': 'no_entry_sign',\n",
       " '🚬': 'smoking_symbol',\n",
       " '🚭': 'no_smoking_symbol',\n",
       " '🚮': 'put_litter_in_its_place_symbol',\n",
       " '🚯': 'do_not_litter_symbol',\n",
       " '🚰': 'potable_water_symbol',\n",
       " '🚱': 'non_potable_water_symbol',\n",
       " '🚲': 'bicycle',\n",
       " '🚳': 'no_bicycles',\n",
       " '🚴': 'bicyclist',\n",
       " '🚵': 'mountain_bicyclist',\n",
       " '🚶': 'pedestrian',\n",
       " '🚷': 'no_pedestrians',\n",
       " '🚸': 'children_crossing',\n",
       " '🚹': 'mens_symbol',\n",
       " '🚺': 'womens_symbol',\n",
       " '🚻': 'restroom',\n",
       " '🚼': 'baby_symbol',\n",
       " '🚽': 'toilet',\n",
       " '🚾': 'water_closet',\n",
       " '🚿': 'shower',\n",
       " '🛀': 'bath',\n",
       " '🛁': 'bathtub',\n",
       " '🛂': 'passport_control',\n",
       " '🛃': 'customs',\n",
       " '🛄': 'baggage_claim',\n",
       " '🛅': 'left_luggage',\n",
       " '🛋': 'couch_and_lamp',\n",
       " '🛌': 'sleeping_accommodation',\n",
       " '🛍': 'shopping_bags',\n",
       " '🛎': 'bellhop_bell',\n",
       " '🛏': 'bed',\n",
       " '🛐': 'place_of_worship',\n",
       " '🛑': 'octagonal_sign',\n",
       " '🛒': 'shopping_trolley',\n",
       " '🛠': 'hammer_and_wrench',\n",
       " '🛡': 'shield',\n",
       " '🛢': 'oil_drum',\n",
       " '🛣': 'motorway',\n",
       " '🛤': 'railway_track',\n",
       " '🛥': 'motor_boat',\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the emoji dataset\n",
    "def load_unicode_mapping():\n",
    "    emoji_dict = {}\n",
    "    with open(\"emoji_image_to_whatIs.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            emoji_dict[tokens[0]] = tokens[1]\n",
    "    return emoji_dict\n",
    "emoji_dict = load_unicode_mapping()\n",
    "emoji_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "tweetCounts = []\n",
    "# get unigram counts for data\n",
    "def emoji_counts():\n",
    "    uni = Counter()\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        for word in s:\n",
    "            count = sum(map(lambda word : 1 if word in emoji_dict else 0, s))\n",
    "            tweetCounts.append(count)\n",
    "    return find_quartile_values(tweetCounts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams for the data splitting on spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams():\n",
    "    unigrams = Counter()\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        for word in s:\n",
    "            unigrams[word] += 1\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make count features binary by finding median values over entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into quartiles?\n",
    "def find_quartile_values(counts):\n",
    "    counts.sort()\n",
    "    first = np.quantile(counts, .25)\n",
    "    second = np.quantile(counts, .5)\n",
    "    third = np.quantile(counts, .75)\n",
    "    return [first, second, third] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_at_bins():\n",
    "    at_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        at_counts.append(count)\n",
    "    return find_quartile_values(at_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_num_token_bins():\n",
    "    lens = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = len(s)\n",
    "        lens.append(count)\n",
    "    \n",
    "    return find_quartile_values(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_swear_bins():\n",
    "    bad_words_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_bad = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "        bad_words_counts.append(tot_bad)\n",
    "    \n",
    "    return find_quartile_values(bad_words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mention_bins():\n",
    "    mentions = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        lens.append(count)\n",
    "    \n",
    "    return find_quartile_values(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashtag_bins():\n",
    "    hashtag_counts = []\n",
    "    at_sum = 0\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '#' in word else 0, s))\n",
    "        hashtag_counts.append(count)\n",
    "    \n",
    "    return find_quartile_values(hashtag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 4.0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_misspelling_bins():\n",
    "    misspell_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_misspelled = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() not in words_set:\n",
    "                tot_misspelled+=1\n",
    "        misspell_counts.append(tot_misspelled)\n",
    "    \n",
    "    return find_quartile_values(misspell_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hatebase_bins():\n",
    "    hatabase_words_set = set(open(\"hatebase_terms.txt\").read().split())\n",
    "    hatebase_words_counts = []\n",
    "    \n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if word in hatabase_words_set else 0, s))\n",
    "        hatebase_words_counts.append(count)\n",
    "    \n",
    "    hatebase_words_counts.sort()\n",
    "    median = statistics.median(hatebase_words_counts) #Get Median for binning\n",
    "    \n",
    "    return [0, median]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hatebase'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f3b69016e41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhatebase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHatebaseAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rTUCCDVYCcsEGmVKzQJjKwDFQsNcvUNa'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhatebase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHatebaseAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hatebase'"
     ]
    }
   ],
   "source": [
    "#Get All Eng Vocabulary\n",
    "import json \n",
    "import requests\n",
    "import pandas as pd\n",
    "from hatebase import HatebaseAPI\n",
    "key = 'rTUCCDVYCcsEGmVKzQJjKwDFQsNcvUNa'\n",
    "hatebase = HatebaseAPI({\"key\": key})\n",
    "filters = {\"language\": \"eng\"}\n",
    "format = \"json\"\n",
    "# initialize list for all vocabulary entry dictionaries\n",
    "eng_vocab = []\n",
    "response = hatebase.getVocabulary(filters=filters, format=format)\n",
    "pages = response[\"number_of_pages\"]\n",
    "# fill the vocabulary list with all entries of all pages\n",
    "# this might take some time...\n",
    "for page in range(1, pages+1):\n",
    "    filters[\"page\"] = str(page) \n",
    "    response = hatebase.getVocabulary(filters=filters, format=format)\n",
    "    eng_vocab.append(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_vocab = pd.DataFrame()\n",
    "# fill df\n",
    "for elem in eng_vocab:\n",
    "    df_eng_vocab = df_eng_vocab.append(elem)\n",
    "# reset the df index\n",
    "df_eng_vocab.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_eng_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatebase_words_set = set(open(\"hatebase_terms.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_eng_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5da393b42afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Get lists of all words from hatebase pertaining to a certain category\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhb_religion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_about_religion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhb_sexual_orientation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_about_sexual_orientation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhb_ethnicity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_about_ethnicity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhb_disability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_about_disability'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_eng_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "#Get lists of all words from hatebase pertaining to a certain category\"\n",
    "hb_religion = df_eng_vocab.loc[df_eng_vocab['is_about_religion']][['term']].values\n",
    "hb_sexual_orientation = df_eng_vocab.loc[df_eng_vocab['is_about_sexual_orientation']][['term']].values\n",
    "hb_ethnicity = df_eng_vocab.loc[df_eng_vocab['is_about_ethnicity']][['term']].values\n",
    "hb_disability = df_eng_vocab.loc[df_eng_vocab['is_about_disability']][['term']].values\n",
    "hb_social_class = df_eng_vocab.loc[df_eng_vocab['is_about_class']][['term']].values\n",
    "hb_nationality = df_eng_vocab.loc[df_eng_vocab['is_about_nationality']][['term']].values\n",
    "hb_gender = df_eng_vocab.loc[df_eng_vocab['is_about_gender']][['term']].values\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_nationality():\n",
    "    is_about_nationality_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        totalIsAboutNationality = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_nationality'] == True:\n",
    "                        totalIsAboutNationality+=1\n",
    "        is_about_nationality_counts.append(totalIsAboutNationality)\n",
    "        \n",
    "    return find_quartile_values(is_about_nationality_counts)\n",
    "\n",
    "hits = is_about_nationality()\n",
    "len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_class():\n",
    "    is_about_class_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_class'] == True:\n",
    "                        total+=1\n",
    "        is_about_class_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_class_counts)\n",
    "\n",
    "hits = is_about_class()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_disability():\n",
    "    is_about_disability_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_disability'] == True:\n",
    "                        total+=1\n",
    "        is_about_disability_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_disability_counts)\n",
    "\n",
    "hits = is_about_disability()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def is_about_ethnicity():\n",
    "    is_about_ethnicity_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_ethnicity'] == True:\n",
    "                        total+=1\n",
    "        is_about_ethnicity_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_ethnicity_counts)\n",
    "\n",
    "hits = is_about_ethnicity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def is_about_gender():\n",
    "    is_about_gender_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_gender'] == True:\n",
    "                        total+=1\n",
    "        is_about_gender_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_gender_counts)\n",
    "\n",
    "hits = is_about_gender()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def is_about_religion():\n",
    "    is_about_religion_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_religion'] == True:\n",
    "                        total+=1\n",
    "        is_about_religion_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_religion_counts)\n",
    "\n",
    "hits = is_about_religion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_sexual_orientation():\n",
    "    is_about_sexual_orientation_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_sexual_orientation'] == True:\n",
    "                        total+=1\n",
    "        is_about_sexual_orientation_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_sexual_orientation_counts)\n",
    "\n",
    "hits = is_about_sexual_orientation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get lists of all words from hatebase pertaining to a certain category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_religion = df_eng_vocab.loc[df_eng_vocab['is_about_religion']][['term']].values\n",
    "hb_sexual_orientation = df_eng_vocab.loc[df_eng_vocab['is_about_sexual_orientation']][['term']].values\n",
    "hb_ethnicity = df_eng_vocab.loc[df_eng_vocab['is_about_ethnicity']][['term']].values\n",
    "hb_disability = df_eng_vocab.loc[df_eng_vocab['is_about_disability']][['term']].values\n",
    "hb_social_class = df_eng_vocab.loc[df_eng_vocab['is_about_class']][['term']].values\n",
    "hb_nationality = df_eng_vocab.loc[df_eng_vocab['is_about_nationality']][['term']].values\n",
    "hb_gender = df_eng_vocab.loc[df_eng_vocab['is_about_gender']][['term']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bins contain the splits for which bin a tweet's feature counts will land in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token_bins = make_num_token_bins()\n",
    "swear_bins = make_swear_bins()\n",
    "at_bins = make_at_bins()\n",
    "hashtag_bins = make_hashtag_bins()\n",
    "#hatebase_words_bins = hatebase_words_bins()\n",
    "misspell_bins =  make_misspelling_bins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_more_upper(tweet):\n",
    "    total_caps = sum(map(lambda ch : 1 if ch.isupper() else 0, tweet))\n",
    "    if total_caps > len(tweet) // 2:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_consecutive_punc(tweet):\n",
    "    for word in tweet.split():\n",
    "        if 'http://' in word: continue\n",
    "        for i in range(len(word)-1):\n",
    "            if word[i] in string.punctuation and word[i+1] in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_person = ['he', 'she', 'they', 'him', 'her', 'them', 'his', 'hers', 'their', 'theirs', 'themselves', 'himself', 'herself']\n",
    "second_person = ['you', 'your', 'yours']\n",
    "first_person =['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if first word in tweet is a third person pronoun. Also check if number of third person pronouns is greater\n",
    "# than first person pronouns\n",
    "def get_pronouns(tweet):\n",
    "    first_pronoun = 0\n",
    "    third_and_second_greater_than_first = 0\n",
    "    words = tweet.split()\n",
    "    if words[0] in third_person or words[0] in second_person: \n",
    "        first_pronoun = 1\n",
    "    \n",
    "    first_person_count = 0\n",
    "    third_second_person_count = 0\n",
    "    for word in words:\n",
    "        if word in third_person or word in second_person:\n",
    "            third_second_person_count += 1\n",
    "        elif word in first_person:\n",
    "            first_person_count += 1\n",
    "            \n",
    "    if first_person_count < third_second_person_count:\n",
    "        third_and_second_greater_than_first = 1\n",
    "       \n",
    "    return first_pronoun, third_and_second_greater_than_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite lexicon:\n",
    "    Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \n",
    "        Proceedings of the ACM SIGKDD International Conference on Knowledge \n",
    "        Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \n",
    "        Washington, USA,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = set(open(\"opinion-lexicon-English/positive-words.txt\").read().split())\n",
    "neg_words = set(open(\"opinion-lexicon-English/negative-words.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns 1, 1 if negative words outnumber positive words and there are no positive words\n",
    "def get_sentiment(tweet):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for word in tweet.split():\n",
    "        if word in pos_words:\n",
    "            pos_count += 1\n",
    "        if word in neg_words:\n",
    "            neg_count += 1\n",
    "    if neg_count > pos_count:\n",
    "        if pos_count > 0:\n",
    "            return 1, 0\n",
    "        else:\n",
    "            return 1, 1\n",
    "    else:\n",
    "        if pos_count > 0:\n",
    "            return 0, 0\n",
    "        else:\n",
    "            return 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns one if a word in the tweet has non alphanumeric characters (not including punctuation at the end of a word)\n",
    "def contains_non_alphanum(tweet):\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if not word.isalnum():\n",
    "            if word[-1] not in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most common unigrams\n",
    "unigram_counts = get_unigrams()\n",
    "top_unigrams = unigram_counts.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each count feature has four bins: 1 for < 25th percentile, 2 for < 50th percentile, \n",
    "#3 for < 75th percentile, and 4 for < 100th percentile\n",
    "def find_bin(count, bin_name):\n",
    "    for i in range(len(bin_name)):\n",
    "        if count < bin_name[i]:\n",
    "            return i + 1\n",
    "    return len(bin_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the tweets by getting their feature representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets, judgements):\n",
    "#     for word in [u[0] for u in top_unigrams]:\n",
    "#         tweets[word] = tweets['tweet'].str.contains(word).astype(int)\n",
    "    word_counts = []\n",
    "    swear_counts = []\n",
    "    at_counts = []\n",
    "    contains_at = []\n",
    "    hashtag_counts = []\n",
    "    contains_hashtag = []\n",
    "    consecutive_punc = []\n",
    "    more_upper = []\n",
    "    first_pronoun = []\n",
    "    fewer_first_person = []\n",
    "    more_negative = []\n",
    "    no_positive = []\n",
    "    contains_url = []\n",
    "    not_alphanum = []\n",
    "    misspellings = []\n",
    "    disagreements = []\n",
    "    in_hatebase = []\n",
    "    about_gender = []\n",
    "    about_religion = []\n",
    "    about_sexual_orientation = []\n",
    "    about_ethnicity = []\n",
    "    about_disability = []\n",
    "    about_social_class = []\n",
    "    about_nationality = []\n",
    "\n",
    "    for tweet in tweets['tweet']:\n",
    "        #count tokens\n",
    "        tweet_words = tweet.split()\n",
    "        num_token_bin = find_bin(len(tweet_words), num_token_bins)\n",
    "        word_counts.append(num_token_bin)\n",
    "        \n",
    "        #count swear words, misspellings, and if a word is in hatebase\n",
    "        misspell_count = 0\n",
    "        tot_bad = 0\n",
    "        hatebase = 0\n",
    "        hatebase_words = []\n",
    "        for word in tweet_words:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "            if word.lower() not in words_set:\n",
    "                misspell_count+=1\n",
    "            if word.lower() in hatebase_words_set:\n",
    "                hatebase = 1\n",
    "                hatebase_words.append(word)\n",
    "        swear_bin = find_bin(tot_bad, swear_bins)\n",
    "        swear_counts.append(swear_bin)\n",
    "        misspell_bin = find_bin(misspell_count, misspell_bins)\n",
    "        misspellings.append(misspell_bin)\n",
    "        in_hatebase.append(hatebase)\n",
    "    \n",
    "        gender = religion = sexual_orientation = ethnicity = disability = social_class = nationality = 0\n",
    "        if hatebase == 1:\n",
    "            for word in hatebase_words:\n",
    "                if word in hb_gender: gender = 1\n",
    "                if word in hb_religion: religion = 1\n",
    "                if word in hb_sexual_orientation: sexual_orientation = 1\n",
    "                if word in hb_ethnicity: ethnicity = 1\n",
    "                if word in hb_disability: disability = 1\n",
    "                if word in hb_social_class: social_class = 1\n",
    "                if word in hb_nationality: nationality = 1\n",
    "        about_gender.append(gender)\n",
    "        about_religion.append(religion)\n",
    "        about_sexual_orientation.append(sexual_orientation)\n",
    "        about_ethnicity.append(ethnicity)\n",
    "        about_disability.append(disability)\n",
    "        about_social_class.append(social_class)\n",
    "        about_nationality.append(nationality)\n",
    "        \n",
    "        #count mentions\n",
    "        at_count = tweet.count('@')\n",
    "        if at_count > 0:\n",
    "            contains_at.append(1)\n",
    "        else:\n",
    "            contains_at.append(0)\n",
    "        at_bin = find_bin(at_count, at_bins)\n",
    "        at_counts.append(at_bin)\n",
    "        \n",
    "        #count hashtags\n",
    "        hash_count = tweet.count('#')\n",
    "        if hash_count > 0:\n",
    "            contains_hashtag.append(1)\n",
    "        else:\n",
    "            contains_hashtag.append(0)\n",
    "        hash_bin = find_bin(hash_count, hashtag_bins)\n",
    "        hashtag_counts.append(hash_bin)\n",
    "\n",
    "        more_upper.append(has_more_upper(tweet))\n",
    "        consecutive_punc.append(has_consecutive_punc(tweet))\n",
    "        first, more = get_pronouns(tweet)\n",
    "        first_pronoun.append(first)\n",
    "        fewer_first_person.append(more)\n",
    "        \n",
    "        more_neg, pos = get_sentiment(tweet)\n",
    "        more_negative.append(more_neg)\n",
    "        no_positive.append(pos)\n",
    "        \n",
    "        if 'http://' in tweet:\n",
    "            contains_url.append(1)\n",
    "        else:\n",
    "            contains_url.append(0)\n",
    "            \n",
    "        not_alphanum.append(contains_non_alphanum(tweet))\n",
    "        \n",
    "        #See if there were disagreements about classification\n",
    "        i = tweets.loc[tweets['tweet']==tweet].index[0]\n",
    "        total_votes = judgements.at[i, 'count']\n",
    "        if (judgements.at[i, 'hate_speech'] == total_votes) or (judgements.at[i, 'offensive_language'] == total_votes)\\\n",
    "            or (judgements.at[i, 'neither'] == total_votes):\n",
    "            disagreements.append(0)\n",
    "        else:\n",
    "            disagreements.append(1)\n",
    "        \n",
    "\n",
    "    tweets['Word Counts'] = word_counts\n",
    "    tweets['Swear Counts'] = swear_counts\n",
    "    tweets['@ Counts'] = at_counts\n",
    "    tweets['Mention'] = contains_at\n",
    "    tweets['Hashtag Counts'] = hashtag_counts\n",
    "    tweets['Contains Hashtag'] = contains_hashtag\n",
    "    tweets['Consecutive Punctuation'] = consecutive_punc\n",
    "    tweets['Majority Uppercase Letters'] = more_upper\n",
    "    tweets['First Word Second or Third Person Pronoun'] = first_pronoun\n",
    "    tweets['More Second or Third Person Pronouns than First'] = fewer_first_person\n",
    "    tweets['Majority Negative Words'] = more_negative\n",
    "    tweets['No Positive Words'] = no_positive\n",
    "    tweets['Contains URL'] = contains_url\n",
    "    tweets['Contains Non Alphanumeric Word'] = not_alphanum\n",
    "    tweets['Misspelling Count'] = misspellings\n",
    "    tweets['Judgement Disagreements'] = disagreements\n",
    "    tweets['About Gender (Hatebase)'] = about_gender\n",
    "    tweets['About Religion (Hatebase)'] = about_religion\n",
    "    tweets['About Ethnicity (Hatebase)'] = about_ethnicity\n",
    "    tweets['About Sexual Orientation (Hatebase)'] = about_sexual_orientation\n",
    "    tweets['About Disability (Hatebase)'] = about_disability\n",
    "    tweets['About Class (Hatebase)'] = about_social_class\n",
    "    tweets['About Nationality (Hatebase)'] = about_nationality\n",
    "    X = tweets[[col for col in tweets.columns if col!=\"tweet\"]].values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine train and dev sets for k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, dev_data], sort=False)\n",
    "tweets = data[['tweet']]\n",
    "judgements = data[['count', 'hate_speech', 'offensive_language', 'neither']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X = process_tweets(tweets, judgements)\n",
    "y = data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22214, 23)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10-fold cross validation on combined training and dev sets on LR, SVM, and NB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(fit_intercept=True, max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "lr_y_pred = cross_val_predict(LR, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.06      0.10      1282\n",
      "           1       0.87      0.92      0.89     17186\n",
      "           2       0.62      0.64      0.63      3746\n",
      "\n",
      "    accuracy                           0.82     22214\n",
      "   macro avg       0.66      0.54      0.54     22214\n",
      "weighted avg       0.80      0.82      0.80     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LR:\", classification_report(y, lr_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1282\n",
      "           1       0.87      0.92      0.89     17186\n",
      "           2       0.63      0.65      0.64      3746\n",
      "\n",
      "    accuracy                           0.82     22214\n",
      "   macro avg       0.50      0.53      0.51     22214\n",
      "weighted avg       0.78      0.82      0.80     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(gamma='auto') \n",
    "svm_y_pred = cross_val_predict(svm, X, y, cv=cv)\n",
    "print(classification_report(y, svm_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.59      0.29      1282\n",
      "           1       0.95      0.75      0.84     17186\n",
      "           2       0.55      0.69      0.61      3746\n",
      "\n",
      "    accuracy                           0.73     22214\n",
      "   macro avg       0.56      0.68      0.58     22214\n",
      "weighted avg       0.84      0.73      0.77     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "nb_y_pred = cross_val_predict(gnb, X, y, cv=cv)\n",
    "print(classification_report(y, nb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

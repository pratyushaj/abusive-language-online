{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "    \n",
    "TRAIN = '../data/train/training_data.csv'\n",
    "train_data = pd.read_csv(TRAIN, index_col=1)\n",
    "dev_data = pd.read_csv('../data/dev/development_data.csv', index_col=1)\n",
    "\n",
    "tweets = train_data[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "words_set = set(open(\"allwords.txt\").read().split())\n",
    "words_set = set(item.lower() for item in words_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams for the data splitting on spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams():\n",
    "    unigrams = Counter()\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        for word in s:\n",
    "            unigrams[word] += 1\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Emoji Dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def make_emoji_dict():\n",
    "    emoji_dict = {}\n",
    "    with open(\"emoji_image_to_whatIs.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            emoji_dict[tokens[0]] = tokens[1]\n",
    "    return emoji_dict\n",
    "emoji_dict = make_emoji_dict()\n",
    "#emoji_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into quartiles?\n",
    "def find_quartile_values(counts):\n",
    "    counts.sort()\n",
    "    first = np.quantile(counts, .25)\n",
    "    second = np.quantile(counts, .5)\n",
    "    third = np.quantile(counts, .75)\n",
    "    return [first, second, third] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Sentiment Features\n",
    "\n",
    "Emoji sentiment dictionary\n",
    "\n",
    "Features:\n",
    "Negative Sentiment,\n",
    "Neutral Sentiment,\n",
    "Positive Sentiment,\n",
    "Overall Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def load_emoji_sentiment_dict():\n",
    "    emoji_sentiment_dict = {}\n",
    "    with open(\"emoji_image_sentimentScore_definition_category.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            #each emoji has a list of 5 terms for a value\n",
    "            #emoji_sentiment_dict[tokens[0]] = [('neg':tokens[1]), ('neutral':tokens[2]), ('positive':tokens[3]), ('sentiment score':tokens[4]), ('name':tokens[5]), ('category':tokens[6])]\n",
    "            emoji_sentiment_dict[tokens[0]] = [tokens[1], tokens[2], tokens[3], tokens[4], tokens[5], tokens[6]]\n",
    "    return emoji_sentiment_dict\n",
    "emoji_sentiment_dict = load_emoji_sentiment_dict()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cite\n",
    "\n",
    "Emoji\tNeg[0...1]\tNeut[0...1]\tPos[0...1]\tSentiment score[-1...+1]\tUnicode name\tUnicode block\n",
    "\n",
    "@article{Kralj2015emojis,\n",
    "  author={{Kralj Novak}, Petra and Smailovi{\\'c}, Jasmina and Sluban, Borut and Mozeti\\v{c}, Igor},\n",
    "  title={Sentiment of emojis},\n",
    "  journal={PLoS ONE},\n",
    "  volume={10},\n",
    "  number={12},\n",
    "  pages={e0144296},\n",
    "  url={http://dx.doi.org/10.1371/journal.pone.0144296},\n",
    "  year={2015}\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "negativeValues = []\n",
    "def make_emoji_negative_sentiment_bins():\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #negative  \n",
    "                score += float(emoji_sentiment_dict[word][0])\n",
    "                negativeValues.append(score)\n",
    "    return find_quartile_values(negativeValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emoji counts for data\n",
    "def negative_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    check = False\n",
    "    emojis = []\n",
    "    score = 0\n",
    "    for word in s:\n",
    "        if word in emoji_sentiment_dict:\n",
    "            check = True\n",
    "            emojis.append(word)\n",
    "    if check:\n",
    "        for emoji in emojis:\n",
    "            num = emoji_sentiment_dict[emoji][0]\n",
    "            if type(num) is not str:\n",
    "                score += float(num)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutralValues = []\n",
    "\n",
    "def make_emoji_neutral_sentiment_bins():\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #neutral \n",
    "                score += float(emoji_sentiment_dict[word][1])\n",
    "                neutralValues.append(score)\n",
    "    return find_quartile_values(negativeValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emoji counts for data\n",
    "def neutral_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    check = False\n",
    "    emojis = []\n",
    "    score = 0\n",
    "    for word in s:\n",
    "        if word in emoji_sentiment_dict:\n",
    "            check = True\n",
    "            emojis.append(word)\n",
    "    if check:\n",
    "        for emoji in emojis:\n",
    "            num = emoji_sentiment_dict[emoji][1]\n",
    "            if type(num) is not str:\n",
    "                score += float(num)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveValues = []\n",
    "\n",
    "def make_emoji_positive_sentiment_bins():\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #positive \n",
    "                score += float(emoji_sentiment_dict[word][2])\n",
    "                positiveValues.append(score)\n",
    "    return find_quartile_values(positiveValues)\n",
    "#make_emoji_positive_sentiment_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emoji counts for data\n",
    "def positive_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    check = False\n",
    "    emojis = []\n",
    "    score = 0\n",
    "    for word in s:\n",
    "        if word in emoji_sentiment_dict:\n",
    "            check = True\n",
    "            emojis.append(word)\n",
    "    if check:\n",
    "        for emoji in emojis:\n",
    "            num = emoji_sentiment_dict[emoji][2]\n",
    "            if type(num) is not str:\n",
    "                score += float(num)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentimentValues = []\n",
    "\n",
    "def make_emoji_overall_sentiment_bins():\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        score = 0\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #sentiment\n",
    "                score += float(emoji_sentiment_dict[word][3])\n",
    "                sentimentValues.append(score)\n",
    "    return find_quartile_values(sentimentValues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emoji counts for data\n",
    "def overall_sentiment_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    check = False\n",
    "    emojis = []\n",
    "    score = 0\n",
    "    for word in s:\n",
    "        if word in emoji_sentiment_dict:\n",
    "            check = True\n",
    "            emojis.append(word)\n",
    "    if check:\n",
    "        for emoji in emojis:\n",
    "            num = emoji_sentiment_dict[emoji][3]\n",
    "            if type(num) is not str:\n",
    "                score += float(num)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find quartiles based on counts from entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_quartile_values(counts):\n",
    "    counts.sort()\n",
    "    first = np.quantile(counts, .25)\n",
    "    second = np.quantile(counts, .5)\n",
    "    third = np.quantile(counts, .75)\n",
    "    return [first, second, third] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profanity and Racist Lexicon counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make count features binary by finding median values over entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profanity_banned from trinker-lexicon-4c5e22b\n",
    "profanity_banned = set([\"anal\", \"anus\", \"arse\", \"ass\", \"balls\", \"ballsack\", \"bastard\", \n",
    "\"biatch\", \"bitch\", \"bloody\", \"blow job\", \"blowjob\", \"bollock\", \n",
    "\"bollok\", \"boner\", \"boob\", \"bugger\", \"bum\", \"butt\", \"buttplug\", \n",
    "\"clitoris\", \"cock\", \"coon\", \"crap\", \"cunt\", \"damn\", \"dick\", \"dildo\", \n",
    "\"dyke\", \"f u c k\", \"fag\", \"feck\", \"felching\", \"fellate\", \"fellatio\", \n",
    "\"flange\", \"fuck\", \"fudge packer\", \"fudgepacker\", \"God damn\", \n",
    "\"Goddamn\", \"hell\", \"homo\", \"jerk\", \"jizz\", \"knob end\", \"knobend\", \n",
    "\"labia\", \"lmao\", \"lmfao\", \"muff\", \"nigga\", \"nigger\", \"omg\", \"penis\", \n",
    "\"piss\", \"poop\", \"prick\", \"pube\", \"pussy\", \"queer\", \"s hit\", \"scrotum\", \n",
    "\"sex\", \"sh1t\", \"shit\", \"slut\", \"smegma\", \"spunk\", \"tit\", \"tosser\", \n",
    "\"turd\", \"twat\", \"vagina\", \"wank\", \"whore\", \"wtf\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_profanity_bins():\n",
    "    profanity = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if word in profanity_banned else 0, s))\n",
    "        profanity.append(count)\n",
    "    return find_quartile_values(profanity)\n",
    "#make_profanity_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def load_racist_lexicon():\n",
    "    racist_lexicon = []\n",
    "    with open(\"racist_lexicon.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            word = tokens[1].split(\"\\\"\") \n",
    "            racist_lexicon.append(word[1])\n",
    "    racist_lexicon = set(racist_lexicon)\n",
    "    return racist_lexicon\n",
    "racist_lexicon = load_racist_lexicon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE  ------ to discuss ------\n",
    "#very sparse, maybe this should be contains as if it's a word in that lexicon, \n",
    "#it's pretty darn offensive if not guaranteed to be hate speech\n",
    "\n",
    "def make_racist_lexicon_bins():\n",
    "    racist_words = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if word in racist_lexicon else 0, s))\n",
    "        racist_words.append(count)\n",
    "    return find_quartile_values(racist_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_at_bins():\n",
    "    at_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        at_counts.append(count)\n",
    "    return find_quartile_values(at_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_counts = []\n",
    "\n",
    "# get emoji counts for data\n",
    "def make_emoji_bins():\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        count = sum(map(lambda word : 1 if word in emoji_dict else 0, s))\n",
    "        emoji_counts.append(count)\n",
    "    return find_quartile_values(emoji_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_num_token_bins():\n",
    "    lens = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = len(s)\n",
    "        lens.append(count)\n",
    "    \n",
    "    return find_quartile_values(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_swear_bins():\n",
    "    bad_words_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_bad = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "        bad_words_counts.append(tot_bad)\n",
    "    \n",
    "    return find_quartile_values(bad_words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mention_bins():\n",
    "    mentions = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        mentions.append(count)\n",
    "    return find_quartile_values(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashtag_bins():\n",
    "    hashtag_counts = []\n",
    "    at_sum = 0\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '#' in word else 0, s))\n",
    "        hashtag_counts.append(count)\n",
    "    \n",
    "    return find_quartile_values(hashtag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_misspelling_bins():\n",
    "    misspell_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_misspelled = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() not in words_set:\n",
    "                tot_misspelled+=1\n",
    "        misspell_counts.append(tot_misspelled)\n",
    "    \n",
    "    return find_quartile_values(misspell_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get emoji counts for a specific tweet\n",
    "def count_emojis(tweet):\n",
    "    s = tknzr.tokenize(tweet)\n",
    "    count = sum(map(lambda word : 1 if word in emoji_dict else 0, s))\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_vocab = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_vocab = pd.read_csv('eng_lang_lexicon.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatebase_words_set = set(open(\"hatebase_terms.txt\").read().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get lists of all words from hatebase pertaining to a certain category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_religion = df_eng_vocab.loc[df_eng_vocab['is_about_religion']][['term']].values\n",
    "hb_sexual_orientation = df_eng_vocab.loc[df_eng_vocab['is_about_sexual_orientation']][['term']].values\n",
    "hb_ethnicity = df_eng_vocab.loc[df_eng_vocab['is_about_ethnicity']][['term']].values\n",
    "hb_disability = df_eng_vocab.loc[df_eng_vocab['is_about_disability']][['term']].values\n",
    "hb_social_class = df_eng_vocab.loc[df_eng_vocab['is_about_class']][['term']].values\n",
    "hb_nationality = df_eng_vocab.loc[df_eng_vocab['is_about_nationality']][['term']].values\n",
    "hb_gender = df_eng_vocab.loc[df_eng_vocab['is_about_gender']][['term']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bins contain the splits for which bin a tweet's feature counts will land in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token_bins = make_num_token_bins()\n",
    "swear_bins = make_swear_bins()\n",
    "at_bins = make_at_bins()\n",
    "hashtag_bins = make_hashtag_bins()\n",
    "emoji_bins = make_emoji_bins()\n",
    "#hatebase_words_bins = make_hatebase_bins()\n",
    "misspell_bins =  make_misspelling_bins()\n",
    "emoji_words_bins = make_emoji_bins()\n",
    "emoji_overall_sentiment_bins = make_emoji_overall_sentiment_bins()\n",
    "emoji_negative_sentiment_bins = make_emoji_negative_sentiment_bins()\n",
    "emoji_positive_sentiment_bins = make_emoji_positive_sentiment_bins()\n",
    "emoji_neutral_sentiment_bins = make_emoji_neutral_sentiment_bins()\n",
    "profanity_bins = make_profanity_bins()\n",
    "racist_bins = make_racist_lexicon_bins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_more_upper(tweet):\n",
    "    total_caps = sum(map(lambda ch : 1 if ch.isupper() else 0, tweet))\n",
    "    if total_caps > len(tweet) // 2:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_consecutive_punc(tweet):\n",
    "    for word in tweet.split():\n",
    "        if 'http://' in word: continue\n",
    "        for i in range(len(word)-1):\n",
    "            if word[i] in string.punctuation and word[i+1] in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_person = ['he', 'she', 'they', 'him', 'her', 'them', 'his', 'hers', 'their', 'theirs', 'themselves', 'himself', 'herself']\n",
    "second_person = ['you', 'your', 'yours']\n",
    "first_person =['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if first word in tweet is a third person pronoun. Also check if number of third person pronouns is greater\n",
    "# than first person pronouns\n",
    "def get_pronouns(tweet):\n",
    "    first_pronoun = 0\n",
    "    third_and_second_greater_than_first = 0\n",
    "    words = tweet.split()\n",
    "    if words[0] in third_person or words[0] in second_person: \n",
    "        first_pronoun = 1\n",
    "    \n",
    "    first_person_count = 0\n",
    "    third_second_person_count = 0\n",
    "    for word in words:\n",
    "        if word in third_person or word in second_person:\n",
    "            third_second_person_count += 1\n",
    "        elif word in first_person:\n",
    "            first_person_count += 1\n",
    "            \n",
    "    if first_person_count < third_second_person_count:\n",
    "        third_and_second_greater_than_first = 1\n",
    "       \n",
    "    return first_pronoun, third_and_second_greater_than_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite lexicon:\n",
    "    Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \n",
    "        Proceedings of the ACM SIGKDD International Conference on Knowledge \n",
    "        Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \n",
    "        Washington, USA,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = set(open(\"opinion-lexicon-English/positive-words.txt\").read().split())\n",
    "neg_words = set(open(\"opinion-lexicon-English/negative-words.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns 1, 1 if negative words outnumber positive words and there are no positive words\n",
    "def get_sentiment(tweet):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for word in tweet.split():\n",
    "        if word in pos_words:\n",
    "            pos_count += 1\n",
    "        if word in neg_words:\n",
    "            neg_count += 1\n",
    "    if neg_count > pos_count:\n",
    "        if pos_count > 0:\n",
    "            return 1, 0\n",
    "        else:\n",
    "            return 1, 1\n",
    "    else:\n",
    "        if pos_count > 0:\n",
    "            return 0, 0\n",
    "        else:\n",
    "            return 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns one if a word in the tweet has non alphanumeric characters (not including punctuation at the end of a word)\n",
    "def contains_non_alphanum(tweet):\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if not word.isalnum():\n",
    "            if word[-1] not in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most common unigrams\n",
    "unigram_counts = get_unigrams()\n",
    "top_unigrams = unigram_counts.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each count feature has four bins: 1 for < 25th percentile, 2 for < 50th percentile, \n",
    "#3 for < 75th percentile, and 4 for < 100th percentile\n",
    "def find_bin(count, bin_name):\n",
    "    for i in range(len(bin_name)):\n",
    "        if count < bin_name[i]:\n",
    "            return i + 1\n",
    "    return len(bin_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the tweets by getting their feature representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets, judgements):\n",
    "#     for word in [u[0] for u in top_unigrams]:\n",
    "#         tweets[word] = tweets['tweet'].str.contains(word).astype(int)\n",
    "    word_counts = []\n",
    "    swear_counts = []\n",
    "    at_counts = []\n",
    "    emoji_counts = []\n",
    "    \n",
    "    emoji_negative = []\n",
    "    emoji_positive = []\n",
    "    emoji_neutral = []\n",
    "    emoji_overall_sentiment = []\n",
    "    contains_emoji = []\n",
    "    \n",
    "    profanity_counts = []\n",
    "    contains_profanity = []\n",
    "    racist_counts = []\n",
    "    contains_racist = []\n",
    "    \n",
    "    contains_at = []\n",
    "    hashtag_counts = []\n",
    "    contains_hashtag = []\n",
    "    consecutive_punc = []\n",
    "    more_upper = []\n",
    "    first_pronoun = []\n",
    "    fewer_first_person = []\n",
    "    more_negative = []\n",
    "    no_positive = []\n",
    "    contains_url = []\n",
    "    not_alphanum = []\n",
    "    misspellings = []\n",
    "    disagreements = []\n",
    "    in_hatebase = []\n",
    "    about_gender = []\n",
    "    about_religion = []\n",
    "    about_sexual_orientation = []\n",
    "    about_ethnicity = []\n",
    "    about_disability = []\n",
    "    about_social_class = []\n",
    "    about_nationality = []\n",
    "\n",
    "\n",
    "    for tweet in tweets['tweet']:\n",
    "        #count tokens\n",
    "        tweet_words = tweet.split()\n",
    "        num_token_bin = find_bin(len(tweet_words), num_token_bins)\n",
    "        word_counts.append(num_token_bin)\n",
    "        \n",
    "        #emoji tokenizer\n",
    "        s = tknzr.tokenize(tweet)\n",
    "        #num_token_bin = find_bin(len(s), num_token_bins)\n",
    "        #word_counts.append(num_token_bin)\n",
    "        \n",
    "        #count swear words, misspellings, and if a word is in hatebase\n",
    "        misspell_count = 0\n",
    "        tot_bad = 0\n",
    "        tot_prof = isProfane = 0\n",
    "        tot_racist = isRacist = 0\n",
    "        hatebase = 0\n",
    "        hatebase_words = []\n",
    "        for word in tweet_words:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in profanity_banned:\n",
    "                tot_prof+=1\n",
    "                isProfane = 1\n",
    "            if word.lower() in racist_lexicon:\n",
    "                tot_racist+=1\n",
    "                isRacist = 1\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "            if word.lower() not in words_set:\n",
    "                misspell_count+=1\n",
    "            if word.lower() in hatebase_words_set:\n",
    "                hatebase = 1\n",
    "                hatebase_words.append(word)\n",
    "        swear_bin = find_bin(tot_bad, swear_bins)\n",
    "        swear_counts.append(swear_bin)\n",
    "        misspell_bin = find_bin(misspell_count, misspell_bins)\n",
    "        misspellings.append(misspell_bin)\n",
    "        in_hatebase.append(hatebase)\n",
    "        \n",
    "        racist_counts.append(tot_racist)\n",
    "        racist_bin = find_bin(tot_racist, racist_bins)\n",
    "        \n",
    "        \n",
    "        profanity_counts.append(tot_prof)\n",
    "        profanity_bin = find_bin(tot_prof, profanity_bins)\n",
    "        \n",
    "    \n",
    "        gender = religion = sexual_orientation = ethnicity = disability = social_class = nationality = 0\n",
    "        if hatebase == 1:\n",
    "            for word in hatebase_words:\n",
    "                if word in hb_gender: gender = 1\n",
    "                if word in hb_religion: religion = 1\n",
    "                if word in hb_sexual_orientation: sexual_orientation = 1\n",
    "                if word in hb_ethnicity: ethnicity = 1\n",
    "                if word in hb_disability: disability = 1\n",
    "                if word in hb_social_class: social_class = 1\n",
    "                if word in hb_nationality: nationality = 1\n",
    "        about_gender.append(gender)\n",
    "        about_religion.append(religion)\n",
    "        about_sexual_orientation.append(sexual_orientation)\n",
    "        about_ethnicity.append(ethnicity)\n",
    "        about_disability.append(disability)\n",
    "        about_social_class.append(social_class)\n",
    "        about_nationality.append(nationality)\n",
    "        contains_racist.append(isRacist)\n",
    "        contains_profanity.append(isProfane)\n",
    "        \n",
    "        #count mentions\n",
    "        at_count = tweet.count('@')\n",
    "        if at_count > 0:\n",
    "            contains_at.append(1)\n",
    "        else:\n",
    "            contains_at.append(0)\n",
    "        at_bin = find_bin(at_count, at_bins)\n",
    "        at_counts.append(at_bin)\n",
    "        \n",
    "        #count hashtags\n",
    "        hash_count = tweet.count('#')\n",
    "        if hash_count > 0:\n",
    "            contains_hashtag.append(1)\n",
    "        else:\n",
    "            contains_hashtag.append(0)\n",
    "        hash_bin = find_bin(hash_count, hashtag_bins)\n",
    "        hashtag_counts.append(hash_bin)\n",
    "        \n",
    "        #count emojis\n",
    "        emoji_count = count_emojis(tweet)\n",
    "        if emoji_count > 0:\n",
    "            contains_emoji.append(1)\n",
    "        else:\n",
    "            contains_emoji.append(0)\n",
    "        emoji_bin = find_bin(emoji_count, emoji_bins)\n",
    "        emoji_counts.append(emoji_bin)\n",
    "        \n",
    "        #emoji positive sentiment \n",
    "        emoji_positive_score = positive_sentiment_emojis(tweet) \n",
    "        emoji_positive_bin = find_bin(emoji_positive_score, emoji_bins)\n",
    "        emoji_positive.append(emoji_positive_bin)\n",
    "        \n",
    "        #emoji neutral sentiment \n",
    "        emoji_neutral_score = neutral_sentiment_emojis(tweet) \n",
    "        emoji_neutral_bin = find_bin(emoji_neutral_score, emoji_bins)\n",
    "        emoji_neutral.append(emoji_neutral_bin)\n",
    "        \n",
    "        #emoji negative sentiment \n",
    "        emoji_negative_score = negative_sentiment_emojis(tweet) \n",
    "        emoji_negative_bin = find_bin(emoji_negative_score, emoji_bins)\n",
    "        emoji_negative.append(emoji_negative_bin)\n",
    "        \n",
    "        #emoji overall sentiment \n",
    "        emoji_overall_score = overall_sentiment_emojis(tweet) \n",
    "        emoji_overall_bin = find_bin(emoji_overall_score, emoji_bins)\n",
    "        emoji_overall_sentiment.append(emoji_overall_bin)\n",
    "        \n",
    "        more_upper.append(has_more_upper(tweet))\n",
    "        consecutive_punc.append(has_consecutive_punc(tweet))\n",
    "        first, more = get_pronouns(tweet)\n",
    "        first_pronoun.append(first)\n",
    "        fewer_first_person.append(more)\n",
    "        \n",
    "        more_neg, pos = get_sentiment(tweet)\n",
    "        more_negative.append(more_neg)\n",
    "        no_positive.append(pos)\n",
    "        \n",
    "        if 'http://' in tweet:\n",
    "            contains_url.append(1)\n",
    "        else:\n",
    "            contains_url.append(0)\n",
    "            \n",
    "        not_alphanum.append(contains_non_alphanum(tweet))\n",
    "        \n",
    "        #See if there were disagreements about classification\n",
    "        i = tweets.loc[tweets['tweet']==tweet].index[0]\n",
    "        total_votes = judgements.at[i, 'count']\n",
    "        if (judgements.at[i, 'hate_speech'] == total_votes) or (judgements.at[i, 'offensive_language'] == total_votes)\\\n",
    "            or (judgements.at[i, 'neither'] == total_votes):\n",
    "            disagreements.append(0)\n",
    "        else:\n",
    "            disagreements.append(1)\n",
    "        \n",
    "    tweets['Word Counts'] = word_counts\n",
    "    tweets['Swear Counts'] = swear_counts\n",
    "    tweets['Profanity Counts'] = profanity_counts\n",
    "    tweets['Contains Profanity'] = contains_profanity\n",
    "    tweets['Racist Counts'] = racist_counts\n",
    "    tweets['Contains Racist'] = contains_racist\n",
    "    tweets['@ Counts'] = at_counts\n",
    "    tweets['Mention'] = contains_at\n",
    "    tweets['Contains Hashtag'] = contains_hashtag\n",
    "    tweets['Hashtag Counts'] = hashtag_counts\n",
    "    tweets['Emoji Counts'] = emoji_counts\n",
    "    tweets['Contains Emoji'] = contains_emoji\n",
    "    tweets['Negative Emoji'] = emoji_negative\n",
    "    tweets['Positive Emoji'] = emoji_positive\n",
    "    tweets['Neutral Emoji'] = emoji_neutral \n",
    "    tweets['Overall Emoji'] = emoji_overall_sentiment\n",
    "    tweets['Consecutive Punctuation'] = consecutive_punc\n",
    "    tweets['Majority Uppercase Letters'] = more_upper\n",
    "    tweets['First Word Second or Third Person Pronoun'] = first_pronoun\n",
    "    tweets['More Second or Third Person Pronouns than First'] = fewer_first_person\n",
    "    tweets['Majority Negative Words'] = more_negative\n",
    "    tweets['No Positive Words'] = no_positive\n",
    "    tweets['Contains URL'] = contains_url\n",
    "    tweets['Contains Non Alphanumeric Word'] = not_alphanum\n",
    "    tweets['Misspelling Count'] = misspellings\n",
    "    tweets['Judgement Disagreements'] = disagreements\n",
    "    tweets['About Gender (Hatebase)'] = about_gender\n",
    "    tweets['About Religion (Hatebase)'] = about_religion\n",
    "    tweets['About Ethnicity (Hatebase)'] = about_ethnicity\n",
    "    tweets['About Sexual Orientation (Hatebase)'] = about_sexual_orientation\n",
    "    tweets['About Disability (Hatebase)'] = about_disability\n",
    "    tweets['About Class (Hatebase)'] = about_social_class\n",
    "    tweets['About Nationality (Hatebase)'] = about_nationality\n",
    "    X = tweets[[col for col in tweets.columns if col!=\"tweet\"]]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine train and dev sets for k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, dev_data], sort=False)\n",
    "train_tweets = data[['tweet']]\n",
    "\n",
    "judgements = data[['count', 'hate_speech', 'offensive_language', 'neither']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:182: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:183: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:185: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:186: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:187: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X = process_tweets(train_tweets, judgements)\n",
    "y = data['class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10-fold cross validation on combined training and dev sets on LR, SVM, and NB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(fit_intercept=True, max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "lr_y_pred = cross_val_score(LR, X.values, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='lbfgs', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR.fit(X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Mix type of y not allowed, got types {'multiclass', 'continuous'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-190-d458af9548ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LR:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_y_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36mclassification_report\u001b[0;34m(y_true, y_pred, labels, target_names, sample_weight, digits)\u001b[0m\n\u001b[1;32m   1419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1421\u001b[0;31m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1422\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1423\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36munique_labels\u001b[0;34m(*ys)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Mix type of y not allowed, got types %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mlabel_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mys_types\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Mix type of y not allowed, got types {'multiclass', 'continuous'}"
     ]
    }
   ],
   "source": [
    "print(\"LR:\", classification_report(y, lr_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = SVC(gamma='auto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.00      0.00      0.00      1282\n",
      "          1       0.90      0.93      0.91     17186\n",
      "          2       0.66      0.79      0.72      3746\n",
      "\n",
      "avg / total       0.81      0.85      0.83     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_y_pred = cross_val_predict(svm, X.values, y, cv=cv)\n",
    "print(classification_report(y, svm_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB(priors=None, var_smoothing=1e-09)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gnb.fit(X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.17      0.78      0.27      1282\n",
      "          1       0.95      0.72      0.82     17186\n",
      "          2       0.68      0.57      0.62      3746\n",
      "\n",
      "    accuracy                           0.70     22214\n",
      "   macro avg       0.60      0.69      0.57     22214\n",
      "weighted avg       0.86      0.70      0.76     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_y_pred = cross_val_predict(gnb, X.values, y, cv=cv)\n",
    "print(classification_report(y, nb_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../data/test/testing_data.csv', index_col=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweets = test[['tweet']]\n",
    "test_judgements = test[['count', 'hate_speech', 'offensive_language', 'neither']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = test['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'http://t.co/QecHHKO42q' in emoji_sentiment_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:182: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:183: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:184: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:185: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:186: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "test_X = process_tweets(test_tweets, test_judgements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "This LogisticRegression instance is not fitted yet",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-f498e4ac36b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mLR_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0mPredicted\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \"\"\"\n\u001b[0;32m--> 324\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mdecision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'coef_'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             raise NotFittedError(\"This %(name)s instance is not fitted \"\n\u001b[0;32m--> 298\u001b[0;31m                                  \"yet\" % {'name': type(self).__name__})\n\u001b[0m\u001b[1;32m    299\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFittedError\u001b[0m: This LogisticRegression instance is not fitted yet"
     ]
    }
   ],
   "source": [
    "LR_preds = LR.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.13      0.20       142\n",
      "           1       0.90      0.93      0.91      1928\n",
      "           2       0.67      0.71      0.69       399\n",
      "\n",
      "    accuracy                           0.85      2469\n",
      "   macro avg       0.67      0.59      0.60      2469\n",
      "weighted avg       0.83      0.85      0.84      2469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, LR_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_preds = svm.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       142\n",
      "           1       0.90      0.93      0.91      1928\n",
      "           2       0.65      0.79      0.72       399\n",
      "\n",
      "    accuracy                           0.85      2469\n",
      "   macro avg       0.52      0.57      0.54      2469\n",
      "weighted avg       0.81      0.85      0.83      2469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/221/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_preds = gnb.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.17      0.70      0.27       142\n",
      "           1       0.96      0.72      0.82      1928\n",
      "           2       0.63      0.67      0.65       399\n",
      "\n",
      "    accuracy                           0.71      2469\n",
      "   macro avg       0.58      0.70      0.58      2469\n",
      "weighted avg       0.86      0.71      0.76      2469\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_y, gnb_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Error Analysis: Missed Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = data[['tweet', 'class']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_missed_classifications(true, y_preds):\n",
    "    misses = np.where(np.asarray(true) != y_preds)[0]\n",
    "    missed_preds = []\n",
    "    for i in range(len(true)):\n",
    "        if np.asarray(true)[i] != y_preds[i]:\n",
    "            missed_preds.append(y_preds[i])\n",
    "\n",
    "    return misses, missed_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = data['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_missed_indices, LR_missed_preds = get_missed_classifications(true_labels, lr_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_missed = all_tweets.iloc[LR_missed_indices]\n",
    "LR_missed.loc[:,'prediction'] = LR_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_missed_indices, SVM_missed_preds = get_missed_classifications(true_labels, svm_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_missed = all_tweets.iloc[SVM_missed_indices]\n",
    "SVM_missed.loc[:,'prediction'] = SVM_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_missed_indices, NB_missed_preds = get_missed_classifications(true_labels, nb_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_missed = all_tweets.iloc[NB_missed_indices]\n",
    "NB_missed.loc[:,'prediction'] = NB_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_missed.to_csv(\"LR_missed.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_missed.to_csv(\"SVM_missed.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_missed.to_csv(\"NB_missed.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test error analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-83895b9ac4fe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtrue_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "all_test = test[['tweet', 'class']]\n",
    "true_y = test['class'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_test_missed_indices, LR_test_missed_preds = get_missed_classifications(true_y, LR_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/221/lib/python3.7/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "LR_missed_test = all_test.iloc[LR_missed_indices]\n",
    "LR_missed_test.loc[:,'prediction'] = LR_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_missed_indices, svm_missed_preds = get_missed_classifications(true_y, svm_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_missed = all_test.iloc[svm_missed_indices]\n",
    "svm_missed.loc[:,'prediction'] = svm_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_missed_indices, gnb_missed_preds = get_missed_classifications(true_y, gnb_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb_missed = all_test.iloc[gnb_missed_indices]\n",
    "gnb_missed.loc[:,'prediction'] = gnb_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_missed.to_csv(\"test_LR_missed.csv\", sep='\\t')\n",
    "svm_missed.to_csv(\"test_svm_missed.csv\", sep='\\t')\n",
    "gnb_missed.to_csv(\"test_gnb_missed.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((22214, 33), (22214,))"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, true_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['class'] = true_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X['class'] = true_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "#need to add labels back into the data, since they technically will be a node in the network\n",
    "#allData = np.hstack((X, true_labels.values.reshape(-1,1)))\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pomegranate import *\n",
    "\n",
    "#single train test split\n",
    "#Note: didn't go for k-fold CV because then the network structure/parameters would have to be learned k times,\n",
    "#which seemed too time intensive to be really useful\n",
    "\n",
    "train = X.values\n",
    "test = test_X.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "#here's where the structure and params are learned - as it is, structure is learned with greedy algo. \n",
    "#To switch structure learning to Chow-Liu, run network = BayesianNetwork.from_samples(train, algorithm='chow-liu')\n",
    "#otherwise, if you want to learn with the greedy approach, just run BayesianNetwork.from_samples(train)\n",
    "network = BayesianNetwork.from_samples(train,algorithm=\"chow-liu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2469, 33)"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Each sample of the test set should be of shape (1,34) where the 34th column, corresponding to the label, \n",
    "#has the value None. That way, when we use network.predict_proba, we infer the label value given the feature values\n",
    "testNone = np.asarray([None for item in test])\n",
    "testSamples = np.asarray([item[:-1] for item in test])\n",
    "testToPredict = np.hstack((testSamples, testNone.reshape(-1,1)))\n",
    "testToPredict.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [item[-1] for item in test]\n",
    "y_pred = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in testToPredict:\n",
    "    \n",
    "    #gets a posterior distribution P(label | feature_1 = f1, feature_2 = f2,..., feature_n = fn)\n",
    "    dist = network.predict_proba(item)[-1]\n",
    "    \n",
    "    #gets the most likely label from that posterior distribution, which becomes our prediction\n",
    "    prediction = dist.mle()\n",
    "    #print(prediction)\n",
    "    #print(prediction)\n",
    "    y_pred.append(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.98      1.00      0.99      2428\n",
      "          1       0.00      0.00      0.00        41\n",
      "\n",
      "avg / total       0.97      0.98      0.98      2469\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:362: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[key] = _infer_fill_value(value)\n",
      "/Users/susanabenavidez/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:543: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "#y_pred = np.loadtxt(\"CL_pred_values.txt\")\n",
    "\n",
    "BN_missed_indices, BN_missed_preds = get_missed_classifications(y_true, y_pred)\n",
    "BN_missed = test_tweets.iloc[BN_missed_indices]\n",
    "BN_missed.loc[:,'prediction'] = BN_missed_preds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "BN_missed.to_csv(\"test_BN_missed.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>Word Counts</th>\n",
       "      <th>Swear Counts</th>\n",
       "      <th>Profanity Counts</th>\n",
       "      <th>Contains Profanity</th>\n",
       "      <th>Racist Counts</th>\n",
       "      <th>Contains Racist</th>\n",
       "      <th>@ Counts</th>\n",
       "      <th>Mention</th>\n",
       "      <th>Contains Hashtag</th>\n",
       "      <th>...</th>\n",
       "      <th>Misspelling Count</th>\n",
       "      <th>Judgement Disagreements</th>\n",
       "      <th>About Gender (Hatebase)</th>\n",
       "      <th>About Religion (Hatebase)</th>\n",
       "      <th>About Ethnicity (Hatebase)</th>\n",
       "      <th>About Sexual Orientation (Hatebase)</th>\n",
       "      <th>About Disability (Hatebase)</th>\n",
       "      <th>About Class (Hatebase)</th>\n",
       "      <th>About Nationality (Hatebase)</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11779</th>\n",
       "      <td>If California Chrome doesn't go off at even mo...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21405</th>\n",
       "      <td>So is the term wigger. If someone chooses to l...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13746</th>\n",
       "      <td>Noap. Not taking them. RT @TiffNCompany: Maca ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17532</th>\n",
       "      <td>RT @RudeBoi_Drew: @PAPER_CHAYSIN condom were i...</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9791</th>\n",
       "      <td>Got chunky marmalade today of a greatful custo...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6375</th>\n",
       "      <td>@izzayy_bitches &amp;amp; 4th my guinea pig will h...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10823</th>\n",
       "      <td>I have yet to understand why so many bytches g...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11414</th>\n",
       "      <td>I would describe @whatupag's work as blaxican.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14505</th>\n",
       "      <td>RT @AmandaMarcotte: &amp;#8220;Not all men interru...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20867</th>\n",
       "      <td>RT @yungkatana: free wop</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24315</th>\n",
       "      <td>fat white bitch an a skinny po' black nigga......</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11736</th>\n",
       "      <td>I've pretty much eaten an entire box of my nep...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14352</th>\n",
       "      <td>RT @23Vnds: Check out the homie @renz360 for t...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3037</th>\n",
       "      <td>@DannyMndz93 @Titan21Mtzzz he's still a pogue ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10108</th>\n",
       "      <td>Hmm.. Ass all chunky, brain is insanity.</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9800</th>\n",
       "      <td>Got my vans on.. My pockets chunky</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3633</th>\n",
       "      <td>@ItsYOUR_man you a fucking wigger so stfu</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2616</th>\n",
       "      <td>@Barbod6FCB. Wigga wigga wigga wigga wigga</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24397</th>\n",
       "      <td>hate when you point out truths to people, and ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23750</th>\n",
       "      <td>You boil my blood, just how spoilt and how muc...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20351</th>\n",
       "      <td>RT @renz360: Got 5 20% off slots for sole sauc...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>@CapitolEVAN That's where I saw the leprechaun...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22001</th>\n",
       "      <td>The chinks really hunting their food... RT @WW...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8714</th>\n",
       "      <td>Chuck E Cheese isn't the same as when I was a ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3469</th>\n",
       "      <td>@HollowDaDonLOM those god damn chinks. http://...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8864</th>\n",
       "      <td>Damn internal alarm clock. Woke up at 6:35am f...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16124</th>\n",
       "      <td>RT @JareerKassis: If there was an invisible ce...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9829</th>\n",
       "      <td>Grandma's Homemade Potato Soup - An old fashio...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24792</th>\n",
       "      <td>nobody likes bloody nips!</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11042</th>\n",
       "      <td>I miss my flip phone .. That bitch never died</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22075</th>\n",
       "      <td>The right for an English person to go about hi...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6770</th>\n",
       "      <td>@littlelouie33 eh, take off hoser.</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2385</th>\n",
       "      <td>@5Dimes fuck you tony...you greaseball piece o...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23318</th>\n",
       "      <td>Whops @_JAMAICAJONES yardie. Go smoke a blunt ...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10933</th>\n",
       "      <td>I like Chinese buffets but I hate all the chinks</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14531</th>\n",
       "      <td>RT @AngelGuapo23: @wigga32 can't dm wigga caus...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8227</th>\n",
       "      <td>Best rider? That goes to a blaxican.</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21673</th>\n",
       "      <td>Subtweet me one more time, you dirty chink whore</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23360</th>\n",
       "      <td>Why everybody talking about Rihanna nudes like...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15446</th>\n",
       "      <td>RT @ESPNNFL: For the 1st time in the Tom Brady...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5080</th>\n",
       "      <td>@TheLovely_CC whatever lol my tan line is reta...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tweet  Word Counts  \\\n",
       "11779  If California Chrome doesn't go off at even mo...            3   \n",
       "21405  So is the term wigger. If someone chooses to l...            3   \n",
       "13746  Noap. Not taking them. RT @TiffNCompany: Maca ...            3   \n",
       "17532  RT @RudeBoi_Drew: @PAPER_CHAYSIN condom were i...            2   \n",
       "9791   Got chunky marmalade today of a greatful custo...            3   \n",
       "6375   @izzayy_bitches &amp; 4th my guinea pig will h...            2   \n",
       "10823  I have yet to understand why so many bytches g...            3   \n",
       "11414     I would describe @whatupag's work as blaxican.            1   \n",
       "14505  RT @AmandaMarcotte: &#8220;Not all men interru...            3   \n",
       "20867                           RT @yungkatana: free wop            1   \n",
       "24315  fat white bitch an a skinny po' black nigga......            3   \n",
       "11736  I've pretty much eaten an entire box of my nep...            3   \n",
       "14352  RT @23Vnds: Check out the homie @renz360 for t...            3   \n",
       "3037   @DannyMndz93 @Titan21Mtzzz he's still a pogue ...            1   \n",
       "10108           Hmm.. Ass all chunky, brain is insanity.            1   \n",
       "9800                  Got my vans on.. My pockets chunky            1   \n",
       "3633           @ItsYOUR_man you a fucking wigger so stfu            1   \n",
       "2616          @Barbod6FCB. Wigga wigga wigga wigga wigga            1   \n",
       "24397  hate when you point out truths to people, and ...            3   \n",
       "23750  You boil my blood, just how spoilt and how muc...            3   \n",
       "20351  RT @renz360: Got 5 20% off slots for sole sauc...            3   \n",
       "2829   @CapitolEVAN That's where I saw the leprechaun...            3   \n",
       "22001  The chinks really hunting their food... RT @WW...            3   \n",
       "8714   Chuck E Cheese isn't the same as when I was a ...            3   \n",
       "3469   @HollowDaDonLOM those god damn chinks. http://...            1   \n",
       "8864   Damn internal alarm clock. Woke up at 6:35am f...            3   \n",
       "16124  RT @JareerKassis: If there was an invisible ce...            3   \n",
       "9829   Grandma's Homemade Potato Soup - An old fashio...            3   \n",
       "24792                          nobody likes bloody nips!            1   \n",
       "11042      I miss my flip phone .. That bitch never died            2   \n",
       "22075  The right for an English person to go about hi...            3   \n",
       "6770                  @littlelouie33 eh, take off hoser.            1   \n",
       "2385   @5Dimes fuck you tony...you greaseball piece o...            1   \n",
       "23318  Whops @_JAMAICAJONES yardie. Go smoke a blunt ...            3   \n",
       "10933   I like Chinese buffets but I hate all the chinks            2   \n",
       "14531  RT @AngelGuapo23: @wigga32 can't dm wigga caus...            3   \n",
       "8227                Best rider? That goes to a blaxican.            1   \n",
       "21673   Subtweet me one more time, you dirty chink whore            2   \n",
       "23360  Why everybody talking about Rihanna nudes like...            3   \n",
       "15446  RT @ESPNNFL: For the 1st time in the Tom Brady...            3   \n",
       "5080   @TheLovely_CC whatever lol my tan line is reta...            3   \n",
       "\n",
       "       Swear Counts  Profanity Counts  Contains Profanity  Racist Counts  \\\n",
       "11779             3                 1                   1              0   \n",
       "21405             3                 0                   0              1   \n",
       "13746             3                 1                   1              0   \n",
       "17532             3                 0                   0              0   \n",
       "9791              1                 0                   0              1   \n",
       "6375              1                 0                   0              1   \n",
       "10823             1                 0                   0              1   \n",
       "11414             1                 0                   0              0   \n",
       "14505             1                 0                   0              0   \n",
       "20867             3                 0                   0              0   \n",
       "24315             3                 1                   1              0   \n",
       "11736             1                 0                   0              0   \n",
       "14352             1                 0                   0              0   \n",
       "3037              1                 0                   0              0   \n",
       "10108             3                 1                   1              1   \n",
       "9800              1                 0                   0              1   \n",
       "3633              3                 0                   0              1   \n",
       "2616              1                 0                   0              5   \n",
       "24397             3                 1                   1              0   \n",
       "23750             3                 1                   1              0   \n",
       "20351             1                 0                   0              0   \n",
       "2829              3                 0                   0              0   \n",
       "22001             1                 0                   0              1   \n",
       "8714              3                 2                   1              0   \n",
       "3469              3                 1                   1              1   \n",
       "8864              3                 1                   1              1   \n",
       "16124             3                 0                   0              0   \n",
       "9829              1                 0                   0              1   \n",
       "24792             1                 1                   1              0   \n",
       "11042             3                 1                   1              0   \n",
       "22075             3                 0                   0              1   \n",
       "6770              3                 0                   0              0   \n",
       "2385              3                 1                   1              1   \n",
       "23318             3                 1                   1              1   \n",
       "10933             3                 0                   0              1   \n",
       "14531             1                 0                   0              1   \n",
       "8227              1                 0                   0              0   \n",
       "21673             3                 1                   1              1   \n",
       "23360             3                 0                   0              0   \n",
       "15446             1                 0                   0              0   \n",
       "5080              3                 0                   0              0   \n",
       "\n",
       "       Contains Racist  @ Counts  Mention  Contains Hashtag     ...      \\\n",
       "11779                0         2        0                 0     ...       \n",
       "21405                1         2        0                 0     ...       \n",
       "13746                0         3        1                 0     ...       \n",
       "17532                0         3        1                 0     ...       \n",
       "9791                 1         2        0                 1     ...       \n",
       "6375                 1         3        1                 0     ...       \n",
       "10823                1         2        0                 0     ...       \n",
       "11414                0         3        1                 0     ...       \n",
       "14505                0         3        1                 1     ...       \n",
       "20867                0         3        1                 0     ...       \n",
       "24315                0         2        0                 0     ...       \n",
       "11736                0         2        0                 0     ...       \n",
       "14352                0         3        1                 1     ...       \n",
       "3037                 0         3        1                 0     ...       \n",
       "10108                1         2        0                 0     ...       \n",
       "9800                 1         2        0                 0     ...       \n",
       "3633                 1         3        1                 0     ...       \n",
       "2616                 1         3        1                 0     ...       \n",
       "24397                0         2        0                 0     ...       \n",
       "23750                0         2        0                 1     ...       \n",
       "20351                0         3        1                 1     ...       \n",
       "2829                 0         3        1                 0     ...       \n",
       "22001                1         3        1                 0     ...       \n",
       "8714                 0         2        0                 0     ...       \n",
       "3469                 1         3        1                 0     ...       \n",
       "8864                 1         2        0                 1     ...       \n",
       "16124                0         3        1                 1     ...       \n",
       "9829                 1         2        0                 0     ...       \n",
       "24792                0         2        0                 0     ...       \n",
       "11042                0         2        0                 0     ...       \n",
       "22075                1         2        0                 0     ...       \n",
       "6770                 0         3        1                 0     ...       \n",
       "2385                 1         3        1                 0     ...       \n",
       "23318                1         3        1                 0     ...       \n",
       "10933                1         2        0                 0     ...       \n",
       "14531                1         3        1                 1     ...       \n",
       "8227                 0         2        0                 0     ...       \n",
       "21673                1         2        0                 0     ...       \n",
       "23360                0         2        0                 0     ...       \n",
       "15446                0         3        1                 0     ...       \n",
       "5080                 0         3        1                 0     ...       \n",
       "\n",
       "       Misspelling Count  Judgement Disagreements  About Gender (Hatebase)  \\\n",
       "11779                  2                        1                        0   \n",
       "21405                  1                        1                        0   \n",
       "13746                  3                        1                        0   \n",
       "17532                  3                        0                        0   \n",
       "9791                   3                        0                        0   \n",
       "6375                   3                        1                        0   \n",
       "10823                  3                        1                        0   \n",
       "11414                  3                        1                        0   \n",
       "14505                  3                        0                        0   \n",
       "20867                  2                        1                        0   \n",
       "24315                  3                        1                        1   \n",
       "11736                  1                        0                        0   \n",
       "14352                  3                        0                        0   \n",
       "3037                   3                        0                        0   \n",
       "10108                  1                        1                        0   \n",
       "9800                   1                        1                        0   \n",
       "3633                   3                        1                        0   \n",
       "2616                   3                        1                        0   \n",
       "24397                  2                        0                        1   \n",
       "23750                  2                        0                        1   \n",
       "20351                  3                        0                        1   \n",
       "2829                   2                        0                        0   \n",
       "22001                  3                        1                        0   \n",
       "8714                   2                        1                        1   \n",
       "3469                   3                        1                        0   \n",
       "8864                   3                        0                        0   \n",
       "16124                  3                        1                        0   \n",
       "9829                   3                        0                        0   \n",
       "24792                  1                        1                        0   \n",
       "11042                  2                        0                        1   \n",
       "22075                  1                        0                        0   \n",
       "6770                   3                        0                        0   \n",
       "2385                   3                        1                        0   \n",
       "23318                  3                        0                        0   \n",
       "10933                  1                        1                        0   \n",
       "14531                  3                        0                        0   \n",
       "8227                   2                        1                        0   \n",
       "21673                  2                        0                        1   \n",
       "23360                  3                        0                        1   \n",
       "15446                  3                        0                        0   \n",
       "5080                   3                        1                        0   \n",
       "\n",
       "       About Religion (Hatebase)  About Ethnicity (Hatebase)  \\\n",
       "11779                          0                           1   \n",
       "21405                          0                           1   \n",
       "13746                          0                           0   \n",
       "17532                          0                           1   \n",
       "9791                           0                           1   \n",
       "6375                           0                           1   \n",
       "10823                          0                           0   \n",
       "11414                          0                           1   \n",
       "14505                          0                           1   \n",
       "20867                          0                           0   \n",
       "24315                          0                           0   \n",
       "11736                          0                           0   \n",
       "14352                          0                           1   \n",
       "3037                           0                           1   \n",
       "10108                          0                           1   \n",
       "9800                           0                           1   \n",
       "3633                           0                           1   \n",
       "2616                           0                           1   \n",
       "24397                          0                           0   \n",
       "23750                          0                           0   \n",
       "20351                          0                           1   \n",
       "2829                           0                           0   \n",
       "22001                          0                           0   \n",
       "8714                           0                           0   \n",
       "3469                           0                           0   \n",
       "8864                           0                           1   \n",
       "16124                          0                           1   \n",
       "9829                           0                           1   \n",
       "24792                          0                           0   \n",
       "11042                          0                           0   \n",
       "22075                          0                           1   \n",
       "6770                           0                           0   \n",
       "2385                           0                           1   \n",
       "23318                          0                           1   \n",
       "10933                          0                           0   \n",
       "14531                          0                           1   \n",
       "8227                           0                           1   \n",
       "21673                          0                           0   \n",
       "23360                          0                           1   \n",
       "15446                          0                           1   \n",
       "5080                           0                           0   \n",
       "\n",
       "       About Sexual Orientation (Hatebase)  About Disability (Hatebase)  \\\n",
       "11779                                    0                            0   \n",
       "21405                                    0                            0   \n",
       "13746                                    0                            0   \n",
       "17532                                    0                            0   \n",
       "9791                                     0                            0   \n",
       "6375                                     0                            0   \n",
       "10823                                    0                            0   \n",
       "11414                                    0                            0   \n",
       "14505                                    0                            0   \n",
       "20867                                    0                            0   \n",
       "24315                                    0                            0   \n",
       "11736                                    0                            0   \n",
       "14352                                    0                            0   \n",
       "3037                                     0                            0   \n",
       "10108                                    0                            0   \n",
       "9800                                     0                            0   \n",
       "3633                                     0                            0   \n",
       "2616                                     0                            0   \n",
       "24397                                    0                            0   \n",
       "23750                                    0                            0   \n",
       "20351                                    0                            0   \n",
       "2829                                     0                            0   \n",
       "22001                                    0                            0   \n",
       "8714                                     0                            0   \n",
       "3469                                     0                            0   \n",
       "8864                                     0                            0   \n",
       "16124                                    0                            0   \n",
       "9829                                     0                            0   \n",
       "24792                                    0                            0   \n",
       "11042                                    0                            0   \n",
       "22075                                    0                            0   \n",
       "6770                                     0                            0   \n",
       "2385                                     0                            0   \n",
       "23318                                    0                            0   \n",
       "10933                                    0                            0   \n",
       "14531                                    0                            0   \n",
       "8227                                     0                            0   \n",
       "21673                                    0                            0   \n",
       "23360                                    0                            0   \n",
       "15446                                    0                            0   \n",
       "5080                                     0                            1   \n",
       "\n",
       "       About Class (Hatebase)  About Nationality (Hatebase)  prediction  \n",
       "11779                       1                             1           0  \n",
       "21405                       0                             1           0  \n",
       "13746                       0                             1           0  \n",
       "17532                       0                             1           0  \n",
       "9791                        0                             1           0  \n",
       "6375                        0                             1           0  \n",
       "10823                       0                             1           0  \n",
       "11414                       0                             1           0  \n",
       "14505                       0                             1           0  \n",
       "20867                       0                             1           0  \n",
       "24315                       0                             1           0  \n",
       "11736                       0                             1           0  \n",
       "14352                       0                             1           0  \n",
       "3037                        0                             1           0  \n",
       "10108                       0                             1           0  \n",
       "9800                        0                             1           0  \n",
       "3633                        0                             1           0  \n",
       "2616                        0                             1           0  \n",
       "24397                       0                             1           0  \n",
       "23750                       0                             1           0  \n",
       "20351                       0                             1           0  \n",
       "2829                        0                             1           0  \n",
       "22001                       0                             1           0  \n",
       "8714                        0                             1           0  \n",
       "3469                        0                             1           0  \n",
       "8864                        0                             1           0  \n",
       "16124                       0                             1           0  \n",
       "9829                        0                             1           0  \n",
       "24792                       0                             1           0  \n",
       "11042                       0                             1           0  \n",
       "22075                       0                             1           0  \n",
       "6770                        1                             1           0  \n",
       "2385                        0                             1           0  \n",
       "23318                       1                             1           0  \n",
       "10933                       0                             1           0  \n",
       "14531                       0                             1           0  \n",
       "8227                        0                             1           0  \n",
       "21673                       0                             1           0  \n",
       "23360                       0                             1           0  \n",
       "15446                       0                             1           0  \n",
       "5080                        0                             1           0  \n",
       "\n",
       "[41 rows x 35 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BN_missed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes,fname,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            x = 5\n",
    "            #strr = format(cm[i, j], '.2f')\n",
    "            #ax.text(j,i,str(round(cm[i,j],2)))\n",
    "            ax.text(j, i, str(round(cm[i,j],2)),ha=\"center\", va=\"center\",color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(fname)\n",
    "    return ax\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot normalized confusion matrix\n",
    "#plot_confusion_matrix(np.asarray(y_true), np.asarray(y_pred), classes=['hate','off','nei'],\n",
    "                      #title='Normalized confusion matrix')\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized confusion matrix\n",
      "[[1. 0.]\n",
      " [1. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1ba6a278>"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot confusion matrix bayesian network\n",
    "plot_confusion_matrix(np.asarray(y_true), np.asarray(y_pred), classes=np.asarray([0,1,2]),fname='bn.png',\n",
    "                      title='Confusion matrix, Bayesian Network',normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = plot_confusion_matrix(np.asarray(y), np.asarray(nb_y_pred), classes=np.asarray([0,1,2]),fname='nb.png',\n",
    "                      title='Confusion matrix, Naive Bayes',normalize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(np.asarray(y), np.asarray(svm_y_pred), classes=np.asarray([0,1,2]),fname='svm.png',\n",
    "                      title='Confusion matrix, SVM',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(np.asarray(y), np.asarray(lr_y_pred), classes=np.asarray([0,1,2]),fname='lr.png',\n",
    "                      title='Confusion matrix, Logistic Regression',normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pygraphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Viewing the DAG structure (need graphviz, pygraphviz)\n",
    "network.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unfortunate idiosyncrasy with saving: can't get the save to happen outside the exact current directory, so once\n",
    "#you make the image you should move it to misc\n",
    "network.plot('dag.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random aside: If you wanted to know the final log likelihood of the data given the learned network, log P(D | G)\n",
    "network.log_probability(train).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

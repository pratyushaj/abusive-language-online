{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "    \n",
    "TRAIN = '../data/train/training_data.csv'\n",
    "train_data = pd.read_csv(TRAIN, index_col=1)\n",
    "dev_data = pd.read_csv('../data/dev/development_data.csv', index_col=1)\n",
    "\n",
    "tweets = train_data[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "words_set = set(open(\"allwords.txt\").read().split())\n",
    "words_set = set(item.lower() for item in words_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Â©': 'copyright_sign',\n",
       " 'Â®': 'registered_sign',\n",
       " 'â€¼': 'double_exclamation_mark',\n",
       " 'â‰': 'exclamation_question_mark',\n",
       " 'â„¢': 'trade_mark_sign',\n",
       " 'â„¹': 'information_source',\n",
       " 'â†”': 'left_right_arrow',\n",
       " 'â†•': 'up_down_arrow',\n",
       " 'â†–': 'north_west_arrow',\n",
       " 'â†—': 'north_east_arrow',\n",
       " 'â†˜': 'south_east_arrow',\n",
       " 'â†™': 'south_west_arrow',\n",
       " 'â†©': 'leftwards_arrow_with_hook',\n",
       " 'â†ª': 'rightwards_arrow_with_hook',\n",
       " 'âŒš': 'watch',\n",
       " 'âŒ›': 'hourglass',\n",
       " 'âŒ¨': 'keyboard',\n",
       " 'â': 'eject_symbol',\n",
       " 'â©': 'black_right_pointing_double_triangle',\n",
       " 'âª': 'black_left_pointing_double_triangle',\n",
       " 'â«': 'black_up_pointing_double_triangle',\n",
       " 'â¬': 'black_down_pointing_double_triangle',\n",
       " 'â­': 'black_right_pointing_double_triangle_with_vertical_bar',\n",
       " 'â®': 'black_left_pointing_double_triangle_with_vertical_bar',\n",
       " 'â¯': 'black_right_pointing_triangle_with_double_vertical_bar',\n",
       " 'â°': 'alarm_clock',\n",
       " 'â±': 'stopwatch',\n",
       " 'â²': 'timer_clock',\n",
       " 'â³': 'hourglass_with_flowing_sand',\n",
       " 'â¸': 'double_vertical_bar',\n",
       " 'â¹': 'black_square_for_stop',\n",
       " 'âº': 'black_circle_for_record',\n",
       " 'â“‚': 'circled_latin_capital_letter_m',\n",
       " 'â–ª': 'black_small_square',\n",
       " 'â–«': 'white_small_square',\n",
       " 'â–¶': 'black_right_pointing_triangle',\n",
       " 'â—€': 'black_left_pointing_triangle',\n",
       " 'â—»': 'white_medium_square',\n",
       " 'â—¼': 'black_medium_square',\n",
       " 'â—½': 'white_medium_small_square',\n",
       " 'â—¾': 'black_medium_small_square',\n",
       " 'â˜€': 'black_sun_with_rays',\n",
       " 'â˜': 'cloud',\n",
       " 'â˜‚': 'umbrella',\n",
       " 'â˜ƒ': 'snowman',\n",
       " 'â˜„': 'comet',\n",
       " 'â˜': 'black_telephone',\n",
       " 'â˜‘': 'ballot_box_with_check',\n",
       " 'â˜”': 'umbrella_with_rain_drops',\n",
       " 'â˜•': 'hot_beverage',\n",
       " 'â˜˜': 'shamrock',\n",
       " 'â˜': 'white_up_pointing_index',\n",
       " 'â˜ ': 'skull_and_crossbones',\n",
       " 'â˜¢': 'radioactive_sign',\n",
       " 'â˜£': 'biohazard_sign',\n",
       " 'â˜¦': 'orthodox_cross',\n",
       " 'â˜ª': 'star_and_crescent',\n",
       " 'â˜®': 'peace_symbol',\n",
       " 'â˜¯': 'yin_yang',\n",
       " 'â˜¸': 'wheel_of_dharma',\n",
       " 'â˜¹': 'white_frowning_face',\n",
       " 'â˜º': 'white_smiling_face',\n",
       " 'â™€': 'female_sign',\n",
       " 'â™‚': 'male_sign',\n",
       " 'â™ˆ': 'aries',\n",
       " 'â™‰': 'taurus',\n",
       " 'â™Š': 'gemini',\n",
       " 'â™‹': 'cancer',\n",
       " 'â™Œ': 'leo',\n",
       " 'â™': 'virgo',\n",
       " 'â™': 'libra',\n",
       " 'â™': 'scorpius',\n",
       " 'â™': 'sagittarius',\n",
       " 'â™‘': 'capricorn',\n",
       " 'â™’': 'aquarius',\n",
       " 'â™“': 'pisces',\n",
       " 'â™ ': 'black_spade_suit',\n",
       " 'â™£': 'black_club_suit',\n",
       " 'â™¥': 'black_heart_suit',\n",
       " 'â™¦': 'black_diamond_suit',\n",
       " 'â™¨': 'hot_springs',\n",
       " 'â™»': 'black_universal_recycling_symbol',\n",
       " 'â™¿': 'wheelchair_symbol',\n",
       " 'âš’': 'hammer_and_pick',\n",
       " 'âš“': 'anchor',\n",
       " 'âš”': 'crossed_swords',\n",
       " 'âš•': 'staff_of_aesculapius',\n",
       " 'âš–': 'scales',\n",
       " 'âš—': 'alembic',\n",
       " 'âš™': 'gear',\n",
       " 'âš›': 'atom_symbol',\n",
       " 'âšœ': 'fleur_de_lis',\n",
       " 'âš ': 'warning_sign',\n",
       " 'âš¡': 'high_voltage_sign',\n",
       " 'âšª': 'medium_white_circle',\n",
       " 'âš«': 'medium_black_circle',\n",
       " 'âš°': 'coffin',\n",
       " 'âš±': 'funeral_urn',\n",
       " 'âš½': 'soccer_ball',\n",
       " 'âš¾': 'baseball',\n",
       " 'â›„': 'snowman_without_snow',\n",
       " 'â›…': 'sun_behind_cloud',\n",
       " 'â›ˆ': 'thunder_cloud_and_rain',\n",
       " 'â›': 'ophiuchus',\n",
       " 'â›': 'pick',\n",
       " 'â›‘': 'helmet_with_white_cross',\n",
       " 'â›“': 'chains',\n",
       " 'â›”': 'no_entry',\n",
       " 'â›©': 'shinto_shrine',\n",
       " 'â›ª': 'church',\n",
       " 'â›°': 'mountain',\n",
       " 'â›±': 'umbrella_on_ground',\n",
       " 'â›²': 'fountain',\n",
       " 'â›³': 'flag_in_hole',\n",
       " 'â›´': 'ferry',\n",
       " 'â›µ': 'sailboat',\n",
       " 'â›·': 'skier',\n",
       " 'â›¸': 'ice_skate',\n",
       " 'â›¹': 'person_with_ball',\n",
       " 'â›º': 'tent',\n",
       " 'â›½': 'fuel_pump',\n",
       " 'âœ‚': 'black_scissors',\n",
       " 'âœ…': 'white_heavy_check_mark',\n",
       " 'âœˆ': 'airplane',\n",
       " 'âœ‰': 'envelope',\n",
       " 'âœŠ': 'raised_fist',\n",
       " 'âœ‹': 'raised_hand',\n",
       " 'âœŒ': 'victory_hand',\n",
       " 'âœ': 'writing_hand',\n",
       " 'âœ': 'pencil',\n",
       " 'âœ’': 'black_nib',\n",
       " 'âœ”': 'heavy_check_mark',\n",
       " 'âœ–': 'heavy_multiplication_x',\n",
       " 'âœ': 'latin_cross',\n",
       " 'âœ¡': 'star_of_david',\n",
       " 'âœ¨': 'sparkles',\n",
       " 'âœ³': 'eight_spoked_asterisk',\n",
       " 'âœ´': 'eight_pointed_black_star',\n",
       " 'â„': 'snowflake',\n",
       " 'â‡': 'sparkle',\n",
       " 'âŒ': 'cross_mark',\n",
       " 'â': 'negative_squared_cross_mark',\n",
       " 'â“': 'black_question_mark_ornament',\n",
       " 'â”': 'white_question_mark_ornament',\n",
       " 'â•': 'white_exclamation_mark_ornament',\n",
       " 'â—': 'heavy_exclamation_mark_symbol',\n",
       " 'â£': 'heavy_heart_exclamation_mark_ornament',\n",
       " 'â¤': 'heavy_black_heart',\n",
       " 'â•': 'heavy_plus_sign',\n",
       " 'â–': 'heavy_minus_sign',\n",
       " 'â—': 'heavy_division_sign',\n",
       " 'â¡': 'black_rightwards_arrow',\n",
       " 'â°': 'curly_loop',\n",
       " 'â¿': 'double_curly_loop',\n",
       " 'â¤´': 'arrow_pointing_rightwards_then_curving_upwards',\n",
       " 'â¤µ': 'arrow_pointing_rightwards_then_curving_downwards',\n",
       " 'â¬…': 'leftwards_black_arrow',\n",
       " 'â¬†': 'upwards_black_arrow',\n",
       " 'â¬‡': 'downwards_black_arrow',\n",
       " 'â¬›': 'black_large_square',\n",
       " 'â¬œ': 'white_large_square',\n",
       " 'â­': 'white_medium_star',\n",
       " 'â­•': 'heavy_large_circle',\n",
       " 'ã€°': 'wavy_dash',\n",
       " 'ã€½': 'part_alternation_mark',\n",
       " 'ãŠ—': 'circled_ideograph_congratulation',\n",
       " 'ãŠ™': 'circled_ideograph_secret',\n",
       " 'ğŸ€„': 'mahjong_tile_red_dragon',\n",
       " 'ğŸƒ': 'playing_card_black_joker',\n",
       " 'ğŸ…°': 'negative_squared_latin_capital_letter_a',\n",
       " 'ğŸ…±': 'negative_squared_latin_capital_letter_b',\n",
       " 'ğŸ…¾': 'negative_squared_latin_capital_letter_o',\n",
       " 'ğŸ…¿': 'negative_squared_latin_capital_letter_p',\n",
       " 'ğŸ†': 'negative_squared_ab',\n",
       " 'ğŸ†‘': 'squared_cl',\n",
       " 'ğŸ†’': 'squared_cool',\n",
       " 'ğŸ†“': 'squared_free',\n",
       " 'ğŸ†”': 'squared_id',\n",
       " 'ğŸ†•': 'squared_new',\n",
       " 'ğŸ†–': 'squared_ng',\n",
       " 'ğŸ†—': 'squared_ok',\n",
       " 'ğŸ†˜': 'squared_sos',\n",
       " 'ğŸ†™': 'squared_up_with_exclamation_mark',\n",
       " 'ğŸ†š': 'squared_vs',\n",
       " 'ğŸˆ': 'squared_katakana_koko',\n",
       " 'ğŸˆ‚': 'squared_katakana_sa',\n",
       " 'ğŸˆš': 'squared_cjk_unified_ideograph_7121',\n",
       " 'ğŸˆ¯': 'squared_cjk_unified_ideograph_6307',\n",
       " 'ğŸˆ²': 'squared_cjk_unified_ideograph_7981',\n",
       " 'ğŸˆ³': 'squared_cjk_unified_ideograph_7a7a',\n",
       " 'ğŸˆ´': 'squared_cjk_unified_ideograph_5408',\n",
       " 'ğŸˆµ': 'squared_cjk_unified_ideograph_6e80',\n",
       " 'ğŸˆ¶': 'squared_cjk_unified_ideograph_6709',\n",
       " 'ğŸˆ·': 'squared_cjk_unified_ideograph_6708',\n",
       " 'ğŸˆ¸': 'squared_cjk_unified_ideograph_7533',\n",
       " 'ğŸˆ¹': 'squared_cjk_unified_ideograph_5272',\n",
       " 'ğŸˆº': 'squared_cjk_unified_ideograph_55b6',\n",
       " 'ğŸ‰': 'circled_ideograph_advantage',\n",
       " 'ğŸ‰‘': 'circled_ideograph_accept',\n",
       " 'ğŸŒ€': 'cyclone',\n",
       " 'ğŸŒ': 'foggy',\n",
       " 'ğŸŒ‚': 'closed_umbrella',\n",
       " 'ğŸŒƒ': 'night_with_stars',\n",
       " 'ğŸŒ„': 'sunrise_over_mountains',\n",
       " 'ğŸŒ…': 'sunrise',\n",
       " 'ğŸŒ†': 'cityscape_at_dusk',\n",
       " 'ğŸŒ‡': 'sunset_over_buildings',\n",
       " 'ğŸŒˆ': 'rainbow',\n",
       " 'ğŸŒ‰': 'bridge_at_night',\n",
       " 'ğŸŒŠ': 'water_wave',\n",
       " 'ğŸŒ‹': 'volcano',\n",
       " 'ğŸŒŒ': 'milky_way',\n",
       " 'ğŸŒ': 'earth_globe_europe_africa',\n",
       " 'ğŸŒ': 'earth_globe_americas',\n",
       " 'ğŸŒ': 'earth_globe_asia_australia',\n",
       " 'ğŸŒ': 'globe_with_meridians',\n",
       " 'ğŸŒ‘': 'new_moon_symbol',\n",
       " 'ğŸŒ’': 'waxing_crescent_moon_symbol',\n",
       " 'ğŸŒ“': 'first_quarter_moon_symbol',\n",
       " 'ğŸŒ”': 'waxing_gibbous_moon_symbol',\n",
       " 'ğŸŒ•': 'full_moon_symbol',\n",
       " 'ğŸŒ–': 'waning_gibbous_moon_symbol',\n",
       " 'ğŸŒ—': 'last_quarter_moon_symbol',\n",
       " 'ğŸŒ˜': 'waning_crescent_moon_symbol',\n",
       " 'ğŸŒ™': 'crescent_moon',\n",
       " 'ğŸŒš': 'new_moon_with_face',\n",
       " 'ğŸŒ›': 'first_quarter_moon_with_face',\n",
       " 'ğŸŒœ': 'last_quarter_moon_with_face',\n",
       " 'ğŸŒ': 'full_moon_with_face',\n",
       " 'ğŸŒ': 'sun_with_face',\n",
       " 'ğŸŒŸ': 'glowing_star',\n",
       " 'ğŸŒ ': 'shooting_star',\n",
       " 'ğŸŒ¡': 'thermometer',\n",
       " 'ğŸŒ¤': 'white_sun_with_small_cloud',\n",
       " 'ğŸŒ¥': 'white_sun_behind_cloud',\n",
       " 'ğŸŒ¦': 'white_sun_behind_cloud_with_rain',\n",
       " 'ğŸŒ§': 'cloud_with_rain',\n",
       " 'ğŸŒ¨': 'cloud_with_snow',\n",
       " 'ğŸŒ©': 'cloud_with_lightning',\n",
       " 'ğŸŒª': 'cloud_with_tornado',\n",
       " 'ğŸŒ«': 'fog',\n",
       " 'ğŸŒ¬': 'wind_blowing_face',\n",
       " 'ğŸŒ­': 'hot_dog',\n",
       " 'ğŸŒ®': 'taco',\n",
       " 'ğŸŒ¯': 'burrito',\n",
       " 'ğŸŒ°': 'chestnut',\n",
       " 'ğŸŒ±': 'seedling',\n",
       " 'ğŸŒ²': 'evergreen_tree',\n",
       " 'ğŸŒ³': 'deciduous_tree',\n",
       " 'ğŸŒ´': 'palm_tree',\n",
       " 'ğŸŒµ': 'cactus',\n",
       " 'ğŸŒ¶': 'hot_pepper',\n",
       " 'ğŸŒ·': 'tulip',\n",
       " 'ğŸŒ¸': 'cherry_blossom',\n",
       " 'ğŸŒ¹': 'rose',\n",
       " 'ğŸŒº': 'hibiscus',\n",
       " 'ğŸŒ»': 'sunflower',\n",
       " 'ğŸŒ¼': 'blossom',\n",
       " 'ğŸŒ½': 'ear_of_maize',\n",
       " 'ğŸŒ¾': 'ear_of_rice',\n",
       " 'ğŸŒ¿': 'herb',\n",
       " 'ğŸ€': 'four_leaf_clover',\n",
       " 'ğŸ': 'maple_leaf',\n",
       " 'ğŸ‚': 'fallen_leaf',\n",
       " 'ğŸƒ': 'leaf_fluttering_in_wind',\n",
       " 'ğŸ„': 'mushroom',\n",
       " 'ğŸ…': 'tomato',\n",
       " 'ğŸ†': 'aubergine',\n",
       " 'ğŸ‡': 'grapes',\n",
       " 'ğŸˆ': 'melon',\n",
       " 'ğŸ‰': 'watermelon',\n",
       " 'ğŸŠ': 'tangerine',\n",
       " 'ğŸ‹': 'lemon',\n",
       " 'ğŸŒ': 'banana',\n",
       " 'ğŸ': 'pineapple',\n",
       " 'ğŸ': 'red_apple',\n",
       " 'ğŸ': 'green_apple',\n",
       " 'ğŸ': 'pear',\n",
       " 'ğŸ‘': 'peach',\n",
       " 'ğŸ’': 'cherries',\n",
       " 'ğŸ“': 'strawberry',\n",
       " 'ğŸ”': 'hamburger',\n",
       " 'ğŸ•': 'slice_of_pizza',\n",
       " 'ğŸ–': 'meat_on_bone',\n",
       " 'ğŸ—': 'poultry_leg',\n",
       " 'ğŸ˜': 'rice_cracker',\n",
       " 'ğŸ™': 'rice_ball',\n",
       " 'ğŸš': 'cooked_rice',\n",
       " 'ğŸ›': 'curry_and_rice',\n",
       " 'ğŸœ': 'steaming_bowl',\n",
       " 'ğŸ': 'spaghetti',\n",
       " 'ğŸ': 'bread',\n",
       " 'ğŸŸ': 'french_fries',\n",
       " 'ğŸ ': 'roasted_sweet_potato',\n",
       " 'ğŸ¡': 'dango',\n",
       " 'ğŸ¢': 'oden',\n",
       " 'ğŸ£': 'sushi',\n",
       " 'ğŸ¤': 'fried_shrimp',\n",
       " 'ğŸ¥': 'fish_cake_with_swirl_design',\n",
       " 'ğŸ¦': 'soft_ice_cream',\n",
       " 'ğŸ§': 'shaved_ice',\n",
       " 'ğŸ¨': 'ice_cream',\n",
       " 'ğŸ©': 'doughnut',\n",
       " 'ğŸª': 'cookie',\n",
       " 'ğŸ«': 'chocolate_bar',\n",
       " 'ğŸ¬': 'candy',\n",
       " 'ğŸ­': 'lollipop',\n",
       " 'ğŸ®': 'custard',\n",
       " 'ğŸ¯': 'honey_pot',\n",
       " 'ğŸ°': 'shortcake',\n",
       " 'ğŸ±': 'bento_box',\n",
       " 'ğŸ²': 'pot_of_food',\n",
       " 'ğŸ³': 'cooking',\n",
       " 'ğŸ´': 'fork_and_knife',\n",
       " 'ğŸµ': 'teacup_without_handle',\n",
       " 'ğŸ¶': 'sake_bottle_and_cup',\n",
       " 'ğŸ·': 'wine_glass',\n",
       " 'ğŸ¸': 'cocktail_glass',\n",
       " 'ğŸ¹': 'tropical_drink',\n",
       " 'ğŸº': 'beer_mug',\n",
       " 'ğŸ»': 'clinking_beer_mugs',\n",
       " 'ğŸ¼': 'baby_bottle',\n",
       " 'ğŸ½': 'fork_and_knife_with_plate',\n",
       " 'ğŸ¾': 'bottle_with_popping_cork',\n",
       " 'ğŸ¿': 'popcorn',\n",
       " 'ğŸ€': 'ribbon',\n",
       " 'ğŸ': 'wrapped_present',\n",
       " 'ğŸ‚': 'birthday_cake',\n",
       " 'ğŸƒ': 'jack_o_lantern',\n",
       " 'ğŸ„': 'christmas_tree',\n",
       " 'ğŸ…': 'father_christmas',\n",
       " 'ğŸ†': 'fireworks',\n",
       " 'ğŸ‡': 'firework_sparkler',\n",
       " 'ğŸˆ': 'balloon',\n",
       " 'ğŸ‰': 'party_popper',\n",
       " 'ğŸŠ': 'confetti_ball',\n",
       " 'ğŸ‹': 'tanabata_tree',\n",
       " 'ğŸŒ': 'crossed_flags',\n",
       " 'ğŸ': 'pine_decoration',\n",
       " 'ğŸ': 'japanese_dolls',\n",
       " 'ğŸ': 'carp_streamer',\n",
       " 'ğŸ': 'wind_chime',\n",
       " 'ğŸ‘': 'moon_viewing_ceremony',\n",
       " 'ğŸ’': 'school_satchel',\n",
       " 'ğŸ“': 'graduation_cap',\n",
       " 'ğŸ–': 'military_medal',\n",
       " 'ğŸ—': 'reminder_ribbon',\n",
       " 'ğŸ™': 'studio_microphone',\n",
       " 'ğŸš': 'level_slider',\n",
       " 'ğŸ›': 'control_knobs',\n",
       " 'ğŸ': 'film_frames',\n",
       " 'ğŸŸ': 'admission_tickets',\n",
       " 'ğŸ ': 'carousel_horse',\n",
       " 'ğŸ¡': 'ferris_wheel',\n",
       " 'ğŸ¢': 'roller_coaster',\n",
       " 'ğŸ£': 'fishing_pole_and_fish',\n",
       " 'ğŸ¤': 'microphone',\n",
       " 'ğŸ¥': 'movie_camera',\n",
       " 'ğŸ¦': 'cinema',\n",
       " 'ğŸ§': 'headphone',\n",
       " 'ğŸ¨': 'artist_palette',\n",
       " 'ğŸ©': 'top_hat',\n",
       " 'ğŸª': 'circus_tent',\n",
       " 'ğŸ«': 'ticket',\n",
       " 'ğŸ¬': 'clapper_board',\n",
       " 'ğŸ­': 'performing_arts',\n",
       " 'ğŸ®': 'video_game',\n",
       " 'ğŸ¯': 'direct_hit',\n",
       " 'ğŸ°': 'slot_machine',\n",
       " 'ğŸ±': 'billiards',\n",
       " 'ğŸ²': 'game_die',\n",
       " 'ğŸ³': 'bowling',\n",
       " 'ğŸ´': 'flower_playing_cards',\n",
       " 'ğŸµ': 'musical_note',\n",
       " 'ğŸ¶': 'multiple_musical_notes',\n",
       " 'ğŸ·': 'saxophone',\n",
       " 'ğŸ¸': 'guitar',\n",
       " 'ğŸ¹': 'musical_keyboard',\n",
       " 'ğŸº': 'trumpet',\n",
       " 'ğŸ»': 'violin',\n",
       " 'ğŸ¼': 'musical_score',\n",
       " 'ğŸ½': 'running_shirt_with_sash',\n",
       " 'ğŸ¾': 'tennis_racquet_and_ball',\n",
       " 'ğŸ¿': 'ski_and_ski_boot',\n",
       " 'ğŸ€': 'basketball_and_hoop',\n",
       " 'ğŸ': 'chequered_flag',\n",
       " 'ğŸ‚': 'snowboarder',\n",
       " 'ğŸƒ': 'runner',\n",
       " 'ğŸ„': 'surfer',\n",
       " 'ğŸ…': 'sports_medal',\n",
       " 'ğŸ†': 'trophy',\n",
       " 'ğŸ‡': 'horse_racing',\n",
       " 'ğŸˆ': 'american_football',\n",
       " 'ğŸ‰': 'rugby_football',\n",
       " 'ğŸŠ': 'swimmer',\n",
       " 'ğŸ‹': 'weight_lifter',\n",
       " 'ğŸŒ': 'golfer',\n",
       " 'ğŸ': 'racing_motorcycle',\n",
       " 'ğŸ': 'racing_car',\n",
       " 'ğŸ': 'cricket_bat_and_ball',\n",
       " 'ğŸ': 'volleyball',\n",
       " 'ğŸ‘': 'field_hockey_stick_and_ball',\n",
       " 'ğŸ’': 'ice_hockey_stick_and_puck',\n",
       " 'ğŸ“': 'table_tennis_paddle_and_ball',\n",
       " 'ğŸ”': 'snow_capped_mountain',\n",
       " 'ğŸ•': 'camping',\n",
       " 'ğŸ–': 'beach_with_umbrella',\n",
       " 'ğŸ—': 'building_construction',\n",
       " 'ğŸ˜': 'house_buildings',\n",
       " 'ğŸ™': 'cityscape',\n",
       " 'ğŸš': 'derelict_house_building',\n",
       " 'ğŸ›': 'classical_building',\n",
       " 'ğŸœ': 'desert',\n",
       " 'ğŸ': 'desert_island',\n",
       " 'ğŸ': 'national_park',\n",
       " 'ğŸŸ': 'stadium',\n",
       " 'ğŸ ': 'house_building',\n",
       " 'ğŸ¡': 'house_with_garden',\n",
       " 'ğŸ¢': 'office_building',\n",
       " 'ğŸ£': 'japanese_post_office',\n",
       " 'ğŸ¤': 'european_post_office',\n",
       " 'ğŸ¥': 'hospital',\n",
       " 'ğŸ¦': 'bank',\n",
       " 'ğŸ§': 'automated_teller_machine',\n",
       " 'ğŸ¨': 'hotel',\n",
       " 'ğŸ©': 'love_hotel',\n",
       " 'ğŸª': 'convenience_store',\n",
       " 'ğŸ«': 'school',\n",
       " 'ğŸ¬': 'department_store',\n",
       " 'ğŸ­': 'factory',\n",
       " 'ğŸ®': 'izakaya_lantern',\n",
       " 'ğŸ¯': 'japanese_castle',\n",
       " 'ğŸ°': 'european_castle',\n",
       " 'ğŸ³': 'waving_white_flag',\n",
       " 'ğŸ´': 'waving_black_flag',\n",
       " 'ğŸµ': 'rosette',\n",
       " 'ğŸ·': 'label',\n",
       " 'ğŸ¸': 'badminton_racquet_and_shuttlecock',\n",
       " 'ğŸ¹': 'bow_and_arrow',\n",
       " 'ğŸº': 'amphora',\n",
       " 'ğŸ»': 'emoji_modifier_fitzpatrick_type_1_2',\n",
       " 'ğŸ¼': 'emoji_modifier_fitzpatrick_type_3',\n",
       " 'ğŸ½': 'emoji_modifier_fitzpatrick_type_4',\n",
       " 'ğŸ¾': 'emoji_modifier_fitzpatrick_type_5',\n",
       " 'ğŸ¿': 'emoji_modifier_fitzpatrick_type_6',\n",
       " 'ğŸ€': 'rat',\n",
       " 'ğŸ': 'mouse',\n",
       " 'ğŸ‚': 'ox',\n",
       " 'ğŸƒ': 'water_buffalo',\n",
       " 'ğŸ„': 'cow',\n",
       " 'ğŸ…': 'tiger',\n",
       " 'ğŸ†': 'leopard',\n",
       " 'ğŸ‡': 'rabbit',\n",
       " 'ğŸˆ': 'cat',\n",
       " 'ğŸ‰': 'dragon',\n",
       " 'ğŸŠ': 'crocodile',\n",
       " 'ğŸ‹': 'whale',\n",
       " 'ğŸŒ': 'snail',\n",
       " 'ğŸ': 'snake',\n",
       " 'ğŸ': 'horse',\n",
       " 'ğŸ': 'ram',\n",
       " 'ğŸ': 'goat',\n",
       " 'ğŸ‘': 'sheep',\n",
       " 'ğŸ’': 'monkey',\n",
       " 'ğŸ“': 'rooster',\n",
       " 'ğŸ”': 'chicken',\n",
       " 'ğŸ•': 'dog',\n",
       " 'ğŸ–': 'pig',\n",
       " 'ğŸ—': 'boar',\n",
       " 'ğŸ˜': 'elephant',\n",
       " 'ğŸ™': 'octopus',\n",
       " 'ğŸš': 'spiral_shell',\n",
       " 'ğŸ›': 'bug',\n",
       " 'ğŸœ': 'ant',\n",
       " 'ğŸ': 'honeybee',\n",
       " 'ğŸ': 'lady_beetle',\n",
       " 'ğŸŸ': 'fish',\n",
       " 'ğŸ ': 'tropical_fish',\n",
       " 'ğŸ¡': 'blowfish',\n",
       " 'ğŸ¢': 'turtle',\n",
       " 'ğŸ£': 'hatching_chick',\n",
       " 'ğŸ¤': 'baby_chick',\n",
       " 'ğŸ¥': 'front_facing_baby_chick',\n",
       " 'ğŸ¦': 'bird',\n",
       " 'ğŸ§': 'penguin',\n",
       " 'ğŸ¨': 'koala',\n",
       " 'ğŸ©': 'poodle',\n",
       " 'ğŸª': 'dromedary_camel',\n",
       " 'ğŸ«': 'bactrian_camel',\n",
       " 'ğŸ¬': 'dolphin',\n",
       " 'ğŸ­': 'mouse_face',\n",
       " 'ğŸ®': 'cow_face',\n",
       " 'ğŸ¯': 'tiger_face',\n",
       " 'ğŸ°': 'rabbit_face',\n",
       " 'ğŸ±': 'cat_face',\n",
       " 'ğŸ²': 'dragon_face',\n",
       " 'ğŸ³': 'spouting_whale',\n",
       " 'ğŸ´': 'horse_face',\n",
       " 'ğŸµ': 'monkey_face',\n",
       " 'ğŸ¶': 'dog_face',\n",
       " 'ğŸ·': 'pig_face',\n",
       " 'ğŸ¸': 'frog_face',\n",
       " 'ğŸ¹': 'hamster_face',\n",
       " 'ğŸº': 'wolf_face',\n",
       " 'ğŸ»': 'bear_face',\n",
       " 'ğŸ¼': 'panda_face',\n",
       " 'ğŸ½': 'pig_nose',\n",
       " 'ğŸ¾': 'paw_prints',\n",
       " 'ğŸ¿': 'chipmunk',\n",
       " 'ğŸ‘€': 'eyes',\n",
       " 'ğŸ‘': 'eye',\n",
       " 'ğŸ‘‚': 'ear',\n",
       " 'ğŸ‘ƒ': 'nose',\n",
       " 'ğŸ‘„': 'mouth',\n",
       " 'ğŸ‘…': 'tongue',\n",
       " 'ğŸ‘†': 'white_up_pointing_backhand_index',\n",
       " 'ğŸ‘‡': 'white_down_pointing_backhand_index',\n",
       " 'ğŸ‘ˆ': 'white_left_pointing_backhand_index',\n",
       " 'ğŸ‘‰': 'white_right_pointing_backhand_index',\n",
       " 'ğŸ‘Š': 'fisted_hand_sign',\n",
       " 'ğŸ‘‹': 'waving_hand_sign',\n",
       " 'ğŸ‘Œ': 'ok_hand_sign',\n",
       " 'ğŸ‘': 'thumbs_up_sign',\n",
       " 'ğŸ‘': 'thumbs_down_sign',\n",
       " 'ğŸ‘': 'clapping_hands_sign',\n",
       " 'ğŸ‘': 'open_hands_sign',\n",
       " 'ğŸ‘‘': 'crown',\n",
       " 'ğŸ‘’': 'womans_hat',\n",
       " 'ğŸ‘“': 'eyeglasses',\n",
       " 'ğŸ‘”': 'necktie',\n",
       " 'ğŸ‘•': 't_shirt',\n",
       " 'ğŸ‘–': 'jeans',\n",
       " 'ğŸ‘—': 'dress',\n",
       " 'ğŸ‘˜': 'kimono',\n",
       " 'ğŸ‘™': 'bikini',\n",
       " 'ğŸ‘š': 'womans_clothes',\n",
       " 'ğŸ‘›': 'purse',\n",
       " 'ğŸ‘œ': 'handbag',\n",
       " 'ğŸ‘': 'pouch',\n",
       " 'ğŸ‘': 'mans_shoe',\n",
       " 'ğŸ‘Ÿ': 'athletic_shoe',\n",
       " 'ğŸ‘ ': 'high_heeled_shoe',\n",
       " 'ğŸ‘¡': 'womans_sandal',\n",
       " 'ğŸ‘¢': 'womans_boots',\n",
       " 'ğŸ‘£': 'footprints',\n",
       " 'ğŸ‘¤': 'bust_in_silhouette',\n",
       " 'ğŸ‘¥': 'busts_in_silhouette',\n",
       " 'ğŸ‘¦': 'boy',\n",
       " 'ğŸ‘§': 'girl',\n",
       " 'ğŸ‘¨': 'man',\n",
       " 'ğŸ‘©': 'woman',\n",
       " 'ğŸ‘ª': 'family',\n",
       " 'ğŸ‘«': 'man_and_woman_holding_hands',\n",
       " 'ğŸ‘¬': 'two_men_holding_hands',\n",
       " 'ğŸ‘­': 'two_women_holding_hands',\n",
       " 'ğŸ‘®': 'police_officer',\n",
       " 'ğŸ‘¯': 'woman_with_bunny_ears',\n",
       " 'ğŸ‘°': 'bride_with_veil',\n",
       " 'ğŸ‘±': 'person_with_blond_hair',\n",
       " 'ğŸ‘²': 'man_with_gua_pi_mao',\n",
       " 'ğŸ‘³': 'man_with_turban',\n",
       " 'ğŸ‘´': 'older_man',\n",
       " 'ğŸ‘µ': 'older_woman',\n",
       " 'ğŸ‘¶': 'baby',\n",
       " 'ğŸ‘·': 'construction_worker',\n",
       " 'ğŸ‘¸': 'princess',\n",
       " 'ğŸ‘¹': 'japanese_ogre',\n",
       " 'ğŸ‘º': 'japanese_goblin',\n",
       " 'ğŸ‘»': 'ghost',\n",
       " 'ğŸ‘¼': 'baby_angel',\n",
       " 'ğŸ‘½': 'extraterrestrial_alien',\n",
       " 'ğŸ‘¾': 'alien_monster',\n",
       " 'ğŸ‘¿': 'imp',\n",
       " 'ğŸ’€': 'skull',\n",
       " 'ğŸ’': 'information_desk_person',\n",
       " 'ğŸ’‚': 'guardsman',\n",
       " 'ğŸ’ƒ': 'dancer',\n",
       " 'ğŸ’„': 'lipstick',\n",
       " 'ğŸ’…': 'nail_polish',\n",
       " 'ğŸ’†': 'face_massage',\n",
       " 'ğŸ’‡': 'haircut',\n",
       " 'ğŸ’ˆ': 'barber_pole',\n",
       " 'ğŸ’‰': 'syringe',\n",
       " 'ğŸ’Š': 'pill',\n",
       " 'ğŸ’‹': 'kiss_mark',\n",
       " 'ğŸ’Œ': 'love_letter',\n",
       " 'ğŸ’': 'ring',\n",
       " 'ğŸ’': 'gem_stone',\n",
       " 'ğŸ’': 'kiss',\n",
       " 'ğŸ’': 'bouquet',\n",
       " 'ğŸ’‘': 'couple_with_heart',\n",
       " 'ğŸ’’': 'wedding',\n",
       " 'ğŸ’“': 'beating_heart',\n",
       " 'ğŸ’”': 'broken_heart',\n",
       " 'ğŸ’•': 'two_hearts',\n",
       " 'ğŸ’–': 'sparkling_heart',\n",
       " 'ğŸ’—': 'growing_heart',\n",
       " 'ğŸ’˜': 'heart_with_arrow',\n",
       " 'ğŸ’™': 'blue_heart',\n",
       " 'ğŸ’š': 'green_heart',\n",
       " 'ğŸ’›': 'yellow_heart',\n",
       " 'ğŸ’œ': 'purple_heart',\n",
       " 'ğŸ’': 'heart_with_ribbon',\n",
       " 'ğŸ’': 'revolving_hearts',\n",
       " 'ğŸ’Ÿ': 'heart_decoration',\n",
       " 'ğŸ’ ': 'diamond_shape_with_a_dot_inside',\n",
       " 'ğŸ’¡': 'electric_light_bulb',\n",
       " 'ğŸ’¢': 'anger_symbol',\n",
       " 'ğŸ’£': 'bomb',\n",
       " 'ğŸ’¤': 'sleeping_symbol',\n",
       " 'ğŸ’¥': 'collision_symbol',\n",
       " 'ğŸ’¦': 'splashing_sweat_symbol',\n",
       " 'ğŸ’§': 'droplet',\n",
       " 'ğŸ’¨': 'dash_symbol',\n",
       " 'ğŸ’©': 'pile_of_poo',\n",
       " 'ğŸ’ª': 'flexed_biceps',\n",
       " 'ğŸ’«': 'dizzy_symbol',\n",
       " 'ğŸ’¬': 'speech_balloon',\n",
       " 'ğŸ’­': 'thought_balloon',\n",
       " 'ğŸ’®': 'white_flower',\n",
       " 'ğŸ’¯': 'hundred_points_symbol',\n",
       " 'ğŸ’°': 'money_bag',\n",
       " 'ğŸ’±': 'currency_exchange',\n",
       " 'ğŸ’²': 'heavy_dollar_sign',\n",
       " 'ğŸ’³': 'credit_card',\n",
       " 'ğŸ’´': 'banknote_with_yen_sign',\n",
       " 'ğŸ’µ': 'banknote_with_dollar_sign',\n",
       " 'ğŸ’¶': 'banknote_with_euro_sign',\n",
       " 'ğŸ’·': 'banknote_with_pound_sign',\n",
       " 'ğŸ’¸': 'money_with_wings',\n",
       " 'ğŸ’¹': 'chart_with_upwards_trend_and_yen_sign',\n",
       " 'ğŸ’º': 'seat',\n",
       " 'ğŸ’»': 'personal_computer',\n",
       " 'ğŸ’¼': 'briefcase',\n",
       " 'ğŸ’½': 'minidisc',\n",
       " 'ğŸ’¾': 'floppy_disk',\n",
       " 'ğŸ’¿': 'optical_disc',\n",
       " 'ğŸ“€': 'dvd',\n",
       " 'ğŸ“': 'file_folder',\n",
       " 'ğŸ“‚': 'open_file_folder',\n",
       " 'ğŸ“ƒ': 'page_with_curl',\n",
       " 'ğŸ“„': 'page_facing_up',\n",
       " 'ğŸ“…': 'calendar',\n",
       " 'ğŸ“†': 'tear_off_calendar',\n",
       " 'ğŸ“‡': 'card_index',\n",
       " 'ğŸ“ˆ': 'chart_with_upwards_trend',\n",
       " 'ğŸ“‰': 'chart_with_downwards_trend',\n",
       " 'ğŸ“Š': 'bar_chart',\n",
       " 'ğŸ“‹': 'clipboard',\n",
       " 'ğŸ“Œ': 'pushpin',\n",
       " 'ğŸ“': 'round_pushpin',\n",
       " 'ğŸ“': 'paperclip',\n",
       " 'ğŸ“': 'straight_ruler',\n",
       " 'ğŸ“': 'triangular_ruler',\n",
       " 'ğŸ“‘': 'bookmark_tabs',\n",
       " 'ğŸ“’': 'ledger',\n",
       " 'ğŸ““': 'notebook',\n",
       " 'ğŸ“”': 'notebook_with_decorative_cover',\n",
       " 'ğŸ“•': 'closed_book',\n",
       " 'ğŸ“–': 'open_book',\n",
       " 'ğŸ“—': 'green_book',\n",
       " 'ğŸ“˜': 'blue_book',\n",
       " 'ğŸ“™': 'orange_book',\n",
       " 'ğŸ“š': 'books',\n",
       " 'ğŸ“›': 'name_badge',\n",
       " 'ğŸ“œ': 'scroll',\n",
       " 'ğŸ“': 'memo',\n",
       " 'ğŸ“': 'telephone_receiver',\n",
       " 'ğŸ“Ÿ': 'pager',\n",
       " 'ğŸ“ ': 'fax_machine',\n",
       " 'ğŸ“¡': 'satellite_antenna',\n",
       " 'ğŸ“¢': 'public_address_loudspeaker',\n",
       " 'ğŸ“£': 'cheering_megaphone',\n",
       " 'ğŸ“¤': 'outbox_tray',\n",
       " 'ğŸ“¥': 'inbox_tray',\n",
       " 'ğŸ“¦': 'package',\n",
       " 'ğŸ“§': 'e_mail_symbol',\n",
       " 'ğŸ“¨': 'incoming_envelope',\n",
       " 'ğŸ“©': 'envelope_with_downwards_arrow_above',\n",
       " 'ğŸ“ª': 'closed_mailbox_with_lowered_flag',\n",
       " 'ğŸ“«': 'closed_mailbox_with_raised_flag',\n",
       " 'ğŸ“¬': 'open_mailbox_with_raised_flag',\n",
       " 'ğŸ“­': 'open_mailbox_with_lowered_flag',\n",
       " 'ğŸ“®': 'postbox',\n",
       " 'ğŸ“¯': 'postal_horn',\n",
       " 'ğŸ“°': 'newspaper',\n",
       " 'ğŸ“±': 'mobile_phone',\n",
       " 'ğŸ“²': 'mobile_phone_with_rightwards_arrow_at_left',\n",
       " 'ğŸ“³': 'vibration_mode',\n",
       " 'ğŸ“´': 'mobile_phone_off',\n",
       " 'ğŸ“µ': 'no_mobile_phones',\n",
       " 'ğŸ“¶': 'antenna_with_bars',\n",
       " 'ğŸ“·': 'camera',\n",
       " 'ğŸ“¸': 'camera_with_flash',\n",
       " 'ğŸ“¹': 'video_camera',\n",
       " 'ğŸ“º': 'television',\n",
       " 'ğŸ“»': 'radio',\n",
       " 'ğŸ“¼': 'videocassette',\n",
       " 'ğŸ“½': 'film_projector',\n",
       " 'ğŸ“¿': 'prayer_beads',\n",
       " 'ğŸ”€': 'twisted_rightwards_arrows',\n",
       " 'ğŸ”': 'clockwise_rightwards_and_leftwards_open_circle_arrows',\n",
       " 'ğŸ”‚': 'clockwise_rightwards_and_leftwards_open_circle_arrows_with_circled_one_overlay',\n",
       " 'ğŸ”ƒ': 'clockwise_downwards_and_upwards_open_circle_arrows',\n",
       " 'ğŸ”„': 'anticlockwise_downwards_and_upwards_open_circle_arrows',\n",
       " 'ğŸ”…': 'low_brightness_symbol',\n",
       " 'ğŸ”†': 'high_brightness_symbol',\n",
       " 'ğŸ”‡': 'speaker_with_cancellation_stroke',\n",
       " 'ğŸ”ˆ': 'speaker',\n",
       " 'ğŸ”‰': 'speaker_with_one_sound_wave',\n",
       " 'ğŸ”Š': 'speaker_with_three_sound_waves',\n",
       " 'ğŸ”‹': 'battery',\n",
       " 'ğŸ”Œ': 'electric_plug',\n",
       " 'ğŸ”': 'left_pointing_magnifying_glass',\n",
       " 'ğŸ”': 'right_pointing_magnifying_glass',\n",
       " 'ğŸ”': 'lock_with_ink_pen',\n",
       " 'ğŸ”': 'closed_lock_with_key',\n",
       " 'ğŸ”‘': 'key',\n",
       " 'ğŸ”’': 'lock',\n",
       " 'ğŸ”“': 'open_lock',\n",
       " 'ğŸ””': 'bell',\n",
       " 'ğŸ”•': 'bell_with_cancellation_stroke',\n",
       " 'ğŸ”–': 'bookmark',\n",
       " 'ğŸ”—': 'link_symbol',\n",
       " 'ğŸ”˜': 'radio_button',\n",
       " 'ğŸ”™': 'back_with_leftwards_arrow_above',\n",
       " 'ğŸ”š': 'end_with_leftwards_arrow_above',\n",
       " 'ğŸ”›': 'on_with_exclamation_mark_with_left_right_arrow_above',\n",
       " 'ğŸ”œ': 'soon_with_rightwards_arrow_above',\n",
       " 'ğŸ”': 'top_with_upwards_arrow_above',\n",
       " 'ğŸ”': 'no_one_under_eighteen_symbol',\n",
       " 'ğŸ”Ÿ': 'keycap_ten',\n",
       " 'ğŸ” ': 'input_symbol_for_latin_capital_letters',\n",
       " 'ğŸ”¡': 'input_symbol_for_latin_small_letters',\n",
       " 'ğŸ”¢': 'input_symbol_for_numbers',\n",
       " 'ğŸ”£': 'input_symbol_for_symbols',\n",
       " 'ğŸ”¤': 'input_symbol_for_latin_letters',\n",
       " 'ğŸ”¥': 'fire',\n",
       " 'ğŸ”¦': 'electric_torch',\n",
       " 'ğŸ”§': 'wrench',\n",
       " 'ğŸ”¨': 'hammer',\n",
       " 'ğŸ”©': 'nut_and_bolt',\n",
       " 'ğŸ”ª': 'hocho',\n",
       " 'ğŸ”«': 'pistol',\n",
       " 'ğŸ”¬': 'microscope',\n",
       " 'ğŸ”­': 'telescope',\n",
       " 'ğŸ”®': 'crystal_ball',\n",
       " 'ğŸ”¯': 'six_pointed_star_with_middle_dot',\n",
       " 'ğŸ”°': 'japanese_symbol_for_beginner',\n",
       " 'ğŸ”±': 'trident_emblem',\n",
       " 'ğŸ”²': 'black_square_button',\n",
       " 'ğŸ”³': 'white_square_button',\n",
       " 'ğŸ”´': 'large_red_circle',\n",
       " 'ğŸ”µ': 'large_blue_circle',\n",
       " 'ğŸ”¶': 'large_orange_diamond',\n",
       " 'ğŸ”·': 'large_blue_diamond',\n",
       " 'ğŸ”¸': 'small_orange_diamond',\n",
       " 'ğŸ”¹': 'small_blue_diamond',\n",
       " 'ğŸ”º': 'up_pointing_red_triangle',\n",
       " 'ğŸ”»': 'down_pointing_red_triangle',\n",
       " 'ğŸ”¼': 'up_pointing_small_red_triangle',\n",
       " 'ğŸ”½': 'down_pointing_small_red_triangle',\n",
       " 'ğŸ•‰': 'om_symbol',\n",
       " 'ğŸ•Š': 'dove_of_peace',\n",
       " 'ğŸ•‹': 'kaaba',\n",
       " 'ğŸ•Œ': 'mosque',\n",
       " 'ğŸ•': 'synagogue',\n",
       " 'ğŸ•': 'menorah_with_nine_branches',\n",
       " 'ğŸ•': 'clock_face_one_oclock',\n",
       " 'ğŸ•‘': 'clock_face_two_oclock',\n",
       " 'ğŸ•’': 'clock_face_three_oclock',\n",
       " 'ğŸ•“': 'clock_face_four_oclock',\n",
       " 'ğŸ•”': 'clock_face_five_oclock',\n",
       " 'ğŸ••': 'clock_face_six_oclock',\n",
       " 'ğŸ•–': 'clock_face_seven_oclock',\n",
       " 'ğŸ•—': 'clock_face_eight_oclock',\n",
       " 'ğŸ•˜': 'clock_face_nine_oclock',\n",
       " 'ğŸ•™': 'clock_face_ten_oclock',\n",
       " 'ğŸ•š': 'clock_face_eleven_oclock',\n",
       " 'ğŸ•›': 'clock_face_twelve_oclock',\n",
       " 'ğŸ•œ': 'clock_face_one_thirty',\n",
       " 'ğŸ•': 'clock_face_two_thirty',\n",
       " 'ğŸ•': 'clock_face_three_thirty',\n",
       " 'ğŸ•Ÿ': 'clock_face_four_thirty',\n",
       " 'ğŸ• ': 'clock_face_five_thirty',\n",
       " 'ğŸ•¡': 'clock_face_six_thirty',\n",
       " 'ğŸ•¢': 'clock_face_seven_thirty',\n",
       " 'ğŸ•£': 'clock_face_eight_thirty',\n",
       " 'ğŸ•¤': 'clock_face_nine_thirty',\n",
       " 'ğŸ•¥': 'clock_face_ten_thirty',\n",
       " 'ğŸ•¦': 'clock_face_eleven_thirty',\n",
       " 'ğŸ•§': 'clock_face_twelve_thirty',\n",
       " 'ğŸ•¯': 'candle',\n",
       " 'ğŸ•°': 'mantelpiece_clock',\n",
       " 'ğŸ•³': 'hole',\n",
       " 'ğŸ•´': 'man_in_business_suit_levitating',\n",
       " 'ğŸ•µ': 'sleuth_or_spy',\n",
       " 'ğŸ•¶': 'dark_sunglasses',\n",
       " 'ğŸ•·': 'spider',\n",
       " 'ğŸ•¸': 'spider_web',\n",
       " 'ğŸ•¹': 'joystick',\n",
       " 'ğŸ•º': 'man_dancing',\n",
       " 'ğŸ–‡': 'linked_paperclips',\n",
       " 'ğŸ–Š': 'lower_left_ballpoint_pen',\n",
       " 'ğŸ–‹': 'lower_left_fountain_pen',\n",
       " 'ğŸ–Œ': 'lower_left_paintbrush',\n",
       " 'ğŸ–': 'lower_left_crayon',\n",
       " 'ğŸ–': 'raised_hand_with_fingers_splayed',\n",
       " 'ğŸ–•': 'reversed_hand_with_middle_finger_extended',\n",
       " 'ğŸ––': 'raised_hand_with_part_between_middle_and_ring_fingers',\n",
       " 'ğŸ–¤': 'black_heart',\n",
       " 'ğŸ–¥': 'desktop_computer',\n",
       " 'ğŸ–¨': 'printer',\n",
       " 'ğŸ–±': 'three_button_mouse',\n",
       " 'ğŸ–²': 'trackball',\n",
       " 'ğŸ–¼': 'frame_with_picture',\n",
       " 'ğŸ—‚': 'card_index_dividers',\n",
       " 'ğŸ—ƒ': 'card_file_box',\n",
       " 'ğŸ—„': 'file_cabinet',\n",
       " 'ğŸ—‘': 'wastebasket',\n",
       " 'ğŸ—’': 'spiral_note_pad',\n",
       " 'ğŸ—“': 'spiral_calendar_pad',\n",
       " 'ğŸ—œ': 'compression',\n",
       " 'ğŸ—': 'old_key',\n",
       " 'ğŸ—': 'rolled_up_newspaper',\n",
       " 'ğŸ—¡': 'dagger_knife',\n",
       " 'ğŸ—£': 'speaking_head_in_silhouette',\n",
       " 'ğŸ—¨': 'left_speech_bubble',\n",
       " 'ğŸ—¯': 'right_anger_bubble',\n",
       " 'ğŸ—³': 'ballot_box_with_ballot',\n",
       " 'ğŸ—º': 'world_map',\n",
       " 'ğŸ—»': 'mount_fuji',\n",
       " 'ğŸ—¼': 'tokyo_tower',\n",
       " 'ğŸ—½': 'statue_of_liberty',\n",
       " 'ğŸ—¾': 'silhouette_of_japan',\n",
       " 'ğŸ—¿': 'moyai',\n",
       " 'ğŸ˜€': 'grinning_face',\n",
       " 'ğŸ˜': 'grinning_face_with_smiling_eyes',\n",
       " 'ğŸ˜‚': 'face_with_tears_of_joy',\n",
       " 'ğŸ˜ƒ': 'smiling_face_with_open_mouth',\n",
       " 'ğŸ˜„': 'smiling_face_with_open_mouth_and_smiling_eyes',\n",
       " 'ğŸ˜…': 'smiling_face_with_open_mouth_and_cold_sweat',\n",
       " 'ğŸ˜†': 'smiling_face_with_open_mouth_and_tightly_closed_eyes',\n",
       " 'ğŸ˜‡': 'smiling_face_with_halo',\n",
       " 'ğŸ˜ˆ': 'smiling_face_with_horns',\n",
       " 'ğŸ˜‰': 'winking_face',\n",
       " 'ğŸ˜Š': 'smiling_face_with_smiling_eyes',\n",
       " 'ğŸ˜‹': 'face_savouring_delicious_food',\n",
       " 'ğŸ˜Œ': 'relieved_face',\n",
       " 'ğŸ˜': 'smiling_face_with_heart_shaped_eyes',\n",
       " 'ğŸ˜': 'smiling_face_with_sunglasses',\n",
       " 'ğŸ˜': 'smirking_face',\n",
       " 'ğŸ˜': 'neutral_face',\n",
       " 'ğŸ˜‘': 'expressionless_face',\n",
       " 'ğŸ˜’': 'unamused_face',\n",
       " 'ğŸ˜“': 'face_with_cold_sweat',\n",
       " 'ğŸ˜”': 'pensive_face',\n",
       " 'ğŸ˜•': 'confused_face',\n",
       " 'ğŸ˜–': 'confounded_face',\n",
       " 'ğŸ˜—': 'kissing_face',\n",
       " 'ğŸ˜˜': 'face_throwing_a_kiss',\n",
       " 'ğŸ˜™': 'kissing_face_with_smiling_eyes',\n",
       " 'ğŸ˜š': 'kissing_face_with_closed_eyes',\n",
       " 'ğŸ˜›': 'face_with_stuck_out_tongue',\n",
       " 'ğŸ˜œ': 'face_with_stuck_out_tongue_and_winking_eye',\n",
       " 'ğŸ˜': 'face_with_stuck_out_tongue_and_tightly_closed_eyes',\n",
       " 'ğŸ˜': 'disappointed_face',\n",
       " 'ğŸ˜Ÿ': 'worried_face',\n",
       " 'ğŸ˜ ': 'angry_face',\n",
       " 'ğŸ˜¡': 'pouting_face',\n",
       " 'ğŸ˜¢': 'crying_face',\n",
       " 'ğŸ˜£': 'persevering_face',\n",
       " 'ğŸ˜¤': 'face_with_look_of_triumph',\n",
       " 'ğŸ˜¥': 'disappointed_but_relieved_face',\n",
       " 'ğŸ˜¦': 'frowning_face_with_open_mouth',\n",
       " 'ğŸ˜§': 'anguished_face',\n",
       " 'ğŸ˜¨': 'fearful_face',\n",
       " 'ğŸ˜©': 'weary_face',\n",
       " 'ğŸ˜ª': 'sleepy_face',\n",
       " 'ğŸ˜«': 'tired_face',\n",
       " 'ğŸ˜¬': 'grimacing_face',\n",
       " 'ğŸ˜­': 'loudly_crying_face',\n",
       " 'ğŸ˜®': 'face_with_open_mouth',\n",
       " 'ğŸ˜¯': 'hushed_face',\n",
       " 'ğŸ˜°': 'face_with_open_mouth_and_cold_sweat',\n",
       " 'ğŸ˜±': 'face_screaming_in_fear',\n",
       " 'ğŸ˜²': 'astonished_face',\n",
       " 'ğŸ˜³': 'flushed_face',\n",
       " 'ğŸ˜´': 'sleeping_face',\n",
       " 'ğŸ˜µ': 'dizzy_face',\n",
       " 'ğŸ˜¶': 'face_without_mouth',\n",
       " 'ğŸ˜·': 'face_with_medical_mask',\n",
       " 'ğŸ˜¸': 'grinning_cat_face_with_smiling_eyes',\n",
       " 'ğŸ˜¹': 'cat_face_with_tears_of_joy',\n",
       " 'ğŸ˜º': 'smiling_cat_face_with_open_mouth',\n",
       " 'ğŸ˜»': 'smiling_cat_face_with_heart_shaped_eyes',\n",
       " 'ğŸ˜¼': 'cat_face_with_wry_smile',\n",
       " 'ğŸ˜½': 'kissing_cat_face_with_closed_eyes',\n",
       " 'ğŸ˜¾': 'pouting_cat_face',\n",
       " 'ğŸ˜¿': 'crying_cat_face',\n",
       " 'ğŸ™€': 'weary_cat_face',\n",
       " 'ğŸ™': 'slightly_frowning_face',\n",
       " 'ğŸ™‚': 'slightly_smiling_face',\n",
       " 'ğŸ™ƒ': 'upside_down_face',\n",
       " 'ğŸ™„': 'face_with_rolling_eyes',\n",
       " 'ğŸ™…': 'face_with_no_good_gesture',\n",
       " 'ğŸ™†': 'face_with_ok_gesture',\n",
       " 'ğŸ™‡': 'person_bowing_deeply',\n",
       " 'ğŸ™ˆ': 'see_no_evil_monkey',\n",
       " 'ğŸ™‰': 'hear_no_evil_monkey',\n",
       " 'ğŸ™Š': 'speak_no_evil_monkey',\n",
       " 'ğŸ™‹': 'happy_person_raising_one_hand',\n",
       " 'ğŸ™Œ': 'person_raising_both_hands_in_celebration',\n",
       " 'ğŸ™': 'person_frowning',\n",
       " 'ğŸ™': 'person_with_pouting_face',\n",
       " 'ğŸ™': 'person_with_folded_hands',\n",
       " 'ğŸš€': 'rocket',\n",
       " 'ğŸš': 'helicopter',\n",
       " 'ğŸš‚': 'steam_locomotive',\n",
       " 'ğŸšƒ': 'railway_car',\n",
       " 'ğŸš„': 'high_speed_train',\n",
       " 'ğŸš…': 'high_speed_train_with_bullet_nose',\n",
       " 'ğŸš†': 'train',\n",
       " 'ğŸš‡': 'metro',\n",
       " 'ğŸšˆ': 'light_rail',\n",
       " 'ğŸš‰': 'station',\n",
       " 'ğŸšŠ': 'tram',\n",
       " 'ğŸš‹': 'tram_car',\n",
       " 'ğŸšŒ': 'bus',\n",
       " 'ğŸš': 'oncoming_bus',\n",
       " 'ğŸš': 'trolleybus',\n",
       " 'ğŸš': 'bus_stop',\n",
       " 'ğŸš': 'minibus',\n",
       " 'ğŸš‘': 'ambulance',\n",
       " 'ğŸš’': 'fire_engine',\n",
       " 'ğŸš“': 'police_car',\n",
       " 'ğŸš”': 'oncoming_police_car',\n",
       " 'ğŸš•': 'taxi',\n",
       " 'ğŸš–': 'oncoming_taxi',\n",
       " 'ğŸš—': 'automobile',\n",
       " 'ğŸš˜': 'oncoming_automobile',\n",
       " 'ğŸš™': 'recreational_vehicle',\n",
       " 'ğŸšš': 'delivery_truck',\n",
       " 'ğŸš›': 'articulated_lorry',\n",
       " 'ğŸšœ': 'tractor',\n",
       " 'ğŸš': 'monorail',\n",
       " 'ğŸš': 'mountain_railway',\n",
       " 'ğŸšŸ': 'suspension_railway',\n",
       " 'ğŸš ': 'mountain_cableway',\n",
       " 'ğŸš¡': 'aerial_tramway',\n",
       " 'ğŸš¢': 'ship',\n",
       " 'ğŸš£': 'rowboat',\n",
       " 'ğŸš¤': 'speedboat',\n",
       " 'ğŸš¥': 'horizontal_traffic_light',\n",
       " 'ğŸš¦': 'vertical_traffic_light',\n",
       " 'ğŸš§': 'construction_sign',\n",
       " 'ğŸš¨': 'police_cars_revolving_light',\n",
       " 'ğŸš©': 'triangular_flag_on_post',\n",
       " 'ğŸšª': 'door',\n",
       " 'ğŸš«': 'no_entry_sign',\n",
       " 'ğŸš¬': 'smoking_symbol',\n",
       " 'ğŸš­': 'no_smoking_symbol',\n",
       " 'ğŸš®': 'put_litter_in_its_place_symbol',\n",
       " 'ğŸš¯': 'do_not_litter_symbol',\n",
       " 'ğŸš°': 'potable_water_symbol',\n",
       " 'ğŸš±': 'non_potable_water_symbol',\n",
       " 'ğŸš²': 'bicycle',\n",
       " 'ğŸš³': 'no_bicycles',\n",
       " 'ğŸš´': 'bicyclist',\n",
       " 'ğŸšµ': 'mountain_bicyclist',\n",
       " 'ğŸš¶': 'pedestrian',\n",
       " 'ğŸš·': 'no_pedestrians',\n",
       " 'ğŸš¸': 'children_crossing',\n",
       " 'ğŸš¹': 'mens_symbol',\n",
       " 'ğŸšº': 'womens_symbol',\n",
       " 'ğŸš»': 'restroom',\n",
       " 'ğŸš¼': 'baby_symbol',\n",
       " 'ğŸš½': 'toilet',\n",
       " 'ğŸš¾': 'water_closet',\n",
       " 'ğŸš¿': 'shower',\n",
       " 'ğŸ›€': 'bath',\n",
       " 'ğŸ›': 'bathtub',\n",
       " 'ğŸ›‚': 'passport_control',\n",
       " 'ğŸ›ƒ': 'customs',\n",
       " 'ğŸ›„': 'baggage_claim',\n",
       " 'ğŸ›…': 'left_luggage',\n",
       " 'ğŸ›‹': 'couch_and_lamp',\n",
       " 'ğŸ›Œ': 'sleeping_accommodation',\n",
       " 'ğŸ›': 'shopping_bags',\n",
       " 'ğŸ›': 'bellhop_bell',\n",
       " 'ğŸ›': 'bed',\n",
       " 'ğŸ›': 'place_of_worship',\n",
       " 'ğŸ›‘': 'octagonal_sign',\n",
       " 'ğŸ›’': 'shopping_trolley',\n",
       " 'ğŸ› ': 'hammer_and_wrench',\n",
       " 'ğŸ›¡': 'shield',\n",
       " 'ğŸ›¢': 'oil_drum',\n",
       " 'ğŸ›£': 'motorway',\n",
       " 'ğŸ›¤': 'railway_track',\n",
       " 'ğŸ›¥': 'motor_boat',\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the emoji dataset\n",
    "def load_unicode_mapping():\n",
    "    emoji_dict = {}\n",
    "    with open(\"emoji_image_to_whatIs.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            emoji_dict[tokens[0]] = tokens[1]\n",
    "    return emoji_dict\n",
    "emoji_dict = load_unicode_mapping()\n",
    "emoji_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "tweetCounts = []\n",
    "# get unigram counts for data\n",
    "def emoji_counts():\n",
    "    uni = Counter()\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        for word in s:\n",
    "            count = sum(map(lambda word : 1 if word in emoji_dict else 0, s))\n",
    "            tweetCounts.append(count)\n",
    "    return find_quartile_values(tweetCounts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams for the data splitting on spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams():\n",
    "    unigrams = Counter()\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        for word in s:\n",
    "            unigrams[word] += 1\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make count features binary by finding median values over entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into quartiles?\n",
    "def find_quartile_values(counts):\n",
    "    counts.sort()\n",
    "    first = np.quantile(counts, .25)\n",
    "    second = np.quantile(counts, .5)\n",
    "    third = np.quantile(counts, .75)\n",
    "    return [first, second, third] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_at_bins():\n",
    "    at_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        at_counts.append(count)\n",
    "    return find_quartile_values(at_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_num_token_bins():\n",
    "    lens = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = len(s)\n",
    "        lens.append(count)\n",
    "    \n",
    "    return find_quartile_values(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_swear_bins():\n",
    "    bad_words_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_bad = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "        bad_words_counts.append(tot_bad)\n",
    "    \n",
    "    return find_quartile_values(bad_words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mention_bins():\n",
    "    mentions = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        lens.append(count)\n",
    "    \n",
    "    return find_quartile_values(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashtag_bins():\n",
    "    hashtag_counts = []\n",
    "    at_sum = 0\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '#' in word else 0, s))\n",
    "        hashtag_counts.append(count)\n",
    "    \n",
    "    return find_quartile_values(hashtag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 4.0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_misspelling_bins():\n",
    "    misspell_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_misspelled = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() not in words_set:\n",
    "                tot_misspelled+=1\n",
    "        misspell_counts.append(tot_misspelled)\n",
    "    \n",
    "    return find_quartile_values(misspell_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hatebase_bins():\n",
    "    hatabase_words_set = set(open(\"hatebase_terms.txt\").read().split())\n",
    "    hatebase_words_counts = []\n",
    "    \n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if word in hatabase_words_set else 0, s))\n",
    "        hatebase_words_counts.append(count)\n",
    "    \n",
    "    hatebase_words_counts.sort()\n",
    "    median = statistics.median(hatebase_words_counts) #Get Median for binning\n",
    "    \n",
    "    return [0, median]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hatebase'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f3b69016e41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhatebase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHatebaseAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rTUCCDVYCcsEGmVKzQJjKwDFQsNcvUNa'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhatebase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHatebaseAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hatebase'"
     ]
    }
   ],
   "source": [
    "#Get All Eng Vocabulary\n",
    "import json \n",
    "import requests\n",
    "import pandas as pd\n",
    "from hatebase import HatebaseAPI\n",
    "key = 'rTUCCDVYCcsEGmVKzQJjKwDFQsNcvUNa'\n",
    "hatebase = HatebaseAPI({\"key\": key})\n",
    "filters = {\"language\": \"eng\"}\n",
    "format = \"json\"\n",
    "# initialize list for all vocabulary entry dictionaries\n",
    "eng_vocab = []\n",
    "response = hatebase.getVocabulary(filters=filters, format=format)\n",
    "pages = response[\"number_of_pages\"]\n",
    "# fill the vocabulary list with all entries of all pages\n",
    "# this might take some time...\n",
    "for page in range(1, pages+1):\n",
    "    filters[\"page\"] = str(page) \n",
    "    response = hatebase.getVocabulary(filters=filters, format=format)\n",
    "    eng_vocab.append(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_vocab = pd.DataFrame()\n",
    "# fill df\n",
    "for elem in eng_vocab:\n",
    "    df_eng_vocab = df_eng_vocab.append(elem)\n",
    "# reset the df index\n",
    "df_eng_vocab.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_eng_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatebase_words_set = set(open(\"hatebase_terms.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_eng_vocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-5da393b42afe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Get lists of all words from hatebase pertaining to a certain category\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhb_religion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_about_religion'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mhb_sexual_orientation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_about_sexual_orientation'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhb_ethnicity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_about_ethnicity'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mhb_disability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf_eng_vocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_about_disability'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'term'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_eng_vocab' is not defined"
     ]
    }
   ],
   "source": [
    "#Get lists of all words from hatebase pertaining to a certain category\"\n",
    "hb_religion = df_eng_vocab.loc[df_eng_vocab['is_about_religion']][['term']].values\n",
    "hb_sexual_orientation = df_eng_vocab.loc[df_eng_vocab['is_about_sexual_orientation']][['term']].values\n",
    "hb_ethnicity = df_eng_vocab.loc[df_eng_vocab['is_about_ethnicity']][['term']].values\n",
    "hb_disability = df_eng_vocab.loc[df_eng_vocab['is_about_disability']][['term']].values\n",
    "hb_social_class = df_eng_vocab.loc[df_eng_vocab['is_about_class']][['term']].values\n",
    "hb_nationality = df_eng_vocab.loc[df_eng_vocab['is_about_nationality']][['term']].values\n",
    "hb_gender = df_eng_vocab.loc[df_eng_vocab['is_about_gender']][['term']].values\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_nationality():\n",
    "    is_about_nationality_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        totalIsAboutNationality = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_nationality'] == True:\n",
    "                        totalIsAboutNationality+=1\n",
    "        is_about_nationality_counts.append(totalIsAboutNationality)\n",
    "        \n",
    "    return find_quartile_values(is_about_nationality_counts)\n",
    "\n",
    "hits = is_about_nationality()\n",
    "len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_class():\n",
    "    is_about_class_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_class'] == True:\n",
    "                        total+=1\n",
    "        is_about_class_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_class_counts)\n",
    "\n",
    "hits = is_about_class()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_disability():\n",
    "    is_about_disability_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_disability'] == True:\n",
    "                        total+=1\n",
    "        is_about_disability_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_disability_counts)\n",
    "\n",
    "hits = is_about_disability()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def is_about_ethnicity():\n",
    "    is_about_ethnicity_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_ethnicity'] == True:\n",
    "                        total+=1\n",
    "        is_about_ethnicity_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_ethnicity_counts)\n",
    "\n",
    "hits = is_about_ethnicity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def is_about_gender():\n",
    "    is_about_gender_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_gender'] == True:\n",
    "                        total+=1\n",
    "        is_about_gender_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_gender_counts)\n",
    "\n",
    "hits = is_about_gender()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def is_about_religion():\n",
    "    is_about_religion_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_religion'] == True:\n",
    "                        total+=1\n",
    "        is_about_religion_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_religion_counts)\n",
    "\n",
    "hits = is_about_religion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_sexual_orientation():\n",
    "    is_about_sexual_orientation_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_sexual_orientation'] == True:\n",
    "                        total+=1\n",
    "        is_about_sexual_orientation_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_sexual_orientation_counts)\n",
    "\n",
    "hits = is_about_sexual_orientation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get lists of all words from hatebase pertaining to a certain category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_religion = df_eng_vocab.loc[df_eng_vocab['is_about_religion']][['term']].values\n",
    "hb_sexual_orientation = df_eng_vocab.loc[df_eng_vocab['is_about_sexual_orientation']][['term']].values\n",
    "hb_ethnicity = df_eng_vocab.loc[df_eng_vocab['is_about_ethnicity']][['term']].values\n",
    "hb_disability = df_eng_vocab.loc[df_eng_vocab['is_about_disability']][['term']].values\n",
    "hb_social_class = df_eng_vocab.loc[df_eng_vocab['is_about_class']][['term']].values\n",
    "hb_nationality = df_eng_vocab.loc[df_eng_vocab['is_about_nationality']][['term']].values\n",
    "hb_gender = df_eng_vocab.loc[df_eng_vocab['is_about_gender']][['term']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bins contain the splits for which bin a tweet's feature counts will land in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token_bins = make_num_token_bins()\n",
    "swear_bins = make_swear_bins()\n",
    "at_bins = make_at_bins()\n",
    "hashtag_bins = make_hashtag_bins()\n",
    "#hatebase_words_bins = hatebase_words_bins()\n",
    "misspell_bins =  make_misspelling_bins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_more_upper(tweet):\n",
    "    total_caps = sum(map(lambda ch : 1 if ch.isupper() else 0, tweet))\n",
    "    if total_caps > len(tweet) // 2:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_consecutive_punc(tweet):\n",
    "    for word in tweet.split():\n",
    "        if 'http://' in word: continue\n",
    "        for i in range(len(word)-1):\n",
    "            if word[i] in string.punctuation and word[i+1] in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_person = ['he', 'she', 'they', 'him', 'her', 'them', 'his', 'hers', 'their', 'theirs', 'themselves', 'himself', 'herself']\n",
    "second_person = ['you', 'your', 'yours']\n",
    "first_person =['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if first word in tweet is a third person pronoun. Also check if number of third person pronouns is greater\n",
    "# than first person pronouns\n",
    "def get_pronouns(tweet):\n",
    "    first_pronoun = 0\n",
    "    third_and_second_greater_than_first = 0\n",
    "    words = tweet.split()\n",
    "    if words[0] in third_person or words[0] in second_person: \n",
    "        first_pronoun = 1\n",
    "    \n",
    "    first_person_count = 0\n",
    "    third_second_person_count = 0\n",
    "    for word in words:\n",
    "        if word in third_person or word in second_person:\n",
    "            third_second_person_count += 1\n",
    "        elif word in first_person:\n",
    "            first_person_count += 1\n",
    "            \n",
    "    if first_person_count < third_second_person_count:\n",
    "        third_and_second_greater_than_first = 1\n",
    "       \n",
    "    return first_pronoun, third_and_second_greater_than_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite lexicon:\n",
    "    Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \n",
    "        Proceedings of the ACM SIGKDD International Conference on Knowledge \n",
    "        Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \n",
    "        Washington, USA,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = set(open(\"opinion-lexicon-English/positive-words.txt\").read().split())\n",
    "neg_words = set(open(\"opinion-lexicon-English/negative-words.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns 1, 1 if negative words outnumber positive words and there are no positive words\n",
    "def get_sentiment(tweet):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for word in tweet.split():\n",
    "        if word in pos_words:\n",
    "            pos_count += 1\n",
    "        if word in neg_words:\n",
    "            neg_count += 1\n",
    "    if neg_count > pos_count:\n",
    "        if pos_count > 0:\n",
    "            return 1, 0\n",
    "        else:\n",
    "            return 1, 1\n",
    "    else:\n",
    "        if pos_count > 0:\n",
    "            return 0, 0\n",
    "        else:\n",
    "            return 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns one if a word in the tweet has non alphanumeric characters (not including punctuation at the end of a word)\n",
    "def contains_non_alphanum(tweet):\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if not word.isalnum():\n",
    "            if word[-1] not in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most common unigrams\n",
    "unigram_counts = get_unigrams()\n",
    "top_unigrams = unigram_counts.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each count feature has four bins: 1 for < 25th percentile, 2 for < 50th percentile, \n",
    "#3 for < 75th percentile, and 4 for < 100th percentile\n",
    "def find_bin(count, bin_name):\n",
    "    for i in range(len(bin_name)):\n",
    "        if count < bin_name[i]:\n",
    "            return i + 1\n",
    "    return len(bin_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the tweets by getting their feature representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets, judgements):\n",
    "#     for word in [u[0] for u in top_unigrams]:\n",
    "#         tweets[word] = tweets['tweet'].str.contains(word).astype(int)\n",
    "    word_counts = []\n",
    "    swear_counts = []\n",
    "    at_counts = []\n",
    "    contains_at = []\n",
    "    hashtag_counts = []\n",
    "    contains_hashtag = []\n",
    "    consecutive_punc = []\n",
    "    more_upper = []\n",
    "    first_pronoun = []\n",
    "    fewer_first_person = []\n",
    "    more_negative = []\n",
    "    no_positive = []\n",
    "    contains_url = []\n",
    "    not_alphanum = []\n",
    "    misspellings = []\n",
    "    disagreements = []\n",
    "    in_hatebase = []\n",
    "    about_gender = []\n",
    "    about_religion = []\n",
    "    about_sexual_orientation = []\n",
    "    about_ethnicity = []\n",
    "    about_disability = []\n",
    "    about_social_class = []\n",
    "    about_nationality = []\n",
    "\n",
    "    for tweet in tweets['tweet']:\n",
    "        #count tokens\n",
    "        tweet_words = tweet.split()\n",
    "        num_token_bin = find_bin(len(tweet_words), num_token_bins)\n",
    "        word_counts.append(num_token_bin)\n",
    "        \n",
    "        #count swear words, misspellings, and if a word is in hatebase\n",
    "        misspell_count = 0\n",
    "        tot_bad = 0\n",
    "        hatebase = 0\n",
    "        hatebase_words = []\n",
    "        for word in tweet_words:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "            if word.lower() not in words_set:\n",
    "                misspell_count+=1\n",
    "            if word.lower() in hatebase_words_set:\n",
    "                hatebase = 1\n",
    "                hatebase_words.append(word)\n",
    "        swear_bin = find_bin(tot_bad, swear_bins)\n",
    "        swear_counts.append(swear_bin)\n",
    "        misspell_bin = find_bin(misspell_count, misspell_bins)\n",
    "        misspellings.append(misspell_bin)\n",
    "        in_hatebase.append(hatebase)\n",
    "    \n",
    "        gender = religion = sexual_orientation = ethnicity = disability = social_class = nationality = 0\n",
    "        if hatebase == 1:\n",
    "            for word in hatebase_words:\n",
    "                if word in hb_gender: gender = 1\n",
    "                if word in hb_religion: religion = 1\n",
    "                if word in hb_sexual_orientation: sexual_orientation = 1\n",
    "                if word in hb_ethnicity: ethnicity = 1\n",
    "                if word in hb_disability: disability = 1\n",
    "                if word in hb_social_class: social_class = 1\n",
    "                if word in hb_nationality: nationality = 1\n",
    "        about_gender.append(gender)\n",
    "        about_religion.append(religion)\n",
    "        about_sexual_orientation.append(sexual_orientation)\n",
    "        about_ethnicity.append(ethnicity)\n",
    "        about_disability.append(disability)\n",
    "        about_social_class.append(social_class)\n",
    "        about_nationality.append(nationality)\n",
    "        \n",
    "        #count mentions\n",
    "        at_count = tweet.count('@')\n",
    "        if at_count > 0:\n",
    "            contains_at.append(1)\n",
    "        else:\n",
    "            contains_at.append(0)\n",
    "        at_bin = find_bin(at_count, at_bins)\n",
    "        at_counts.append(at_bin)\n",
    "        \n",
    "        #count hashtags\n",
    "        hash_count = tweet.count('#')\n",
    "        if hash_count > 0:\n",
    "            contains_hashtag.append(1)\n",
    "        else:\n",
    "            contains_hashtag.append(0)\n",
    "        hash_bin = find_bin(hash_count, hashtag_bins)\n",
    "        hashtag_counts.append(hash_bin)\n",
    "\n",
    "        more_upper.append(has_more_upper(tweet))\n",
    "        consecutive_punc.append(has_consecutive_punc(tweet))\n",
    "        first, more = get_pronouns(tweet)\n",
    "        first_pronoun.append(first)\n",
    "        fewer_first_person.append(more)\n",
    "        \n",
    "        more_neg, pos = get_sentiment(tweet)\n",
    "        more_negative.append(more_neg)\n",
    "        no_positive.append(pos)\n",
    "        \n",
    "        if 'http://' in tweet:\n",
    "            contains_url.append(1)\n",
    "        else:\n",
    "            contains_url.append(0)\n",
    "            \n",
    "        not_alphanum.append(contains_non_alphanum(tweet))\n",
    "        \n",
    "        #See if there were disagreements about classification\n",
    "        i = tweets.loc[tweets['tweet']==tweet].index[0]\n",
    "        total_votes = judgements.at[i, 'count']\n",
    "        if (judgements.at[i, 'hate_speech'] == total_votes) or (judgements.at[i, 'offensive_language'] == total_votes)\\\n",
    "            or (judgements.at[i, 'neither'] == total_votes):\n",
    "            disagreements.append(0)\n",
    "        else:\n",
    "            disagreements.append(1)\n",
    "        \n",
    "\n",
    "    tweets['Word Counts'] = word_counts\n",
    "    tweets['Swear Counts'] = swear_counts\n",
    "    tweets['@ Counts'] = at_counts\n",
    "    tweets['Mention'] = contains_at\n",
    "    tweets['Hashtag Counts'] = hashtag_counts\n",
    "    tweets['Contains Hashtag'] = contains_hashtag\n",
    "    tweets['Consecutive Punctuation'] = consecutive_punc\n",
    "    tweets['Majority Uppercase Letters'] = more_upper\n",
    "    tweets['First Word Second or Third Person Pronoun'] = first_pronoun\n",
    "    tweets['More Second or Third Person Pronouns than First'] = fewer_first_person\n",
    "    tweets['Majority Negative Words'] = more_negative\n",
    "    tweets['No Positive Words'] = no_positive\n",
    "    tweets['Contains URL'] = contains_url\n",
    "    tweets['Contains Non Alphanumeric Word'] = not_alphanum\n",
    "    tweets['Misspelling Count'] = misspellings\n",
    "    tweets['Judgement Disagreements'] = disagreements\n",
    "    tweets['About Gender (Hatebase)'] = about_gender\n",
    "    tweets['About Religion (Hatebase)'] = about_religion\n",
    "    tweets['About Ethnicity (Hatebase)'] = about_ethnicity\n",
    "    tweets['About Sexual Orientation (Hatebase)'] = about_sexual_orientation\n",
    "    tweets['About Disability (Hatebase)'] = about_disability\n",
    "    tweets['About Class (Hatebase)'] = about_social_class\n",
    "    tweets['About Nationality (Hatebase)'] = about_nationality\n",
    "    X = tweets[[col for col in tweets.columns if col!=\"tweet\"]].values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine train and dev sets for k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, dev_data], sort=False)\n",
    "tweets = data[['tweet']]\n",
    "judgements = data[['count', 'hate_speech', 'offensive_language', 'neither']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X = process_tweets(tweets, judgements)\n",
    "y = data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22214, 23)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10-fold cross validation on combined training and dev sets on LR, SVM, and NB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(fit_intercept=True, max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "lr_y_pred = cross_val_predict(LR, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.06      0.10      1282\n",
      "           1       0.87      0.92      0.89     17186\n",
      "           2       0.62      0.64      0.63      3746\n",
      "\n",
      "    accuracy                           0.82     22214\n",
      "   macro avg       0.66      0.54      0.54     22214\n",
      "weighted avg       0.80      0.82      0.80     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LR:\", classification_report(y, lr_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1282\n",
      "           1       0.87      0.92      0.89     17186\n",
      "           2       0.63      0.65      0.64      3746\n",
      "\n",
      "    accuracy                           0.82     22214\n",
      "   macro avg       0.50      0.53      0.51     22214\n",
      "weighted avg       0.78      0.82      0.80     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(gamma='auto') \n",
    "svm_y_pred = cross_val_predict(svm, X, y, cv=cv)\n",
    "print(classification_report(y, svm_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.59      0.29      1282\n",
      "           1       0.95      0.75      0.84     17186\n",
      "           2       0.55      0.69      0.61      3746\n",
      "\n",
      "    accuracy                           0.73     22214\n",
      "   macro avg       0.56      0.68      0.58     22214\n",
      "weighted avg       0.84      0.73      0.77     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "nb_y_pred = cross_val_predict(gnb, X, y, cv=cv)\n",
    "print(classification_report(y, nb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

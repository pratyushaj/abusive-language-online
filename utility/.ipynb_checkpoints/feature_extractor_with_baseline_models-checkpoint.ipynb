{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import statistics\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "    \n",
    "TRAIN = '../data/train/training_data.csv'\n",
    "train_data = pd.read_csv(TRAIN, index_col=1)\n",
    "dev_data = pd.read_csv('../data/dev/development_data.csv', index_col=1)\n",
    "\n",
    "tweets = train_data[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "words_set = set(open(\"allwords.txt\").read().split())\n",
    "words_set = set(item.lower() for item in words_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unigrams for the data splitting on spaces"
   ]
  },
  {
   "cell_type": "code",

   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unigrams():\n",
    "    unigrams = Counter()\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        for word in s:\n",
    "            unigrams[word] += 1\n",
    "    return unigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def make_emoji_dict():\n",
    "    emoji_dict = {}\n",
    "    with open(\"emoji_image_to_whatIs.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            emoji_dict[tokens[0]] = tokens[1]\n",
    "    return emoji_dict\n",
    "emoji_dict = make_emoji_dict()\n",
    "#emoji_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "tweetCounts = []\n",
    "# get emoji counts for data\n",
    "def emoji_counts():\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        for word in s:\n",
    "            count = sum(map(lambda word : 1 if word in emoji_dict else 0, s))\n",
    "            tweetCounts.append(count)\n",
    "    return find_quartile_values(tweetCounts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Sentiment Features\n",
    "\n",
    "Emoji sentiment dictionary\n",
    "\n",
    "Features:\n",
    "Negative Sentiment,\n",
    "Neutral Sentiment,\n",
    "Positive Sentiment,\n",
    "Overall Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emoji\tNeg[0...1]\tNeut[0...1]\tPos[0...1]\tSentiment score[-1...+1]\tUnicode name\tUnicode block\n",
    "\n",
    "'''Derived from:\n",
    "@article{Kralj2015emojis,\n",
    "  author={{Kralj Novak}, Petra and Smailovi{\\'c}, Jasmina and Sluban, Borut and Mozeti\\v{c}, Igor},\n",
    "  title={Sentiment of emojis},\n",
    "  journal={PLoS ONE},\n",
    "  volume={10},\n",
    "  number={12},\n",
    "  pages={e0144296},\n",
    "  url={http://dx.doi.org/10.1371/journal.pone.0144296},\n",
    "  year={2015}\n",
    "}\n",
    "'''\n",
    "\n",
    "# loading the emoji dataset\n",
    "def make_emoji_sentiment_dict():\n",
    "    emoji_sentiment_dict = {}\n",
    "    with open(\"emoji_image_sentimentScore_definition_category.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            #each emoji has a list of 5 terms for a value\n",
    "            #emoji_sentiment_dict[tokens[0]] = [('neg':tokens[1]), ('neutral':tokens[2]), ('positive':tokens[3]), ('sentiment score':tokens[4]), ('name':tokens[5]), ('category':tokens[6])]\n",
    "            emoji_sentiment_dict[tokens[0]] = [tokens[1], tokens[2], tokens[3], tokens[4], tokens[5], tokens[6]]\n",
    "    return emoji_sentiment_dict\n",
    "emoji_sentiment_dict = make_emoji_sentiment_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, '0.247']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "negativeValues = []\n",
    "\n",
    "def emoji_negative_sentiment():\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #negative  \n",
    "                negativeValues.append(emoji_sentiment_dict[word][0])\n",
    "    median = statistics.median(negativeValues)\n",
    "    return [0, median]\n",
    "emoji_negative_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, '0.285']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "neutralValues = []\n",
    "\n",
    "def emoji_neutral_sentiment():\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #neutral \n",
    "                neutralValues.append(emoji_sentiment_dict[word][1])\n",
    "    median = statistics.median(neutralValues)\n",
    "    return [0, median]\n",
    "emoji_neutral_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, '0.468']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "positiveValues = []\n",
    "\n",
    "def emoji_positive_sentiment():\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #positive \n",
    "                positiveValues.append(emoji_sentiment_dict[word][2])\n",
    "    median = statistics.median(positiveValues)\n",
    "    return [0, median]\n",
    "emoji_positive_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, '0.221']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "sentimentValues = []\n",
    "\n",
    "def emoji_overall_sentiment():\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                #sentiment\n",
    "                sentimentValues.append(emoji_sentiment_dict[word][3])\n",
    "    median = statistics.median(sentimentValues)\n",
    "    return [0, median]\n",
    "emoji_overall_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Sentiment Features\n",
    "\n",
    "Emoji Feature: highest sentiment category for emojis in tweet \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#NOTE: I need to work on this but am going to leave for now until we talk about how you\n",
    "#updated the hatebase features and others. \n",
    "\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "\n",
    "def emoji_highest_sentiment():\n",
    "    highestCounts = [0, 0, 0]\n",
    "    indices = []\n",
    "\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        emojiFlag = False\n",
    "        highestCountPerTweet = [0, 0, 0]\n",
    "        for word in s:\n",
    "            if word in emoji_sentiment_dict:\n",
    "                emojiFlag = True\n",
    "                # neg, neu, pos\n",
    "                sentiments = [float(emoji_sentiment_dict[word][0]), float(emoji_sentiment_dict[word][1]), float(emoji_sentiment_dict[word][2])]\n",
    "                max_value = max(sentiments)\n",
    "                max_index = sentiments.index(max_value)\n",
    "                highestCounts[max_index] += 1\n",
    "                highestCountPerTweet[max_index] += 1 \n",
    "        if emojiFlag:\n",
    "            max_value = max(highestCountPerTweet)\n",
    "            highest = highestCountPerTweet.index(max_value)\n",
    "            if highest == 0:\n",
    "                indices.append(-1)\n",
    "            elif highest == 1:\n",
    "                indices.append(-0)\n",
    "            elif highest == 2:\n",
    "                indices.append(1)\n",
    "    #print(indices)      \n",
    "    return find_quartile_values(indices)\n",
    "emoji_highest_sentiment()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make count features binary by finding median values over entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into quartiles?\n",
    "def find_quartile_values(counts):\n",
    "    counts.sort()\n",
    "    first = np.quantile(counts, .25)\n",
    "    second = np.quantile(counts, .5)\n",
    "    third = np.quantile(counts, .75)\n",
    "    return [first, second, third] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_at_bins():\n",
    "    at_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        at_counts.append(count)\n",
    "    return find_quartile_values(at_counts)\n",
    "make_at_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.0, 13.0, 19.0]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_num_token_bins():\n",
    "    lens = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = len(s)\n",
    "        lens.append(count)\n",
    "    \n",
    "    return find_quartile_values(lens)\n",
    "make_num_token_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_swear_bins():\n",
    "    bad_words_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_bad = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "        bad_words_counts.append(tot_bad)\n",
    "    \n",
    "    return find_quartile_values(bad_words_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_mention_bins():\n",
    "    mentions = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        mentions.append(count)\n",
    "    \n",
    "    return find_quartile_values(mentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_hashtag_bins():\n",
    "    hashtag_counts = []\n",
    "    at_sum = 0\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '#' in word else 0, s))\n",
    "        hashtag_counts.append(count)\n",
    "    \n",
    "    return find_quartile_values(hashtag_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 4.0]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_misspelling_bins():\n",
    "    misspell_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_misspelled = 0\n",
    "        for word in s:\n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\").replace(\";\",\"\")\n",
    "            if word.lower() not in words_set:\n",
    "                tot_misspelled+=1\n",
    "        misspell_counts.append(tot_misspelled)\n",
    "    \n",
    "    return find_quartile_values(misspell_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2.0]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_hatebase_bins():\n",
    "    hatabase_words_set = set(open(\"hatebase_terms.txt\").read().split())\n",
    "    hatebase_words_counts = []\n",
    "    \n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if word in hatabase_words_set else 0, s))\n",
    "        hatebase_words_counts.append(count)\n",
    "    \n",
    "    hatebase_words_counts.sort()\n",
    "    median = statistics.median(hatebase_words_counts) #Get Median for binning\n",
    "    \n",
    "    return [0, median]\n",
    "make_hatebase_bins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profanity and Racist Lexicon counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#profanity_banned from trinker-lexicon-4c5e22b\n",
    "profanity_banned = set([\"anal\", \"anus\", \"arse\", \"ass\", \"balls\", \"ballsack\", \"bastard\", \n",
    "\"biatch\", \"bitch\", \"bloody\", \"blow job\", \"blowjob\", \"bollock\", \n",
    "\"bollok\", \"boner\", \"boob\", \"bugger\", \"bum\", \"butt\", \"buttplug\", \n",
    "\"clitoris\", \"cock\", \"coon\", \"crap\", \"cunt\", \"damn\", \"dick\", \"dildo\", \n",
    "\"dyke\", \"f u c k\", \"fag\", \"feck\", \"felching\", \"fellate\", \"fellatio\", \n",
    "\"flange\", \"fuck\", \"fudge packer\", \"fudgepacker\", \"God damn\", \n",
    "\"Goddamn\", \"hell\", \"homo\", \"jerk\", \"jizz\", \"knob end\", \"knobend\", \n",
    "\"labia\", \"lmao\", \"lmfao\", \"muff\", \"nigga\", \"nigger\", \"omg\", \"penis\", \n",
    "\"piss\", \"poop\", \"prick\", \"pube\", \"pussy\", \"queer\", \"s hit\", \"scrotum\", \n",
    "\"sex\", \"sh1t\", \"shit\", \"slut\", \"smegma\", \"spunk\", \"tit\", \"tosser\", \n",
    "\"turd\", \"twat\", \"vagina\", \"wank\", \"whore\", \"wtf\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_profanity_bins():\n",
    "    profanity = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if word in profanity_banned else 0, s))\n",
    "        \n",
    "        profanity.append(count)\n",
    "    \n",
    "    return find_quartile_values(profanity)\n",
    "#make_profanity_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the emoji dataset\n",
    "def load_racist_lexicon():\n",
    "    racist_lexicon = []\n",
    "    with open(\"racist_lexicon.txt\", 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            tokens = line.strip().split('\\t')\n",
    "            word = tokens[1].split(\"\\\"\") \n",
    "            racist_lexicon.append(word[1])\n",
    "    racist_lexicon = set(racist_lexicon)\n",
    "    return racist_lexicon\n",
    "racist_lexicon = load_racist_lexicon()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE  ------ to discuss ------\n",
    "#very sparse, maybe this should be contains as if it's a word in that lexicon, \n",
    "#it's pretty darn offensive if not guaranteed to be hate speech\n",
    "\n",
    "def make_racist_lexicon_bins():\n",
    "    racist_words = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if word in racist_lexicon else 0, s))\n",
    "    \n",
    "    return find_quartile_values(racist_words)\n",
    "#make_racist_lexicon_bins()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hatebase'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f3b69016e41d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhatebase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHatebaseAPI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'rTUCCDVYCcsEGmVKzQJjKwDFQsNcvUNa'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhatebase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHatebaseAPI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"key\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'hatebase'"
     ]
    }
   ],
   "source": [
    "#Get All Eng Vocabulary\n",
    "import json \n",
    "import requests\n",
    "import pandas as pd\n",
    "from hatebase import HatebaseAPI\n",
    "key = 'rTUCCDVYCcsEGmVKzQJjKwDFQsNcvUNa'\n",
    "hatebase = HatebaseAPI({\"key\": key})\n",
    "filters = {\"language\": \"eng\"}\n",
    "format = \"json\"\n",
    "# initialize list for all vocabulary entry dictionaries\n",
    "eng_vocab = []\n",
    "response = hatebase.getVocabulary(filters=filters, format=format)\n",
    "pages = response[\"number_of_pages\"]\n",
    "# fill the vocabulary list with all entries of all pages\n",
    "# this might take some time...\n",
    "for page in range(1, pages+1):\n",
    "    filters[\"page\"] = str(page) \n",
    "    response = hatebase.getVocabulary(filters=filters, format=format)\n",
    "    eng_vocab.append(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_vocab = pd.DataFrame()\n",
    "# fill df\n",
    "for elem in eng_vocab:\n",
    "    df_eng_vocab = df_eng_vocab.append(elem)\n",
    "# reset the df index\n",
    "df_eng_vocab.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_eng_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "hatebase_words_set = set(open(\"hatebase_terms.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_nationality():\n",
    "    is_about_nationality_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        totalIsAboutNationality = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_nationality'] == True:\n",
    "                        totalIsAboutNationality+=1\n",
    "        is_about_nationality_counts.append(totalIsAboutNationality)\n",
    "        \n",
    "    return find_quartile_values(is_about_nationality_counts)\n",
    "\n",
    "hits = is_about_nationality()\n",
    "len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_class():\n",
    "    is_about_class_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_class'] == True:\n",
    "                        total+=1\n",
    "        is_about_class_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_class_counts)\n",
    "\n",
    "hits = is_about_class()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_disability():\n",
    "    is_about_disability_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_disability'] == True:\n",
    "                        total+=1\n",
    "        is_about_disability_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_disability_counts)\n",
    "\n",
    "hits = is_about_disability()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def is_about_ethnicity():\n",
    "    is_about_ethnicity_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_ethnicity'] == True:\n",
    "                        total+=1\n",
    "        is_about_ethnicity_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_ethnicity_counts)\n",
    "\n",
    "hits = is_about_ethnicity()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def is_about_gender():\n",
    "    is_about_gender_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_gender'] == True:\n",
    "                        total+=1\n",
    "        is_about_gender_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_gender_counts)\n",
    "\n",
    "hits = is_about_gender()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "def is_about_religion():\n",
    "    is_about_religion_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_religion'] == True:\n",
    "                        total+=1\n",
    "        is_about_religion_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_religion_counts)\n",
    "\n",
    "hits = is_about_religion()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def is_about_sexual_orientation():\n",
    "    is_about_sexual_orientation_counts = []\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        total = 0\n",
    "        #check if it's a hatebase word\n",
    "        for word in s:\n",
    "            if word in hatabase_words_set:\n",
    "                #select_indices = list(np.where(df_eng_vocab['is_about_nationality'] == True)[0])\n",
    "                for index, hb_row in df_eng_vocab.iterrows():\n",
    "                    if word == hb_row['term'] and hb_row['is_about_sexual_orientation'] == True:\n",
    "                        total+=1\n",
    "        is_about_sexual_orientation_counts.append(total)\n",
    "        \n",
    "    return find_quartile_values(is_about_sexual_orientation_counts)\n",
    "\n",
    "hits = is_about_sexual_orientation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get lists of all words from hatebase pertaining to a certain category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "hb_religion = df_eng_vocab.loc[df_eng_vocab['is_about_religion']][['term']].values\n",
    "hb_sexual_orientation = df_eng_vocab.loc[df_eng_vocab['is_about_sexual_orientation']][['term']].values\n",
    "hb_ethnicity = df_eng_vocab.loc[df_eng_vocab['is_about_ethnicity']][['term']].values\n",
    "hb_disability = df_eng_vocab.loc[df_eng_vocab['is_about_disability']][['term']].values\n",
    "hb_social_class = df_eng_vocab.loc[df_eng_vocab['is_about_class']][['term']].values\n",
    "hb_nationality = df_eng_vocab.loc[df_eng_vocab['is_about_nationality']][['term']].values\n",
    "hb_gender = df_eng_vocab.loc[df_eng_vocab['is_about_gender']][['term']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bins contain the splits for which bin a tweet's feature counts will land in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_token_bins = make_num_token_bins()\n",
    "swear_bins = make_swear_bins()\n",
    "at_bins = make_at_bins()\n",
    "hashtag_bins = make_hashtag_bins()\n",
    "#hatebase_words_bins = hatebase_words_bins()\n",
    "misspell_bins =  make_misspelling_bins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_more_upper(tweet):\n",
    "    total_caps = sum(map(lambda ch : 1 if ch.isupper() else 0, tweet))\n",
    "    if total_caps > len(tweet) // 2:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_consecutive_punc(tweet):\n",
    "    for word in tweet.split():\n",
    "        if 'http://' in word: continue\n",
    "        for i in range(len(word)-1):\n",
    "            if word[i] in string.punctuation and word[i+1] in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_person = ['he', 'she', 'they', 'him', 'her', 'them', 'his', 'hers', 'their', 'theirs', 'themselves', 'himself', 'herself']\n",
    "second_person = ['you', 'your', 'yours']\n",
    "first_person =['i', 'me', 'my', 'mine', 'we', 'us', 'our', 'ours']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find if first word in tweet is a third person pronoun. Also check if number of third person pronouns is greater\n",
    "# than first person pronouns\n",
    "def get_pronouns(tweet):\n",
    "    first_pronoun = 0\n",
    "    third_and_second_greater_than_first = 0\n",
    "    words = tweet.split()\n",
    "    if words[0] in third_person or words[0] in second_person: \n",
    "        first_pronoun = 1\n",
    "    \n",
    "    first_person_count = 0\n",
    "    third_second_person_count = 0\n",
    "    for word in words:\n",
    "        if word in third_person or word in second_person:\n",
    "            third_second_person_count += 1\n",
    "        elif word in first_person:\n",
    "            first_person_count += 1\n",
    "            \n",
    "    if first_person_count < third_second_person_count:\n",
    "        third_and_second_greater_than_first = 1\n",
    "       \n",
    "    return first_pronoun, third_and_second_greater_than_first"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite lexicon:\n",
    "    Minqing Hu and Bing Liu. \"Mining and Summarizing Customer Reviews.\" \n",
    "        Proceedings of the ACM SIGKDD International Conference on Knowledge \n",
    "        Discovery and Data Mining (KDD-2004), Aug 22-25, 2004, Seattle, \n",
    "        Washington, USA,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_words = set(open(\"opinion-lexicon-English/positive-words.txt\").read().split())\n",
    "neg_words = set(open(\"opinion-lexicon-English/negative-words.txt\").read().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns 1, 1 if negative words outnumber positive words and there are no positive words\n",
    "def get_sentiment(tweet):\n",
    "    pos_count = 0\n",
    "    neg_count = 0\n",
    "    for word in tweet.split():\n",
    "        if word in pos_words:\n",
    "            pos_count += 1\n",
    "        if word in neg_words:\n",
    "            neg_count += 1\n",
    "    if neg_count > pos_count:\n",
    "        if pos_count > 0:\n",
    "            return 1, 0\n",
    "        else:\n",
    "            return 1, 1\n",
    "    else:\n",
    "        if pos_count > 0:\n",
    "            return 0, 0\n",
    "        else:\n",
    "            return 0, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns one if a word in the tweet has non alphanumeric characters (not including punctuation at the end of a word)\n",
    "def contains_non_alphanum(tweet):\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        if not word.isalnum():\n",
    "            if word[-1] not in string.punctuation:\n",
    "                return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find most common unigrams\n",
    "unigram_counts = get_unigrams()\n",
    "top_unigrams = unigram_counts.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#each count feature has four bins: 1 for < 25th percentile, 2 for < 50th percentile, \n",
    "#3 for < 75th percentile, and 4 for < 100th percentile\n",
    "def find_bin(count, bin_name):\n",
    "    for i in range(len(bin_name)):\n",
    "        if count < bin_name[i]:\n",
    "            return i + 1\n",
    "    return len(bin_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process the tweets by getting their feature representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets, judgements):\n",
    "#     for word in [u[0] for u in top_unigrams]:\n",
    "#         tweets[word] = tweets['tweet'].str.contains(word).astype(int)\n",
    "    word_counts = []\n",
    "    swear_counts = []\n",
    "    at_counts = []\n",
    "    contains_at = []\n",
    "    hashtag_counts = []\n",
    "    contains_hashtag = []\n",
    "    consecutive_punc = []\n",
    "    more_upper = []\n",
    "    first_pronoun = []\n",
    "    fewer_first_person = []\n",
    "    more_negative = []\n",
    "    no_positive = []\n",
    "    contains_url = []\n",
    "    not_alphanum = []\n",
    "    misspellings = []\n",
    "    disagreements = []\n",
    "    in_hatebase = []\n",
    "    about_gender = []\n",
    "    about_religion = []\n",
    "    about_sexual_orientation = []\n",
    "    about_ethnicity = []\n",
    "    about_disability = []\n",
    "    about_social_class = []\n",
    "    about_nationality = []\n",
    "\n",
    "    for tweet in tweets['tweet']:\n",
    "        #count tokens\n",
    "        tweet_words = tweet.split()\n",
    "        num_token_bin = find_bin(len(tweet_words), num_token_bins)\n",
    "        word_counts.append(num_token_bin)\n",
    "        \n",
    "        #count swear words, misspellings, and if a word is in hatebase\n",
    "        misspell_count = 0\n",
    "        tot_bad = 0\n",
    "        hatebase = 0\n",
    "        hatebase_words = []\n",
    "        for word in tweet_words:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "            if word.lower() not in words_set:\n",
    "                misspell_count+=1\n",
    "            if word.lower() in hatebase_words_set:\n",
    "                hatebase = 1\n",
    "                hatebase_words.append(word)\n",
    "        swear_bin = find_bin(tot_bad, swear_bins)\n",
    "        swear_counts.append(swear_bin)\n",
    "        misspell_bin = find_bin(misspell_count, misspell_bins)\n",
    "        misspellings.append(misspell_bin)\n",
    "        in_hatebase.append(hatebase)\n",
    "    \n",
    "        gender = religion = sexual_orientation = ethnicity = disability = social_class = nationality = 0\n",
    "        if hatebase == 1:\n",
    "            for word in hatebase_words:\n",
    "                if word in hb_gender: gender = 1\n",
    "                if word in hb_religion: religion = 1\n",
    "                if word in hb_sexual_orientation: sexual_orientation = 1\n",
    "                if word in hb_ethnicity: ethnicity = 1\n",
    "                if word in hb_disability: disability = 1\n",
    "                if word in hb_social_class: social_class = 1\n",
    "                if word in hb_nationality: nationality = 1\n",
    "        about_gender.append(gender)\n",
    "        about_religion.append(religion)\n",
    "        about_sexual_orientation.append(sexual_orientation)\n",
    "        about_ethnicity.append(ethnicity)\n",
    "        about_disability.append(disability)\n",
    "        about_social_class.append(social_class)\n",
    "        about_nationality.append(nationality)\n",
    "        \n",
    "        #count mentions\n",
    "        at_count = tweet.count('@')\n",
    "        if at_count > 0:\n",
    "            contains_at.append(1)\n",
    "        else:\n",
    "            contains_at.append(0)\n",
    "        at_bin = find_bin(at_count, at_bins)\n",
    "        at_counts.append(at_bin)\n",
    "        \n",
    "        #count hashtags\n",
    "        hash_count = tweet.count('#')\n",
    "        if hash_count > 0:\n",
    "            contains_hashtag.append(1)\n",
    "        else:\n",
    "            contains_hashtag.append(0)\n",
    "        hash_bin = find_bin(hash_count, hashtag_bins)\n",
    "        hashtag_counts.append(hash_bin)\n",
    "\n",
    "        more_upper.append(has_more_upper(tweet))\n",
    "        consecutive_punc.append(has_consecutive_punc(tweet))\n",
    "        first, more = get_pronouns(tweet)\n",
    "        first_pronoun.append(first)\n",
    "        fewer_first_person.append(more)\n",
    "        \n",
    "        more_neg, pos = get_sentiment(tweet)\n",
    "        more_negative.append(more_neg)\n",
    "        no_positive.append(pos)\n",
    "        \n",
    "        if 'http://' in tweet:\n",
    "            contains_url.append(1)\n",
    "        else:\n",
    "            contains_url.append(0)\n",
    "            \n",
    "        not_alphanum.append(contains_non_alphanum(tweet))\n",
    "        \n",
    "        #See if there were disagreements about classification\n",
    "        i = tweets.loc[tweets['tweet']==tweet].index[0]\n",
    "        total_votes = judgements.at[i, 'count']\n",
    "        if (judgements.at[i, 'hate_speech'] == total_votes) or (judgements.at[i, 'offensive_language'] == total_votes)\\\n",
    "            or (judgements.at[i, 'neither'] == total_votes):\n",
    "            disagreements.append(0)\n",
    "        else:\n",
    "            disagreements.append(1)\n",
    "        \n",
    "\n",
    "    tweets['Word Counts'] = word_counts\n",
    "    tweets['Swear Counts'] = swear_counts\n",
    "    tweets['@ Counts'] = at_counts\n",
    "    tweets['Mention'] = contains_at\n",
    "    tweets['Hashtag Counts'] = hashtag_counts\n",
    "    tweets['Contains Hashtag'] = contains_hashtag\n",
    "    tweets['Consecutive Punctuation'] = consecutive_punc\n",
    "    tweets['Majority Uppercase Letters'] = more_upper\n",
    "    tweets['First Word Second or Third Person Pronoun'] = first_pronoun\n",
    "    tweets['More Second or Third Person Pronouns than First'] = fewer_first_person\n",
    "    tweets['Majority Negative Words'] = more_negative\n",
    "    tweets['No Positive Words'] = no_positive\n",
    "    tweets['Contains URL'] = contains_url\n",
    "    tweets['Contains Non Alphanumeric Word'] = not_alphanum\n",
    "    tweets['Misspelling Count'] = misspellings\n",
    "    tweets['Judgement Disagreements'] = disagreements\n",
    "    tweets['About Gender (Hatebase)'] = about_gender\n",
    "    tweets['About Religion (Hatebase)'] = about_religion\n",
    "    tweets['About Ethnicity (Hatebase)'] = about_ethnicity\n",
    "    tweets['About Sexual Orientation (Hatebase)'] = about_sexual_orientation\n",
    "    tweets['About Disability (Hatebase)'] = about_disability\n",
    "    tweets['About Class (Hatebase)'] = about_social_class\n",
    "    tweets['About Nationality (Hatebase)'] = about_nationality\n",
    "    X = tweets[[col for col in tweets.columns if col!=\"tweet\"]].values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine train and dev sets for k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train_data, dev_data], sort=False)\n",
    "tweets = data[['tweet']]\n",
    "judgements = data[['count', 'hate_speech', 'offensive_language', 'neither']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:118: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:120: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:121: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:122: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/anaconda2/envs/221/lib/python3.7/site-packages/ipykernel_launcher.py:123: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "X = process_tweets(tweets, judgements)\n",
    "y = data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22214, 23)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run 10-fold cross validation on combined training and dev sets on LR, SVM, and NB models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR = LogisticRegression(fit_intercept=True, max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "lr_y_pred = cross_val_predict(LR, X, y, cv=cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.48      0.06      0.10      1282\n",
      "           1       0.87      0.92      0.89     17186\n",
      "           2       0.62      0.64      0.63      3746\n",
      "\n",
      "    accuracy                           0.82     22214\n",
      "   macro avg       0.66      0.54      0.54     22214\n",
      "weighted avg       0.80      0.82      0.80     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"LR:\", classification_report(y, lr_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00      1282\n",
      "           1       0.87      0.92      0.89     17186\n",
      "           2       0.63      0.65      0.64      3746\n",
      "\n",
      "    accuracy                           0.82     22214\n",
      "   macro avg       0.50      0.53      0.51     22214\n",
      "weighted avg       0.78      0.82      0.80     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(gamma='auto') \n",
    "svm_y_pred = cross_val_predict(svm, X, y, cv=cv)\n",
    "print(classification_report(y, svm_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.59      0.29      1282\n",
      "           1       0.95      0.75      0.84     17186\n",
      "           2       0.55      0.69      0.61      3746\n",
      "\n",
      "    accuracy                           0.73     22214\n",
      "   macro avg       0.56      0.68      0.58     22214\n",
      "weighted avg       0.84      0.73      0.77     22214\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gnb = GaussianNB()\n",
    "nb_y_pred = cross_val_predict(gnb, X, y, cv=cv)\n",
    "print(classification_report(y, nb_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from twitter_preprocess import *\n",
    "import csv\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "TRAIN = '../data/train/training_data.csv'\n",
    "train_data = pd.read_csv(TRAIN, index_col=0)\n",
    "# get unigram counts for data\n",
    "def get_unigrams():\n",
    "    unigrams = Counter()\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        for word in s:\n",
    "            unigrams[word] += 1\n",
    "    return unigrams\n",
    "\n",
    "# \n",
    "def get_bigrams():\n",
    "    bigrams = Counter()\n",
    "    tweets = train_data[['tweet']]\n",
    "    start = \"<s>\"\n",
    "    end = \"</s>\"\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        word = start + \" \" + s[0]\n",
    "        bigrams[word] += 1\n",
    "        for i in range(len(s)-1):\n",
    "            word = s[i] + \" \" + s[i+1]\n",
    "            bigrams[word] += 1\n",
    "        word = s[len(s) - 1] + \" \" + end\n",
    "        bigrams[word] += 1\n",
    "    #print(bigrams)\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "#Gets the average word counts for data\n",
    "#maybe split tweet on more than just whitespace ie ;:,.')(\n",
    "def get_avg_wc():\n",
    "    wcs = {}\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot = 0.\n",
    "        for word in s:\n",
    "            if \"http://\" in word: continue #ignore hyperlinks\n",
    "            tot += len(word)\n",
    "        wcs[' '.join(s)] = tot / len(s)\n",
    "    return wcs\n",
    "\n",
    "\n",
    "#Gets the count of '@'s in the data\n",
    "def get_at_counts():\n",
    "    ats = {}\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        ats[' '.join(s)] = count\n",
    "            \n",
    "    return ats\n",
    "\n",
    "\n",
    "#Gets the count of swear words in tweets\n",
    "#potentially use regex's to catch purposeful mispellings & other nuances\n",
    "def get_swear_counts():\n",
    "    tweets = train_data[['tweet']]\n",
    "#     bad_word_list = open(\"en_bad_words.txt\").read().replace(\"\\n\", \"\").split(',')\n",
    "#     for i in range(len(bad_word_list)):\n",
    "#         bad_word_list[i] = bad_word_list[i].strip(\" \")\n",
    "    bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "    \n",
    "    bad_words_count = {}\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_bad = 0\n",
    "        for word in s:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "            bad_words_count[\" \".join(s)] = tot_bad\n",
    "            \n",
    "    return bad_words_count\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = get_unigrams()\n",
    "bigram_counts = get_bigrams()\n",
    "avg_wc = get_avg_wc()\n",
    "at_counts = get_at_counts()\n",
    "swear_counts = get_swear_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get top 100 unigrams and bigrams\n",
    "top_unigrams = unigram_counts.most_common(100)\n",
    "top_bigrams = bigram_counts.most_common(100)\n",
    "train_tweets = pd.read_csv(TRAIN, index_col=0)[['tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets):\n",
    "    for word in [u[0] for u in top_unigrams]:\n",
    "        tweets[word] = tweets['tweet'].str.contains(word).astype(int)\n",
    "        \n",
    "    word_counts = []\n",
    "    swear_counts = []\n",
    "    at_counts = []\n",
    "    bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "\n",
    "    for tweet in tweets['tweet']:\n",
    "        tweet_words = tweet.split()\n",
    "        word_counts.append(len(tweet_words))\n",
    "        tot_bad = 0\n",
    "        for word in tweet_words:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "        swear_counts.append(tot_bad)\n",
    "        at_count = tweet_words.count('@')\n",
    "        at_counts.append(at_count)\n",
    "\n",
    "    tweets['Word Counts'] = word_counts\n",
    "    tweets['Swear Counts'] = swear_counts\n",
    "    tweets['@ Counts'] = at_counts\n",
    "    X = tweets[[col for col in tweets.columns if col!=\"tweet\"]].values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = process_tweets(train_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = train_data['class']\n",
    "model = LogisticRegression(\n",
    "        fit_intercept=True, solver='lbfgs').fit(X1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/srizkall/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, ..., 2, 1, 1])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data = pd.read_csv('../data/dev/development_data.csv', index_col=0)\n",
    "dev_tweets = dev_data[['tweet']]\n",
    "X2 = process_tweets(dev_tweets)\n",
    "y2 = dev_data['class']\n",
    "predictions = model.predict(X2)\n",
    "y2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

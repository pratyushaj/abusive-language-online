{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from twitter_preprocess import *\n",
    "import csv\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "TRAIN = '../data/train/training_data.csv'\n",
    "train_data = pd.read_csv(TRAIN, index_col=0)\n",
    "dev_data = pd.read_csv('../data/dev/development_data.csv', index_col=0)\n",
    "\n",
    "\n",
    "# get unigram counts for data\n",
    "def get_unigrams_splitBySpace():\n",
    "    unigrams = Counter()\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        for word in s:\n",
    "            unigrams[word] += 1\n",
    "    return unigrams\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer()\n",
    "# get unigram counts for data\n",
    "def get_unigrams_nltkTokenizer():\n",
    "    uni = Counter()\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index, row in tweets.iterrows():\n",
    "        s = tknzr.tokenize(row['tweet'])\n",
    "        for word in s:\n",
    "            uni[word] += 1    \n",
    "    return uni\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "def get_bigrams():\n",
    "    bigrams = Counter()\n",
    "    tweets = train_data[['tweet']]\n",
    "    start = \"<s>\"\n",
    "    end = \"</s>\"\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        word = start + \" \" + s[0]\n",
    "        bigrams[word] += 1\n",
    "        for i in range(len(s)-1):\n",
    "            word = s[i] + \" \" + s[i+1]\n",
    "            bigrams[word] += 1\n",
    "        word = s[len(s) - 1] + \" \" + end\n",
    "        bigrams[word] += 1\n",
    "    #print(bigrams)\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Gets the average word counts for data\n",
    "#maybe split tweet on more than just whitespace ie ;:,.')(\n",
    "def get_avg_wc():\n",
    "    wcs = {}\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot = 0.\n",
    "        for word in s:\n",
    "            if \"http://\" in word: continue #ignore hyperlinks\n",
    "            tot += len(word)\n",
    "        wcs[' '.join(s)] = tot / len(s)\n",
    "    return wcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the count of '@'s in the data\n",
    "def get_at_counts():\n",
    "    ats = {}\n",
    "    tweets = train_data[['tweet']]\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        count = sum(map(lambda word : 1 if '@' in word else 0, s))\n",
    "        ats[' '.join(s)] = count\n",
    "            \n",
    "    return ats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets the count of swear words in tweets\n",
    "#potentially use regex's to catch purposeful mispellings & other nuances\n",
    "def get_swear_counts():\n",
    "    tweets = train_data[['tweet']]\n",
    "#     bad_word_list = open(\"en_bad_words.txt\").read().replace(\"\\n\", \"\").split(',')\n",
    "#     for i in range(len(bad_word_list)):\n",
    "#         bad_word_list[i] = bad_word_list[i].strip(\" \")\n",
    "    bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "    \n",
    "    bad_words_count = {}\n",
    "    for row_index,row in tweets.iterrows():\n",
    "        s = row['tweet'].split()\n",
    "        tot_bad = 0\n",
    "        for word in s:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "            bad_words_count[\" \".join(s)] = tot_bad\n",
    "            \n",
    "    return bad_words_count\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_counts = get_unigrams_splitBySpace()\n",
    "uni_tokenizer_counts = get_unigrams_nltkTokenizer()\n",
    "bigram_counts = get_bigrams()\n",
    "avg_wc = get_avg_wc()\n",
    "at_counts = get_at_counts()\n",
    "swear_counts = get_swear_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get top 100 unigrams and bigrams\n",
    "top_unigrams = unigram_counts.most_common(100)\n",
    "top_tokenized = uni_tokenizer_counts.most_common(100)\n",
    "top_bigrams = bigram_counts.most_common(100)\n",
    "data = pd.concat([train_data, dev_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_tweets(tweets):\n",
    "    for word in [u[0] for u in top_unigrams]:\n",
    "        tweets[word] = tweets['tweet'].str.contains(word).astype(int)\n",
    "        \n",
    "    word_counts = []\n",
    "    swear_counts = []\n",
    "    at_counts = []\n",
    "    bad_words_set = set(open(\"bad-words.txt\").read().split())\n",
    "\n",
    "    for tweet in tweets['tweet']:\n",
    "        tweet_words = tweet.split()\n",
    "        word_counts.append(len(tweet_words))\n",
    "        tot_bad = 0\n",
    "        for word in tweet_words:                #Use regexs? \n",
    "            word = word.replace(\".\",\"\").replace(\",\",\"\").replace(\"!\",\"\").replace(\"?\",\"\")\n",
    "            if word.lower() in bad_words_set:\n",
    "                tot_bad+=1\n",
    "        swear_counts.append(tot_bad)\n",
    "        at_count = tweet_words.count('@')\n",
    "        at_counts.append(at_count)\n",
    "\n",
    "    tweets['Word Counts'] = word_counts\n",
    "    tweets['Swear Counts'] = swear_counts\n",
    "    tweets['@ Counts'] = at_counts\n",
    "    X = tweets[[col for col in tweets.columns if col!=\"tweet\"]].values\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/susanabenavidez/anaconda3/envs/nlu/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "tweets = data[['tweet']]\n",
    "X = process_tweets(tweets)\n",
    "y = data['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "LR_scores = []\n",
    "SVM_scores = []\n",
    "\n",
    "for train_index, test_index in cv.split(X):\n",
    "    LR = LogisticRegression(fit_intercept=True, max_iter=1000, solver='lbfgs', multi_class='ovr')\n",
    "    svm = SVC(gamma='auto') \n",
    "    X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "    LR.fit(X_train, y_train)\n",
    "    LR_scores.append(LR.score(X_test, y_test))\n",
    "    svm.fit(X_train, y_train)\n",
    "    SVM_scores.append(svm.score(X_test, y_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8618361836183618,\n",
       " 0.8550855085508551,\n",
       " 0.8667866786678667,\n",
       " 0.8861386138613861,\n",
       " 0.8716794236830256,\n",
       " 0.875281404772625,\n",
       " 0.8779828905898244,\n",
       " 0.8923908149482215,\n",
       " 0.8829356145880234,\n",
       " 0.8793336334984241]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LR_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SVM_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b6a4904eab0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mSVM_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'SVM_scores' is not defined"
     ]
    }
   ],
   "source": [
    "SVM_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlu",
   "language": "python",
   "name": "nlu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
